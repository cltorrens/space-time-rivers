---
title: "N model based on Bray"
author: "Christa Torrens"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
#### notes to me
Right now this is failing at the data generation step. May need to change 'ifelse' to 'if... else'; look up format
I feel like this worked earlier... but it's not working now. 


#### Background

End goal: develop a model to predict [NO3-N] across space and time in rivers, using high-frequency SUNA, PAR, Q (and more...) data.

1st subgoal: Take the model from the Bray thesis; fake 1 day's worth of data, add process error
2nd subgoal: Use the Bray model to fake 30 days worth of data, with process error. Params recovered? - eh... 
3rd subgoal: Run the pooled model
4th subgoal: Use for 1-2 other sites (from NEON)
Additional subgoals: Add more hydrology?  e.g.: ..... 



```{r load packages, message=FALSE}
# Load packages
library(scales)
library(magrittr) # moved from  'light ea day as vector'
library(tidyverse)
library(lubridate)
library(streamMetabolizer)
library(rstan)
library(tidybayes)
library(GGally)

```

### Faking data
First, let's fake 1 day's worth of data using the UNPOOLED Bray model AND Bray light data
(then we'll try this with more days and then with the POOLED model)


#### Loading data 
```{r load data}

## EARLY ON: use Bray's Miller Creek dataset
## LATER: use NEON data
# Load Bray data from Miller Creek - for light data
#datain<-read.csv("Miller Creek N uptake input.csv")
#head(datain) # date, Jday, time, light, z, conc (= N conc)

# Bray model - UNPOOLED
# [N]_i = [N]_i-1 + (U/z)(hourly light/total daily light) + K(ambient [N] - [N]_i-1)*dt   
# NOTE: dt = 1/24 for hourly timestep

# From Bray thesis: 
# K = any daily change in NO3-N concentration due to loss from autotrophic uptake and denitrification or gains from upstream or groundwater inputs expressed as a rate (d-1)
# U = daily median autotrophic NO3-N uptake areal flux (mg N m-2 d-1)


```

#### setting seed and parameters
```{r params}

# set seed for consistent results
set.seed(1001)

# Param values
K <- 3  # 3-19 in Bray, units = d^-1
U <- 3 # was 0.4-10 mg/m2/day in Bray) - would be negative (uptake removes N) but the equation subtracts it/ accounts for this
# Per Bob: work in units of days, meters, grams (??)) - nope, take it back, stan needs to work in unit scale
N_amb <- 57   #In mg, mean = 57.16, sd=11.98) #daily value for N_amb
N_init <- 55  
z <- 0.2 #depth in m - constant for now

### time and light for one day:
#date.time <- ymd_hms('20190625 040000', tz='America/Denver') 
#light <- datain$light[5:28]  # For now, using light data from Bray's Miller Creek dataset: 0400 6/25/2019 - 0300 6/26/2019
#sumlight <- sum(light) # daily light

### for multiple days: see below

```

#### Generating light data from streamMetabolizer::calc_light
Steps:
1) Need to generate posix datetimes for streamMetabolizer
2) Calculate solar time from local time streamMetabolizer::calc_solar_time
3) Use the solar.time to generate light data using streamMetabolizer::calc_light


##### Hourly light data reading 
Not needed after the 5-min compiled data is working
```{r hourly light data reading}


# Generate timeseries
t1 <- ymd_hms('20190625 040000', tz='America/Denver')  # as.numeric = 1561460400 (# seconds since 1970)
t2 <- ymd_hms('20190725 030000', tz='America/Denver') ## 030000 for hourly data; 035500 for 5-min data (it's dark so won't really matter...)
tseq <- seq(from=as.numeric(t1), to=as.numeric(t2), by=3600) # from a to b in 1-hour timesteps

date.time <- as_datetime(tseq, tz='America/Denver') 
#head(date.time)

date <- date(date.time)
time.h <- hour(date.time)
Jday <- yday(date.time)

# Calculate light using streamMetabolizer (lat-long for Nyack Creek, MT)
datetime.solar <- streamMetabolizer::calc_solar_time(local.time=date.time, longitude=-113.64569089116506)

light.h <- streamMetabolizer::calc_light(solar.time=datetime.solar, latitude=48.515096115449715, longitude=-113.64569089116506)

# Summarize daily light from hourly timestep
#     First create a dataframe
timelight.df <- data.frame(datetime = date.time,
                           time.h = time.h,
                           Jday = Jday,
                           light.h = light.h)

#     Then summarize by Jday
sumlight.h <- timelight.df %>%
  group_by(Jday) %>%
  summarize(sumlight.h = sum(light.h))

# Take a look
plot(tseq, light.h)
plot(sumlight.h$Jday, sumlight.h$sumlight.h)  # looks fine - but 'sumlight.h' is >3 orders of magnitude too small, need to sum 5-min data to the hour for more accurate light/ sumlight data

# trying to add this to the df.... to make the data faking model easier/ avoid nested for loops
# timelight.df$sumlight.h <- timelight.df %>%
#   group_by(Jday) %>%
#   rep(summarise(sumlight.h = sum(light.h)), 

## Standardize for model
light <- light.h
sumlight <- sumlight.h$sumlight.h

```

##### 5-minute data readings, compiled 
Compiled to 1 hour for the hourly light; might be even better to compile 1-minute data

```{r 5-min light data readings, compiled}

# Generate timeseries
t1 <- ymd_hms('20190625 040000', tz='America/Denver')  # as.numeric = 1561460400 (# seconds since 1970)
t2 <- ymd_hms('20190725 035500', tz='America/Denver') ## 030000 for hourly data; 035500 for 5-min data (although it's dark so that won't affect hourly light or sumlight)
tseq <- seq(from=as.numeric(t1), to=as.numeric(t2), by=300)  # from a to b in 5-minute timesteps 


date.time <- as_datetime(tseq, tz='America/Denver') 
#head(date.time)

date <- date(date.time)
time.h <- hour(date.time)
time.5min <- minute(date.time)
Jday <- yday(date.time)


# Calculate light using streamMetabolizer (lat-long for Nyack Creek, MT)
datetime.solar <- streamMetabolizer::calc_solar_time(local.time=date.time, longitude=-113.64569089116506)

light.5min <- streamMetabolizer::calc_light(solar.time=datetime.solar, latitude=48.515096115449715, longitude=-113.64569089116506)


# Hourly and daily data from 5min sunlight data
timelight.df <- data.frame(datetime = date.time, 
                           time.h = time.h,
                           time.5min = time.5min,
                           Jday = Jday,
                           light.5min = light.5min)

light.h <- timelight.df %>%
  group_by(Jday, time.h) %>%    
  summarize(light.h = sum(light.5min))

sumlight.5m <- timelight.df %>%
  group_by(Jday) %>%
  summarize(sumlight.5m = sum(light.5min))

plot(light.h$time.h, light.h$light.h)
plot(sumlight.5m$Jday, sumlight.5m$sumlight.5m)

#plot(light.h$light.h, light.h1) # checking the difference between methods - other than orders of magnitude


## Standardize for model
light <- light.h$light.h
sumlight <- sumlight.5m$sumlight.5m

```

Use the above to fake some N concentration data for the model: first for one day, then for multiple days

#### One day of N data
```{r generate data - one day}
#### Data model for one day - no error 
# # For one day: 
# for (i in 2:length(light)) {
#   N[i] <- N[i-1] + (U/z)*(light[i]/sumlight) - K*(N_amb-N[i-1])*1/24
# }
# 
# plot(seq(4, 27, 1), N)
# 
# #### Data model for one day with process error
# # For one day: 
# for (i in 2:length(light)) {
#   N[i] <- N[i-1] - (U/z)*(light[i]/sumlight) + K*(N_amb-N[i-1])*1/24  + rnorm(1, mean = 0, sd = 0.4)
# }
# 
# plot(seq(4, 27, 1), N)
# 
# df.fake <- data.frame(date, Jday, time.h, light, z, N)

```

#### Multiple days of N data
Generate N data for 30 days . Use streamMetabolizer::calc_light to fake the light data, using Nyack Creek's lat-long (48.515096115449715, -113.64569089116506). 

```{r generate data - multiple days}

#### Data model for multiple days 

# sumlight changes on day^-1 - or do I add this as a repeating column?
# N and light change on h^-1

# ## sumlight that is added to the original df (repeats/ is the same for each Jday)
# for (i in 2:length(light)) {
#   N[i] <- N[i-1] - (U/z)*(light[i]/sumlight[i]) + K*(N_amb-N[i-1])*1/24
# }

nday <- 30
sumlight <- sumlight[-31]  # removes the final Jday, which has no light (midnight-4a)
lightMA <- matrix(light, nrow=24)

# initialize N as a matrix
N <- matrix(data=NA, nrow = 24, ncol = nday)
N[1,1] <- N_init

for (d in 1:length(sumlight)) {
  ifelse(d > 1, N[1,d]<-N[24, d-1], N[1,d]<-N_init)
  for (h in 2:24) {
  N[h,d] <- N[h-1,d] - (U/z)*(lightMA[h,d]/sumlight[d]) + K*(N_amb-N[h-1,d])*1/24  + rnorm(1, mean = 0, sd = 0.4)
  }

}

plot(N[,11])
#View(N)


```

For Bray's method, I need to change the data vectors for light, z and N concentration into matrices

```{r create matrices}
###Create variables for stan

### Light and N are already in matrix form for the 5-min data generation; sumlight already exists

concMA <- N

###light at each timestep as matrix (col=days, row= hours/day)
#lightMA <- matrix(unlist(df.fake$light), ncol = nday, byrow = FALSE)

###nitrate conc each timestep as matrix (col=days, row= hours/day)
#concMA <- matrix(unlist(df.fake$N), ncol = nday, byrow = FALSE)

###sum total light for each day as vector - needed for Bray but not for me
# 
# sumlightgrouped<-df.fake %>% group_by(Jday) %>%
#   summarize(sumlight = sum(light))
# 
# sumlight<-sumlightgrouped$sumlight
# sumlight <- as.vector(sumlight)
head(sumlight)
#sumlight

#check sum light
plot(sumlight)


###depth at each timestep as matrix (col=days, row= hours/day)
zMA<- matrix(z, ncol = nday, nrow=24)

```

Now that the data is in the correct form, I create the data list and send the call to stan

#### Part 1: unpooled model: 
data list and call to stan
```{r call stan}
###merge matrices, vectors,and real numbers into single list
#T=number of hours in a day, D= number of days of sampling, deltat=change in time between time steps (in units of day)

data <- list(T=24,D=nday,deltat=1/24,lightMA=lightMA,sumlight=sumlight,zMA=zMA,concMA=concMA) # sumlight = 26000 for one day

fit <- stan("unpooled_ct.stan", data = data,  iter = 1000, chains = 4,control = list(max_treedepth = 15))
print(fit)
# stan data: lightMA, concMA, deltat, sumlight, zMA, T, D

# K = 3, ambient N (N_b) = 57, U = 3 -> right now, w 5-min light data, U an order of magnitude too low... 

```

For 1 day, the model kept failing because of a dimension mismatch error in line 12 - it didn't like the vector 'sumlight' for one day, somehow. To get around this, for one day we hard-coded '26000' in place of 'sumlight' in the stan model.

Error message: 
>Error : Exception: mismatch in number dimensions declared and found in context; processing stage=data initialization; variable name=sumlight; dims declared=(1); dims found=()  (in 'model11f6f4aaf76cd_unpooled_ct' at line 13)

The model, as originally written, worked as expected for 30 days. 

I do keep getting warnings re: an 'incomplete final line'  (which doesn't occur in the knit...):  
Warning message:
>In readLines(file, warn = TRUE) :
  incomplete final line found on '/Users/torrensc/Documents/R-working/Modelscapes/space_time_rivers/Bray_code/N uptake/unpooled_ct.stan'


##### Examine data output from the unpooled model: 

```{r unpooled output exploration}

###daily mean, se, sd, percentiles, n_eff, and Rhat
d<-summary(fit)
d

#export summary stats
write.csv(d,file="MCNR Bayesian N uptake output_CT.csv")

output1<-read.csv("MCNR Bayesian N uptake output_CT.csv")
head(output1)
dim(output1)

##create data frame of mean K, confidence interval of K, N_b, and U
#K
K1<-output1$summary.mean[2:31]
end(K1)

#K confidence interval
K_int_high<-output1$summary.97.5.[2:31]
K_int_low<-output1$summary.2.5.[2:31]

K_conf_int1<-(K_int_high-K_int_low)/2
head(K_conf_int1)

#K SD
K1_SD<-output1$summary.sd[2:31]
end(K1)

#N_b
N_b1<-output1$summary.mean[32:61]
end(N_b1)
mean(N_b1) #57.56146

#U
U1<-output1$summary.mean[62:91]
end(U1)

#date
Date <- seq(as.Date('2019-06-25'), as.Date('2019-07-24'), by = 'days')
Date

#dataframe
output_average<-data.frame(Date,K1,N_b1,U1,K_conf_int1,K1_SD)
dim(output_average)
head(output_average)
output_average

###plot K vs. U to check  colinearity
plot(output_average$K1,output_average$U1, xlab="K", ylab="U",main="Unpooled")

#plot U over the study
plot(Date,output_average$U1,xlab="Date",ylab="U",main="Unpooled")
lines(Date,output_average$U1,xlab="Date",ylab="U",main="Unpooled")

#Plot K over study
plot(Date,output_average$K1,xlab="Date",ylab="K",main="Unpooled")
lines(Date,output_average$K1,xlab="Date",ylab="K",main="Unpooled")



```


#### Part Two: pooled model
pool the uptake (U) data per Bray: 
>b1 <- 1.1138
>b0 <- -2.803
>pooled_line  U ~ normal(b0+b1*sumlight*1e-6, sigma_U) = linear relationship between sunlight and N uptake

> mean(U_newp)  [1] 2.925009
> sd(U_newp)  [1] 0.05848184


Re-fake the N data to match the new model: 
```{r generate N data for pooled model}

# lightMA, zMA, sumlight RTS from the unpooled model; need to re-generate the N data

## Pooled U with light

set.seed(1001)

# Param values

nday <- 30
K <- 3  # 3-19 in Bray, units = d^-1
N_amb <- 57   #In mg, mean = 57.16, sd=11.98) #daily value for N_amb
N_init <- 55  
z <- 0.2 #depth in m - constant for now

# for the pooled U
#b0_mean <- 
#b1_mean <- 
mean_U <- 3
#sigma_U <- 0.6  #sd U_newp = 0.0585 - NOT unit scale
error_U <- rnorm(nday, mean=0, sd= 0.5)
mean(error_U)

######## NEEDS WORK ########

# U pooled w. sunlight
# for (d in 1:nday) {
#   U[d] <- rnorm(b0_mean + U_mean*sumlight[d]*1e-6, sigma_U)
#   
#   for h in 2:
# }
# U[d] <- rnorm(nday, b0_mean + U_mean*sumlight[d]*1e-6, sigma_U)


# U pooled by mean
U <- mean_U + error_U

## Faking the data

sumlight <- sumlight[-31]  # removes the final Jday, which has no light (midnight-4a)
lightMA <- matrix(light, nrow=24)
###depth at each timestep as matrix (col=days, row= hours/day)
zMA<- matrix(z, ncol = nday, nrow=24)

# initialize N as a matrix
N <- matrix(data=NA, nrow = 24, ncol = nday)
N[1,1] <- N_init

for (d in 1:length(sumlight)) {
  ifelse(d > 1, N[1,d]<-N[24, d-1], N[1,d]<-N_init)
  for (h in 2:24) {
  N[h,d] <- N[h-1,d] - (U[d]/z)*(lightMA[h,d]/sumlight[d]) + K*(N_amb-N[h-1,d])*1/24  + rnorm(1, mean = 0, sd = 0.4)
  }

}

plot(N[,30])  
#View(N)

concMA <- N

```




Stan data is the same as above: T, D, deltat *(time increment of sampling in d (1/24))*, lightMA, sumlight, zMA, concMA

##### Call to STAN

```{r call to stan}

# linear model to approximate the betas for the prior distributions?
# get N as a vector...

#betas <- lm(c(N)~light)  ## b0 (intercept) = 53.121864,  b1 (slope (light)) = -0.000125  hmmm. Nothing like Bray's values, also not in unit scale... 

# Stan data: T, D, deltat (time increment of sampling in d (so, 1/24)), lightMA, sumlight, zMA, concMA
# same as for the unpooled model, above: 
data <- list(T=24,D=nday,deltat=1/24,lightMA=lightMA,sumlight=sumlight,zMA=zMA,concMA=concMA)

fit <- stan("pooled_ct.stan", data = data,  iter = 3000, chains = 4,control = list(max_treedepth = 15))
print(fit)

p <- summary(fit)

## what's the syntax for a pairs() plot (mentioned in warnings)

```

**To avoid a slew of warnings and failures to converge (see end) used minimally-informed priors in spite of some prior info. How does that square w this method?**

```{r pooled model output}

#view chains
rstan::traceplot(fit,pars="U")
rstan::traceplot(fit,pars="K")
rstan::traceplot(fit,pars="N_b")

# For informed priors, all failed at about 2500 iter

###daily mean, se, sd, percentiles, n_eff, and Rhat
write.csv(p,file="pooled_N_uptake_output_CT.csv")

output_pool<-read.csv("pooled_N_uptake_output_CT.csv")
head(output_pool)
dim(output_pool)


#K
K_pool<-output_pool$summary.mean[2:31]
end(K_pool)

#K confidence interval
K_int_high_pool<-output_pool$summary.97.5.[2:31]
K_int_low_pool<-output_pool$summary.2.5.[2:31]

K_conf_int_pool<-(K_int_high_pool-K_int_low_pool)/2
head(K_conf_int_pool)

#K SD
K_SD_pool<-output_pool$summary.sd[2:31]
end(K_SD_pool)

#N_b
N_b_pool<-output_pool$summary.mean[32:61]
end(N_b_pool)

#U
U_pool<-output_pool$summary.mean[62:91]
end(U_pool)

#date
Date <- seq(as.Date('2019-06-25'), as.Date('2019-07-24'), by = 'days')
Date

#dataframe
output_average_pool<-data.frame(Date,sumlight,K_pool,N_b_pool,U_pool,K_conf_int_pool,K_SD_pool)
dim(output_average_pool)
head(output_average_pool)
output_average_pool


### Pairwise plots to diagnose sampling problems 

ggpairs(output_average_pool[,2:5])

###plot K vs. U to check colinearity
#plot(output_average_newp$K_newp,output_average_newp$U_newp, xlab="K", ylab="U",main="Pooled, new priors")

mean(output_average_pool$K_pool) # 2.80 (3 given)
mean(output_average_pool$N_b_pool) # 58.19  (57 given)
mean(output_average_pool$U_pool) # 2.93 (mean_U = 3)

## plot data vs model confint for each day (facet wrap?)?
#ggplot(data=output_average_pool, aes(x = ))

```

Next, need to work on a better pooled model - not just the mean, but pooling U w sunlight


#### Part Three: longer run (30 days) using NEON data from other streams

Once the pooled model is working, run on other NEON streams: "KING" "WALK" "BIGC" "CARI"
Caribou (AK) and Big Creek (CA) look the best (from earlier explorations)

***NEED TO FIND A RUN OF 30 DAYS W/O NAs*** 

NB: NEON timestamps are always in UTC (https://www.neonscience.org/resources/learning-hub/tutorials/explore-neon-ais-data)

UNITS FOR NO3?!?!  I can't find this anywhere

if mM NO3: NO3 = 62.0049 g/mol, mg/mmol, ug/umol

mM NO3-*N*? *14.001 g/mol etc. 

```{r load NEON N data}

# load NO3 data for all 4 streams
load("~/Documents/R-working/Modelscape/space_time_rivers/NEON/data_derived/no3_dataset.Rdata")
View(no3_data_sensor)

# Filter by site ID  ("KING" "WALK" "BIGC" "CARI") and set appropriate time zone
      # grep("USA", OlsonNames(), value=TRUE)

## Big Creek, CA
bigc.df <- no3_data_sensor %>%
  filter(siteID == "BIGC") %>% 
  mutate(local_datetime = with_tz(startDateTime, tzone="America/Los_Angeles"))

## check years to use for training
ggplot(data = bigc.df, aes(x=startDateTime, y = surfWaterNitrateMean)) + 
  geom_point() + 
  facet_wrap(year(bigc.df$startDateTime))

#### Filter to 30 days 

## 2019
# bigc.df.19 <- bigc.df %>%
#   filter(startDateTime >= ymd_hms('20190625 040000', tz='America/Los_Angeles') & startDateTime <= ymd_hms('20190725 035500', tz='America/Los_Angeles')) %>%
#   mutate(Jday = yday(local_datetime))

## OR use july 4 - august 3?
bigc.df.19 <- bigc.df %>%
  filter(startDateTime >= ymd_hms('20190703 040000', tz='America/Los_Angeles') & startDateTime <= ymd_hms('20190801 035500', tz='America/Los_Angeles')) %>%
  mutate(Jday = yday(local_datetime))

## select hourly no3 measurements to match what we've done
bigc.df.19h <- bigc.df.19 %>%
  filter(minute(startDateTime) == 0)

plot(bigc.df.19h$local_datetime, bigc.df.19h$surfWaterNitrateMean, type = 'l')


##2021
# bigc.df.21 <- bigc.df %>%
#   filter(startDateTime >= ymd_hms('20210625 040000', tz='America/Los_Angeles') & startDateTime <= ymd_hms('20210725 035500', tz='America/Los_Angeles')) %>%
#   mutate(Jday = yday(local_datetime))

bigc.df.21 <- bigc.df %>%
  filter(startDateTime >= ymd_hms('20210715 040000', tz='America/Los_Angeles') & startDateTime <= ymd_hms('20210814 035500', tz='America/Los_Angeles')) %>%
  mutate(Jday = yday(local_datetime))

## select hourly no3 measurements to match what we've done
bigc.df.21h <- bigc.df.21 %>%
  filter(minute(startDateTime) == 0)

plot(bigc.df.21h$local_datetime, bigc.df.21h$surfWaterNitrateMean, type = 'l')
######   UNITS??? If mM: NO3 = 62.0049 g/mol, mg/mmol, ug/umol

bigc.df.21h <- bigc.df.21h %>%
  mutate(nconc_mg = surfWaterNitrateMean*62)

plot(bigc.df.21h$local_datetime, bigc.df.21h$nconc_mg)
```

# OK: working to generate hourly light data and hourly N measurements. Do we really want to limit our data?  


#### Generate light data
```{r derive light from streamMetabolizer}
### fake the light data 
# big creek lat = 39.10507, long = -96.60339  (from no3_spatial_sensor)


# # Generate timeseries
# t1 <- ymd_hms('20190625 040000', tz='America/Los_Angeles')  
# t2 <- ymd_hms('20190725 035500', tz='America/Los_Angeles') 

t1 <- ymd_hms('20190703 040000', tz='America/Los_Angeles')  
t2 <- ymd_hms('20190801 035500', tz='America/Los_Angeles') 

tseq <- seq(from=as.numeric(t1), to=as.numeric(t2), by=300)  # from a to b in 5-minute timesteps 
date.time <- as_datetime(tseq, tz='America/Los_Angeles') 
# #head(date.time)
date <- date(date.time)
time.h <- hour(date.time)
time.5min <- minute(date.time)
Jday <- yday(date.time)


####### Calculate light using streamMetabolizer (lat-long for Nyack Creek, MT)
datetime.solar <- streamMetabolizer::calc_solar_time(local.time=date.time, longitude=-96.60339)

light.5min <- streamMetabolizer::calc_light(solar.time=datetime.solar, latitude=39.10507, longitude=-96.60339)

# Hourly and daily data from 5min sunlight data
timelight.df <- data.frame(datetime = date.time, 
                           time.h = time.h,
                           time.5min = time.5min,
                           Jday = Jday,
                           light.5min = light.5min)

light.h <- timelight.df %>%
  group_by(Jday, time.h) %>%    
  summarize(light.h = sum(light.5min))

sumlight.5m <- timelight.df %>%
  group_by(Jday) %>%
  summarize(sumlight.5m = sum(light.5min))

plot(light.h$time.h, light.h$light.h)
plot(sumlight.5m$Jday, sumlight.5m$sumlight.5m)

## Standardize terms for model
light <- light.h$light.h
sumlight <- sumlight.5m$sumlight.5m

```

#### Other model parameters

```{r model params}

# Param values

nday <- 30

N_amb <- mean(bigc.df.19h$nconc_mg) # ~3.69  UNITS????  228.4 if * by NO3 MW (62)
N_init <- bigc.df.19h$nconc_mg[1] # 3.8... 
concMA <- matrix(unlist(bigc.df.19h$nconc_mg), ncol = nday, byrow = FALSE)
is.na(concMA)

z <- 0.2 #depth in m - constant for now
zMA<- matrix(z, ncol = nday, nrow=24) #depth at each timestep as matrix (col=days, row= hours/day)

sumlight <- sumlight[-31]  # removes the final Jday, which has no light (midnight-4a)
lightMA <- matrix(light, nrow=24)

  
# Stan data: T, D, deltat (time increment of sampling in d (so, 1/24)), lightMA, sumlight, zMA, concMA
# same as for the unpooled model, above: 
data <- list(T=24,D=nday,deltat=1/24,lightMA=lightMA,sumlight=sumlight,zMA=zMA,concMA=concMA)

fit <- stan("pooled_ct.stan", data = data,  iter = 3000, chains = 4,control = list(max_treedepth = 15))
print(fit)

bigc.p <- summary(fit)



```









WARNINGS from pooled model / fake data:
After running w 1K iter => got the following warnings and upped to 3K iter - and got more warning messages
Warning messages:
>1: There were 220 divergent transitions after warmup. See
>https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them. 
2: Examine the pairs() plot to diagnose sampling problems
3: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess 
4: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess 


After 3K iter got *these* warning messages:
Warning messages:
>1: There were 675 divergent transitions after warmup. See
https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them. 
2: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See
https://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
 4: The largest R-hat is 1.07, indicating chains have not mixed.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#r-hat 
5: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess 
6: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess 
> 


Changed the priors (which had large sds, taken from Bray's model), ran 3k iter and still got errors:
Warning messages:
>Warning messages:
1: There were 648 divergent transitions after warmup. See
https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them. 
2: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See
https://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
4: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess 
5: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess 
>

There is also an odd break in 1-2 of the chains around 2500 iter