---
title: "02_select DO data for GPP"
author: "Christa Torrens"
format: html
editor: visual
---

# Intro

The purpose of this script is to take datasets provided by Bobby Hensley (NEON) OR that I created (CLT), and select the days that I need to estimate GPP for my nitrate model. This is the link to the folder Bobby shared, which he is populating with DO **and co-located temperature data** for his collaboration with Kelly Aho. https://drive.google.com/drive/folders/180khdU7rR65UqWKDQu8WPEwMSo_U0J5E?usp=drive_link

## Load packages

```{r packages}

# load packages
library(tidyverse)
library(zoo)
library(here)
library(plotly)

```

## Functions

```{r functions}

############ Convert jday list to tibble with jday, datetime, and yr-jday

julian_to_date <- function(julian_days, year_val) {
  tibble(jday = julian_days,
         date = ymd(paste0(year_val, "-01-01")) + days(julian_days - 1)) %>%
  mutate(Year = year(date),
         jday_pad = str_pad(jday, width = 3, pad = "0")) %>%
  unite("yr_jday", Year, jday_pad, sep = "_", remove = FALSE) %>%
    select(!jday_pad)
}

############ Check for gaps in 15-minute data _ ORIGINAL in 01_prepNEON_streamMetabolizer (mine)
IDtimegaps <- function(df, datetimecol, difference = 15, units = "mins") {
  df %>%
    arrange({{ datetimecol }}) %>%
    mutate(
      time_diff = as.numeric(difftime({{ datetimecol }}, lag({{ datetimecol }}), units = units)),
      gap_start = if_else(time_diff > difference, lag({{ datetimecol }}), as.POSIXct(NA, tz = "UTC")),
      gap_end   = if_else(time_diff > difference, {{ datetimecol }}, as.POSIXct(NA, tz = "UTC"))
    ) %>%
    filter(time_diff > difference) %>%
    select(gap_start, gap_end, time_diff)
}

############ Check for duplicate timestamps in a 15-min timeseries
check_duplicate_timestamps <- function(df, datetime_col) {
  datetime_sym <- rlang::ensym(datetime_col)  # allow column name input
  df %>%
    group_by(!!datetime_sym) %>%
    summarise(count = n(), .groups = "drop") %>%
    filter(count > 1)
}

############ Fills 15-minute gaps in timeseries data (did this manually in 01_prepNEON...)
fill_15min_gaps <- function(df, datetime_col) {
  datetime_sym <- rlang::ensym(datetime_col)
  
  # create full 15-min sequence
  full_times <- tibble(
    !!datetime_sym := seq(
      floor_date(min(df[[rlang::as_string(datetime_sym)]], na.rm = TRUE), unit = "15 mins"),
      ceiling_date(max(df[[rlang::as_string(datetime_sym)]], na.rm = TRUE), unit = "15 mins"),
      by = "15 mins"
    )
  )
  
  # join the original data to the full sequence
  df_filled <- full_times %>%
    left_join(df, by = rlang::as_string(datetime_sym)) %>%
    arrange(!!datetime_col)
  
  return(df_filled)
}


### Use like this: 
# bigc.do.filled <- bigc.do %>%
#   mutate(date = as_date(startDateTime)) %>%
#   group_by(date) %>%
#   group_modify(~ fill_15m_gaps(.x, datetime_col = startDateTime)) %>%
#   ungroup()



############ Summarize NA counts by day

### Returns a df with ONLY rows containing at least one NA 

summarize_NAs_by_day <- function(df, datetime_col, NAcols) {
  datetime_col <- rlang::ensym(datetime_col)
  
  df %>%
    mutate(date = as_date(!!datetime_col)) %>%
    group_by(date) %>%
    summarise(
      n_rows = n(),
      across(all_of(NAcols), ~ sum(is.na(.x)), .names = "n_NA_{.col}"),
      n_NA_total = sum(rowSums(select(cur_data(), all_of(NAcols)) %>% is.na()) > 0),
      .groups = "drop"
    ) %>%
    filter(n_NA_total > 0)   # keep only days with missing values
}




# ### Returns a df with ALL dates (including ones w 0 NAs)
# summarize_NAs_by_day <- function(df, datetime_col, NAcols) {
#   datetime_col <- rlang::ensym(datetime_col)
#   
#   df %>%
#     mutate(date = as_date(!!datetime_col)) %>%
#     group_by(date) %>%
#     summarise(
#       n_rows = n(),
#       across(all_of(NAcols), ~ sum(is.na(.x)), .names = "n_NA_{.col}"),
#       n_NA_total = sum(rowSums(select(cur_data(), all_of(NAcols)) %>% is.na()) > 0),
#       .groups = "drop"
#     )
# }


############ Drop full days with any NA in the given columns
drop_days_with_NAs <- function(df, datetime_col, NAcols) {
  datetime_col <- rlang::ensym(datetime_col)
  
  df %>%
    mutate(date = as_date(!!datetime_col)) %>%
    group_by(date) %>%
    filter(!any(rowSums(select(cur_data(), all_of(NAcols)) %>% is.na()) > 0)) %>%
    ungroup() %>%
    select(-date)
}

############ Replace NA values with the daily mean (for Q)
replace_NAs_with_daily_mean <- function(df, datetime_col, NAcols) {
  datetime_col <- rlang::ensym(datetime_col)
  
  df %>%
    mutate(date = as_date(!!datetime_col)) %>%
    group_by(date) %>%
    mutate(
      across(
        all_of(NAcols),
        ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x),
        .names = "{.col}"
      )
    ) %>%
    ungroup() %>%
    select(-date)
}

############ Check daily observation counts - should be 96/day
check_daily_counts <- function(df, datetime_col, expected_per_day = 96) {
  datetime_col <- rlang::ensym(datetime_col)
  
  df %>%
    mutate(date = as_date(!!datetime_col)) %>%
    count(date, name = "n_obs") %>%
    filter(n_obs != expected_per_day) %>%
    mutate(missing = expected_per_day - n_obs)
}


############ CHECK FOR  incomplete days (< 96 obs per day, or not on 15-min intervals)

find_incomplete_days <- function(df, datetime_col, timestep = "15 min") {
  datetime_col <- rlang::ensym(datetime_col)
  
  df %>%
    mutate(date = as_date(!!datetime_col)) %>%
    group_by(date) %>%
    summarise(
      is_complete = {
        full_seq <- seq(from = as_datetime(first(date)),
                        to   = as_datetime(first(date)) + hours(23) + minutes(45),
                        by   = timestep)
        all(full_seq %in% !!datetime_col) && n() == length(full_seq)
      },
      .groups = "drop"
    ) %>%
    filter(!is_complete) %>%
    pull(date)
}


############ Drop incomplete days (< 96 obs per day, or not on 15-min intervals)
drop_incomplete_days <- function(df, datetime_col, timestep = "15 min") {
  datetime_col <- rlang::ensym(datetime_col)
  
  df %>%
    mutate(date = as_date(!!datetime_col)) %>%
    group_by(date) %>%
    filter({
      # Build expected sequence for this date
      full_seq <- seq(from = as_datetime(first(date)),
                      to   = as_datetime(first(date)) + hours(23) + minutes(45),
                      by   = timestep)
      
      # Keep this date only if datetimes match expected sequence
      all(full_seq %in% !!datetime_col) && n() == length(full_seq)
    }) %>%
    ungroup() %>%
    select(-date)
}

############ calculate barometric pressure (in mb) using elevation - from Bob Hall
# Barometric formula:
# 9.80665 → gravitational acceleration in m/s²
# 0.0289644 → molar mass of dry air in kg/mol
# 8.31447 → universal gas constant in J/(mol·K)
# 273.15+15 → reference temperature = 288.15 K (~15 °C)
# alt is multiplied directly with gravity, so it must be in meters

bpcalc_atm<- function(bpst, alt) {
  bpst*exp((-9.80665*0.0289644*alt)/(8.31447*(273.15+15)))
}


```

## Code for Bobby's DO data

#### Load data

```{r load data for bobby's do}

# FOR BOBBY'S DATA
path <- here("N_uptake_NEON/data/model_output/daily_summary_all.rds")
daily_summary.df <- readRDS(file=path) 


#####  CARI 
cari_do_wy22 <- read_csv(here("N_uptake_NEON/data/DO_data_Hensley/CARI_WY22.csv"))
cari_do_wy23 <- read_csv(here("N_uptake_NEON/data/DO_data_Hensley/CARI_WY23.csv"))
# cari_do_wy24 <- read_csv(here("N_uptake_NEON/data/DO_data_Hensley/CARI_WY24.csv"))


#####  CUPE 
cupe_do_wy22 <- read_csv(here("N_uptake_NEON/data/DO_data_Hensley/CUPE_WY22.csv"))
cupe_do_wy23 <- read_csv(here("N_uptake_NEON/data/DO_data_Hensley/CUPE_WY23.csv"))
# cupe_do_wy24 <- read_csv(here("N_uptake_NEON/data/DO_data_Hensley/CUPE_WY24.csv"))

#####  WLOU 
wlou_do_wy22 <- read_csv(here("N_uptake_NEON/data/DO_data_Hensley/WLOU_WY22.csv"))
wlou_do_wy23 <- read_csv(here("N_uptake_NEON/data/DO_data_Hensley/WLOU_WY23.csv"))
wlou_do_wy24 <- read_csv(here("N_uptake_NEON/data/DO_data_Hensley/WLOU_WY24.csv"))

```

#### Match CARI

```{r match CARI}

# Get object with CARI's yr_jdays 
cari_yrjday <- daily_summary.df %>%
  dplyr::filter(site == "CARI") %>%
  pull(yr_jday) 

# Combine the CARI water year files into 1 file, create a yr_jday column, then select the yr_jdays to match my data
cari_do_all <- bind_rows(cari_do_wy22, cari_do_wy23) %>%
  mutate(Year = year(solar.time), 
         jday = yday(solar.time), 
         jday_pad = str_pad(jday, width=3, pad="0")) %>%
  unite("yr_jday", Year, jday_pad, sep = '_', remove=FALSE) %>%
  select(yr_jday, solar.time, DO.obs, temp.water, discharge) 

cari_do_select <- cari_do_all %>%
  dplyr::filter(yr_jday %in% cari_yrjday)


# the days aren't even: 17569 obs = 116 days + 15 min (1 obs)
#wlou_obsPERday <- wlou_do_select %>% count(yr_jday) # Sept 30, 2022 has 97 obs (not 96) (2022_273)

cari_check <- cari_do_select %>%
  count(yr_jday) %>%
  filter(n != 96)

cari_view <- cari_do_select %>%
  dplyr::filter(yr_jday %in% cari_check$yr_jday)

# one timestep is duplicated (rows 57 and 58): similar values, same timestep. Removing #57
cari_do_select <- cari_do_select %>%
  dplyr::slice(-57)

path <- here("N_uptake_NEON/data/DO_data_model/cari_do.csv")
write_csv(cari_do_select, file=path)


```

#### Match CUPE

```{r match CUPE}

# Get object with CUPE's yr_jdays 
cupe_yrjday <- daily_summary.df %>%
  dplyr::filter(site == "CUPE") %>%
  pull(yr_jday) 

# Combine the CUPE water year files into 1 file, create a yr_jday column, then select the yr_jdays to match my data
cupe_do_all <- bind_rows(cupe_do_wy22, cupe_do_wy23) %>%
  mutate(Year = year(solar.time), 
         jday = yday(solar.time), 
         jday_pad = str_pad(jday, width=3, pad="0")) %>%
  unite("yr_jday", Year, jday_pad, sep = '_', remove=FALSE) %>%
  select(yr_jday, solar.time, DO.obs, temp.water, discharge) 

cupe_do_select <- cupe_do_all %>%
  dplyr::filter(yr_jday %in% cupe_yrjday)

path <- here("N_uptake_NEON/data/DO_data_model/cupe_do.csv")
write_csv(cupe_do_select, file=path)


```

#### Match WLOU

```{r match WLOU}

# Get object with WLOU's yr_jdays 
wlou_yrjday <- daily_summary.df %>%
  dplyr::filter(site == "WLOU") %>%
  pull(yr_jday) 

# Combine the WLOU water year files into 1 file, create a yr_jday column, then select the yr_jdays to match my data
wlou_do_all <- bind_rows(wlou_do_wy22, wlou_do_wy23, wlou_do_wy24) %>%
  mutate(Year = year(solar.time), 
         jday = yday(solar.time), 
         jday_pad = str_pad(jday, width=3, pad="0")) %>%
  unite("yr_jday", Year, jday_pad, sep = '_', remove=FALSE) %>%
  select(yr_jday, solar.time, DO.obs, temp.water, discharge) 

wlou_do_select <- wlou_do_all %>%
  dplyr::filter(yr_jday %in% wlou_yrjday)

# the days aren't even: 11137 obs = 116 days + 15 min (1 obs)
#wlou_obsPERday <- wlou_do_select %>% count(yr_jday) # Sept 30, 2022 has 97 obs (not 96)

wlou_check <- wlou_do_select %>%
  count(yr_jday) %>%
  filter(n != 96)

wlou_view <- wlou_do_select %>%
  dplyr::filter(yr_jday %in% wlou_check$yr_jday)

# one timestep is duplicated (rows 68 and 69): similar values, same timestep. Removing 68
wlou_do_select <- wlou_do_select %>%
  dplyr::slice(-68)

# save the data
path <- here("N_uptake_NEON/data/DO_data_model/wlou_do.csv")
write_csv(wlou_do_select, file=path)


```

#### Explore data - Bobby's DO

```{r explore Bobby's matched DO}

# Explore data: 

wlou.do <- read_csv(here("N_uptake_NEON/data/DO_data_model/wlou_do.csv"))

cupe.do <- read_csv(here("N_uptake_NEON/data/DO_data_model/cupe_do.csv"))

cari.do <- read_csv(here("N_uptake_NEON/data/DO_data_model/cari_do.csv"))



wlou.DOplot <- wlou.do %>% 
  ggplot(aes(x=solar.time, y=DO.obs)) + 
  geom_point() + 
  xlab("Solar time") + ylab("Observed DO") +
  ggtitle("West St. Louis Creek DO") + 
  theme_bw()

ggplotly(wlou.DOplot) %>%
  layout(
  xaxis = list(
    rangeslider = list(visible = TRUE)
    )
  )



cupe.DOplot <- cupe.do %>% 
  ggplot(aes(x=solar.time, y=DO.obs)) + 
  geom_point() + 
  xlab("Solar time") + ylab("Observed DO") +
  ggtitle("Rio Cupeyes DO") + 
  theme_bw()

ggplotly(cupe.DOplot) %>%
  layout(
    xaxis = list(
      rangeslider = list(visible = TRUE)
    )
  )


cari.DOplot <- cari.do %>% 
  ggplot(aes(x=solar.time, y=DO.obs)) + 
  geom_point() + 
  xlab("Solar time") + ylab("Observed DO") +
  ggtitle("Caribou Creek DO") + 
  theme_bw()

ggplotly(cari.DOplot) %>%
  layout(
    xaxis = list(
      rangeslider = list(visible = TRUE)
    )
  )


```

## Code for CLT's DO data

```{r load data for CLT's do}

bigc.do <- read_csv(here("N_uptake_NEON/GPP_model/DO_data_CLT/bigc_do_2224.csv"))
prin.do <- read_csv(here("N_uptake_NEON/GPP_model/DO_data_CLT/prin_do_2224.csv"))
syca.do <- read_csv(here("N_uptake_NEON/GPP_model/DO_data_CLT/syca_do_2224.csv"))


```

### BIGC

#### Select no3-modeled dates

```{r select bigc dates}

############################## BIGC

bigc.list.21 <- c(274, 275, 276, 337, 338, 339, 340)

year_val <- 2021
bigc.dates21 <- julian_to_date(julian_days = bigc.list.21, year_val = year_val)


bigc.list.22 <- c(22, 23, 24, 25, 26, 27, 28, 38, 41, 42, 48, 49, 51, 52, 58, 59, 68, 69, 70, 82, 83, 84, 85, 91, 92, 93, 94, 95, 100, 103, 104, 105, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 147, 148, 155, 160, 170, 195, 196, 204, 205, 206, 221, 222, 230, 231, 232, 233, 234, 237, 238, 239, 244, 247, 251, 252, 257, 258, 259, 266, 267, 268)

year_val <- 2022
bigc.dates22 <- julian_to_date(julian_days = bigc.list.22, year_val = year_val)


bigc.list.23 <- c(33, 34, 41, 43, 44, 46, 47, 48, 49, 50, 51, 91, 92, 96, 101, 291, 292, 293, 294, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 313, 314, 315, 317, 318, 325, 326, 327, 328, 329, 330, 336, 337, 338, 339, 340, 341, 344, 345, 346, 347, 348, 349, 350)

year_val <- 2023
bigc.dates23 <- julian_to_date(julian_days = bigc.list.23, year_val = year_val)


bigc.list.24 <- c(25, 26, 27, 28, 29, 30, 31, 74, 75, 76, 80, 81, 82, 93, 94, 99, 100, 137, 138, 142, 143, 146, 153, 154, 155, 163, 168)

year_val <- 2024
bigc.dates24 <- julian_to_date(julian_days = bigc.list.24, year_val = year_val)

bigc.dates <- bind_rows(bigc.dates21, bigc.dates22, bigc.dates23, bigc.dates24)

```

#### Subset, check, and save the DO data

```{r subset CLT DO bigc}

########## BIGC

## Double-check for gaps in 15-min data timesteps
bigc_gaps <- IDtimegaps(df=bigc.do, datetimecol = startDateTime, difference = 15, units = "mins") # none

# IF NEEDED - Fill the gaps in timeseries steps
# bigc.do <- fill_15min_gaps(df=bigc.do, datetime_col = startDateTime)


# fill NAs with spline, for all numeric cols.
bigc.do <- bigc.do %>%
  mutate(across(where(is.numeric), ~ na.spline(.x, maxgap = 24)))

# Filter down to the modeled dates
bigc.do <- bigc.do %>%
  filter(as_date(startDateTime) %in% bigc.dates$date)

# Check for timeseries gaps on each date - NONE

bigc.gaps <- bigc.do %>%
  mutate(date = as_date(startDateTime)) %>%
  group_by(date) %>%
  group_map(~ IDtimegaps(.x, datetimecol = startDateTime)) %>%
  bind_rows()


# fill timeseries gaps by date
# bigc.do <- bigc.do %>%
#   mutate(date = as_date(startDateTime)) %>%
#   group_by(date) %>%
#   group_modify(~ fill_15m_gaps(.x, datetime_col = startDateTime)) %>%
#   ungroup()

# Check for duplicate timestamps
bigc_dupes <- check_duplicate_timestamps(df=bigc.do, datetime_col = startDateTime) # no duplicates

# Check for NAs
cols <- c("DO.obs", "temp.water", "discharge_cms")

bigc_NAs <- bigc.do %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = cols)


# Remove days with any DO or temp NAs

bigc.do_trimmed <- drop_days_with_NAs(df=bigc.do, datetime_col = startDateTime, NAcols = c("DO.obs", "temp.water"))


# check and fill in Q (can average for that day)
# replace_NAs_with_daily_mean fcn

# Make sure all days are complete days from 00:00 to 23:45  - They are!

bad_days_bigc <- find_incomplete_days(df=bigc.do_trimmed, datetime_col = startDateTime, timestep="15 min") # 15 min is the default

# Remove incomplete days if needed

# bigc.do_trimmed <- drop_incomplete_days(df=bigc.do_trimmed, datetime_col = startDateTime, timestep="15 min")

# final NA check - all 0s

cols <- c("DO.obs", "temp.water", "discharge_cms")
bigcT_NAs <- bigc.do_trimmed %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = cols)

# Are there BP NAs?  YES
BPbigcNAs <- bigc.do_trimmed %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = "barPres_mb")

# calc barometric pressure (bp) using standard bp and site elevation in m
elev_m <- 1131.24
bigc.do_trimmed <- bigc.do_trimmed %>%
  mutate(
    barPres_mb = if_else(
      is.na(barPres_mb),
      bpcalc_atm(bpst=1013, alt=elev_m),   # your function that returns a value
      barPres_mb
    )
  )



# save updated, ready-to-model csv

write_csv(bigc.do_trimmed, here("N_uptake_NEON/GPP_model/DO_data_model/bigc_do.csv"))


```

### PRIN

#### Select no3-modeled dates

```{select prin dates}

############################## PRIN

prin.list.21 <- c(181, 184, 186, 188, 234, 235, 236, 237, 240, 241, 242, 243, 244, 249, 251, 253, 254, 255, 261, 265, 266, 267, 268, 269, 294, 295, 304, 318, 319, 324, 343, 344, 355, 356, 357)

year_val <- 2021
prin.dates21 <- julian_to_date(julian_days = prin.list.21, year_val = year_val)


prin.list.22 <- c(4, 5, 6, 10, 11, 12, 13, 16, 17, 18, 19, 25, 27, 29, 30, 31, 38, 39, 40, 41, 44, 45, 61, 68, 74, 75, 76, 77, 78, 84, 85, 86, 87, 90, 91, 104, 105, 106, 107, 110, 111, 112, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 182, 304, 305, 306, 310)

year_val <- 2022
prin.dates22 <- julian_to_date(julian_days = prin.list.22, year_val = year_val)

prin.list.23 <- c(55, 72, 75, 77, 84, 85, 86, 88, 90, 94, 95, 96, 97, 100, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 115, 121, 122, 123, 124, 125, 126, 137, 138, 139, 140, 141, 142, 143, 146, 147, 148, 149, 150, 151, 152, 328, 329, 337)

year_val <- 2023
prin.dates23 <- julian_to_date(julian_days = prin.list.23, year_val = year_val)

# 2024 not used yet
prin.list.24 <- c(48, 61, 69, 71, 72, 73, 74, 80, 95, 97, 118, 139, 140, 141, 142, 144, 145, 147, 159, 160, 161, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 177, 179)

year_val <- 2024
prin.dates24 <- julian_to_date(julian_days = prin.list.24, year_val = year_val)

prin.dates <- bind_rows(prin.dates21, prin.dates22, prin.dates23, prin.dates24)

```

#### Subset, check, and save the DO data

```{r subset CLT DO prin}
########## PRIN

## Double-check for gaps in 15-min data timesteps
prin_gaps <- IDtimegaps(df=prin.do, datetimecol = startDateTime, difference = 15, units = "mins") # none

# IF NEEDED - Fill the gaps in timeseries steps
# prin.do <- fill_15min_gaps(df=prin.do, datetime_col = startDateTime)


# fill NAs with spline, for all numeric cols.
prin.do <- prin.do %>%
  mutate(across(where(is.numeric), ~ na.spline(.x, maxgap = 24)))

# Filter down to the modeled dates
prin.do <- prin.do %>%
  filter(as_date(startDateTime) %in% prin.dates$date)

# Check for timeseries gaps on each date - NONE

prin.gaps <- prin.do %>%
  mutate(date = as_date(startDateTime)) %>%
  group_by(date) %>%
  group_map(~ IDtimegaps(.x, datetimecol = startDateTime)) %>%
  bind_rows()


# fill timeseries gaps by date
# prin.do <- prin.do %>%
#   mutate(date = as_date(startDateTime)) %>%
#   group_by(date) %>%
#   group_modify(~ fill_15m_gaps(.x, datetime_col = startDateTime)) %>%
#   ungroup()

# Check for duplicate timestamps
prin_dupes <- check_duplicate_timestamps(df=prin.do, datetime_col = startDateTime) # no duplicates

# Check for NAs
cols <- c("DO.obs", "temp.water", "discharge_cms")

prin_NAs <- prin.do %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = cols)


# Remove days with any DO or temp NAs

prin.do_trimmed <- drop_days_with_NAs(df=prin.do, datetime_col = startDateTime, NAcols = c("DO.obs", "temp.water"))

# re-check NAs / ID Q NAs - None left!
prin_NAs <- prin.do_trimmed %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = cols)


# check and fill in Q (can average for that day)
# replace_NAs_with_daily_mean fcn

# Make sure all days are complete days from 00:00 to 23:45  - They are!

bad_days_prin <- find_incomplete_days(df=prin.do_trimmed, datetime_col = startDateTime, timestep="15 min") # 15 min is the default

# Remove incomplete days if needed

# prin.do_trimmed <- drop_incomplete_days(df=prin.do_trimmed, datetime_col = startDateTime, timestep="15 min")

# final NA check - all 0s

cols <- c("DO.obs", "temp.water", "discharge_cms")
prinT_NAs <- prin.do_trimmed %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = cols)


# Are there BP NAs?  NO
BPprinNAs <- prin.do_trimmed %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = "barPres_mb")

# calc barometric pressure (bp) using standard bp and site elevation in m
# elev_m <- 251.34
# prin.do_trimmed <- prin.do_trimmed %>%
#   mutate(
#     barPres_mb = if_else(
#       is.na(barPres_mb),
#       bpcalc_atm(bpst=1013, alt=elev_m),   # your function that returns a value
#       barPres_mb
#     )
#   )


# save updated, ready-to-model csv

write_csv(prin.do_trimmed, here("N_uptake_NEON/GPP_model/DO_data_model/prin_do.csv"))

```

### SYCA

#### Select no3-modeled dates

```{select syca dates}

############################## SYCA

# NO WY22 days in 2021

syca.list.22 <- c(34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 350, 351, 352, 356, 357, 358, 359, 360)

year_val <- 2022
syca.dates22 <- julian_to_date(julian_days = syca.list.22, year_val = year_val)


syca.list.23 <- c(101, 102, 103, 104, 105, 106, 125, 126, 127, 128, 129, 132, 138, 139, 143, 144, 145, 146, 147, 148, 150, 257, 263, 266, 274, 278, 279, 280, 281, 282)

year_val <- 2023
syca.dates23 <- julian_to_date(julian_days = syca.list.23, year_val = year_val)

syca.list.24 <- c(64, 97, 98, 99, 100, 103, 104, 105, 112, 113, 114, 115, 116, 119, 120, 121, 125, 126, 127, 128, 129, 131, 138, 142, 145, 146, 147, 150, 151, 152, 155, 156)

year_val <- 2024
syca.dates24 <- julian_to_date(julian_days = syca.list.24, year_val = year_val)

syca.dates <- bind_rows(syca.dates22, syca.dates23, syca.dates24)

```

#### Subset, check, and save the DO data

```{r subset CLT DO syca}
########## SYCA

## Double-check for gaps in 15-min data timesteps
syca_gaps <- IDtimegaps(df=syca.do, datetimecol = startDateTime, difference = 15, units = "mins") # none

# IF NEEDED - Fill the gaps in timeseries steps
# syca.do <- fill_15min_gaps(df=syca.do, datetime_col = startDateTime)


# fill NAs with spline, for all numeric cols.
syca.do <- syca.do %>%
  mutate(across(where(is.numeric), ~ na.spline(.x, maxgap = 24)))

# Filter down to the modeled dates
syca.do <- syca.do %>%
  filter(as_date(startDateTime) %in% syca.dates$date)

# Check for timeseries gaps on each date - NONE

syca.gaps <- syca.do %>%
  mutate(date = as_date(startDateTime)) %>%
  group_by(date) %>%
  group_map(~ IDtimegaps(.x, datetimecol = startDateTime)) %>%
  bind_rows()


# fill timeseries gaps by date
# syca.do <- syca.do %>%
#   mutate(date = as_date(startDateTime)) %>%
#   group_by(date) %>%
#   group_modify(~ fill_15m_gaps(.x, datetime_col = startDateTime)) %>%
#   ungroup()

# Check for duplicate timestamps
syca_dupes <- check_duplicate_timestamps(df=syca.do, datetime_col = startDateTime) # no duplicates

# Check for NAs
cols <- c("DO.obs", "temp.water", "discharge_cms")

syca_NAs <- syca.do %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = cols)


# Remove days with any DO or temp NAs

syca.do_trimmed <- drop_days_with_NAs(df=syca.do, datetime_col = startDateTime, NAcols = c("DO.obs", "temp.water"))

# Re-check NAs/ check only-Q NAs - none left!
syca_NAs <- syca.do_trimmed %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = cols)

# check and fill in Q (can average for that day)
# replace_NAs_with_daily_mean fcn

# Make sure all days are complete days from 00:00 to 23:45  - They are!

bad_days_syca <- find_incomplete_days(df=syca.do_trimmed, datetime_col = startDateTime, timestep="15 min") # 15 min is the default

# Remove incomplete days if needed

# syca.do_trimmed <- drop_incomplete_days(df=syca.do_trimmed, datetime_col = startDateTime, timestep="15 min")

# final NA check - all 0s

cols <- c("DO.obs", "temp.water", "discharge_cms")
sycaT_NAs <- syca.do_trimmed %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = cols)

# Are there BP NAs?  YES
BPsycaNAs <- syca.do_trimmed %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = "barPres_mb")

# calc barometric pressure (bp) using standard bp and site elevation in m
elev_m <- 643
syca.do_trimmed <- syca.do_trimmed %>%
  mutate(
    barPres_mb = if_else(
      is.na(barPres_mb),
      bpcalc_atm(bpst=1013, alt=elev_m),   # your function that returns a value
      barPres_mb
    )
  )



ggplot(syca.do_trimmed, aes(x=startDateTime, y=discharge_cms)) + 
  geom_point() + 
  theme_bw()



# Check for NAs in the Q column
Qnas <- syca.do_trimmed %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = "discharge_lps")

# Check for no flow days and subzero observations
noflow <- syca.do_trimmed %>%
  dplyr::filter(discharge_cms <= 0)

hist(syca.do_trimmed$discharge_cms)

# SYCA has a lot of 0 and subzero observations at the end of May/ early June 2024. Remove these observations, and replace the NAs with the daily average. If needed, remove days w. zero flow. 

# Hydro geom coords from Aho et al 2024
sitec<-0.22
sitef<-0.36

syca.do_trimmed <- syca.do_trimmed %>%
  dplyr::mutate(discharge_cms = ifelse(discharge_cms <= 0, NA, discharge_cms), 
                discharge_lps = ifelse(discharge_lps <= 0, NA, discharge_lps)) %>%
  replace_NAs_with_daily_mean(datetime_col = startDateTime, NAcols = c("discharge_cms","discharge_lps")) %>%
  mutate(depth = streamMetabolizer::calc_depth(discharge_cms,c=sitec,f=sitef))
  
# NO more zero-flow observaions. 

# ANOTHER VERSION THAT APPLIES MUTATE IN 1 LINE: 

# syca.do_trimmed <- syca.do_trimmed %>%
#   mutate(
#     across(
#       c(discharge_cms, discharge_lps),
#       ~ ifelse(.x < 0, NA, .x)
#     )
#   ) %>%
#   replace_NAs_with_daily_mean(
#     datetime_col = startDateTime,
#     NAcols = c("discharge_cms", "discharge_lps")
#   ) %>%
#   mutate(depth = streamMetabolizer::calc_depth(discharge_cms,c=sitec,f=sitef))



# save updated, ready-to-model csv

write_csv(syca.do_trimmed, here("N_uptake_NEON/GPP_model/DO_data_model/syca_do.csv"))

```

### re-load syca, Q still negative... 
```{r revise syca Q}

syca.do_trimmed <- read_csv(here("N_uptake_NEON/GPP_model/DO_data_model/syca_do.csv"))

Yr = 2024
Mo = 5:6

syca_qplot <- syca.do_trimmed %>%
  dplyr::filter(year(startDateTime) == Yr) %>%
  dplyr::filter(month(startDateTime) == Mo) %>%
  ggplot(aes(x=startDateTime, y=discharge_cms)) + 
  geom_point() + 
  theme_bw()

ggplotly(syca_qplot)


# Check for NAs in the Q column - none
Qnas <- syca.do_trimmed %>%
  summarize_NAs_by_day(datetime_col=startDateTime, NAcols = "discharge_lps")

# Check for no flow days and subzero observations
noflow <- syca.do_trimmed %>%
  dplyr::filter(discharge_lps <= 0)

hist(syca.do_trimmed$discharge_lps)

# NO zero-flow observaions. Not sure what was going on for Bob, but re-shared this df. 


```


~/Documents/R_working/Modelscape/space-time-rivers/N_uptake_NEON/GPP_model/DO_data_model
