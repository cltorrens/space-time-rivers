---
title: "02_select and visualize NEON data for model"
author: "Christa Torrens"
format: html
editor: visual
---

## Selecting and visualizing NEON NO3 data for the stan model

The purpose of this script is to load the NEON nitrate Rdata, separate it by site, and then identify at least 100 complete "good" days per site, to inform the model. "Good" days = complete days where the diel signal of autotrophic nitrate uptake is clearly visible and not affected by, e.g., hydrology or other physical processes. When there are gaps in the data, we will fill up to 2-hour gaps using the zoo() package, linear method. If there are gaps > 2 hours, OR multiple smaller gaps, the day will be discarded. 

First load the required packages

### Loading required packages

Add these before knitting:


```{r loading packages}
#| warning: false
#| output: false

# load packages
library(scales)
library(tidyverse) # includes magrittr, as part of dplyr
library(lubridate)
library(rstan)
library(tidybayes)
library(GGally) # adds functions to ggplot()
# library(shinystan)
library(zoo)
library(neonUtilities)
library(ggpubr) #publication-ready graphs
library(brms)
library(here) # allows project-based file paths
library(pracma) # for 1 type of light AUC calcs
library(dygraphs) # creates interactive ts graphs

# make sure the working directory is set to the project level
setwd(here())

```


### Functions
Define any functions

```{r - functions}

selectRdata <- function(data, site, tz, span) {
  data %>%
  filter(siteID == site) %>%
  mutate(local_datetime = with_tz(startDateTime, tzone=tz),
         Jday = yday(local_datetime),
         model_datetime = local_datetime - hours(4), 
         model_jday = yday(model_datetime)) %>%
   filter(year(local_datetime) %in% span)
}

# data = the name of the NO3 Rdata 
# site = the site ID from the Rdata, in quotes " "
# tz = the desired time zone for the site, in quotes " " 
# span = the year or years to filter for in the Rdata. 

# NB: The current dataset (as of Feb 2025) runs from Dec 2016-Dec 2024; QA/QC'd data w good discharge = Jan 2021-June 2023


```

Then load the NEON NO3 Rdata

### Loading the NEON Rdata

```{r load NEON NO3 data}
#| output: false
#| message: false

load(here("N_uptake_NEON/data/neon_data_derived/no3_dataset.Rdata")) 
#no3_data_sensor is the object name

# View(no3_data_sensor)
# unique(no3_data_sensor$siteID)   

# CURRENT DATA FILE:
# ARIK, BIGC, BLDE, BLUE, BLWA, CARI, COMO, CUPE, FLNT, GUIL, HOPB, KING, LECO, MART, OKSR, POSE, PRIN, REDB, SYCA, TECR, WALK, WLOU

# ORIGINAL FOUR:"KING" "WALK" "BIGC" "CARI"

# Not pulled:Lewis Run [LEWI], Clarke, VA; Mayfield Creek [MAYF], Bibb, AL; McDiffett Creek [MCDI], Wabaunsee, KS; McRae Creek [MCRA], Linn, OR; Tombigbee River [TOMB], Choctaw, AL

```


### Viewing, selecting, and saving data by site

For each site:
* Filter by site ID and set appropriate time zone for local time (default for NEON is UTC)

* Use OlsonNames() to check for accurate timezone names: "US/Eastern" "US/Central" "US/Mountain" "US/Pacific" "US/Alaska" "US/Arizona" "US/Hawaii" "America/Puerto_Rico"

* Select the days to use in the model: create 1 dataframe per site, across mulitple years. Aim for at least 100 full days with clear diel no3 swings. It may be easiest to explore and select data by year... Try it by site and see how it goes. 

* Per Bobby Hensley, data from 2021 on will have the best discharge data (Q and O2 data prior to 2021 are 'shoddy'). Currently focusing on 2021-2023 data. 

* 2024 Q data has been QA/QC'd as of March 2025

* NB: for 2024 NO3 data, Jan-Jun data are QA/QC'd and July-Dec data are still provisional (as of 3/06/2025)

Site descriptions: 
https://www.neonscience.org/field-sites/explore-field-sites

Individual sites follow this pattern: 
BIGC: https://www.neonscience.org/field-sites/bigc 
CARI: https://www.neonscience.org/field-sites/cari 
KING: https://www.neonscience.org/field-sites/king 
WALK: https://www.neonscience.org/field-sites/walk
(etc.)


#### Arikaree River, Yuma, CO
nitrate sensor lat-long: 39.758356	-102.44859
reference elevation (m): 1178.7 

```{r ARIK}

##### Create object for ARIK => Arikaree River, Yuma, CO

# arik.df <- no3_data_sensor %>%
#   filter(siteID == "ARIK") %>%
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Mountain"),
#          Jday = yday(local_datetime)) %>%
#    filter(year(local_datetime) == 2021:2023)
# 

arik.df <- selectRdata(data=no3_data_sensor, site="ARIK", tz="US/Mountain", span=2021:2024)


##### Visualize, select and clean ARIK data

yr <- 2021
# mo <- 1:2

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
arik.df %>%
  filter(year(local_datetime) == yr) %>%
  # filter(month(local_datetime) == mo) %>%
  # ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,4) +
  # facet_wrap(~Jday) + 
  ggtitle("ARIK, 2023") +
  theme_bw()


##### ID Jdays to save for model




##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/arik_df.csv")
write_csv(arik.df, path)

```




#### Upper Big Creek, Fresno, CA
nitrate sensor lat-long: 37.057672	-119.255375	 
elevation (m): 1131.24
tz="US/Pacific"

"overhang" sensor location: lat-long: 37.057515	-119.255046	elevation (m): 1127.43

###### Load data

```{r - BIGC}
#| output: false
#| message: false

##### Create object for BIGC => West St Louis Creek, Grand, CO

bigc.df <- selectRdata(data=no3_data_sensor, site="BIGC", tz="US/Pacific", span=2021:2024)

bigc_light.df <- read_csv(here("N_uptake_NEON/data/NSRDB_lightdata_clean/BIGC_satlight_all.csv")) %>%
  mutate(local_datetime = force_tz(local_datetime, tzone="US/Pacific"))

# make sure the tz assigned correctly - otherwise the join doesn't work correctly, because R/ tidyverse assumes the assigned tz is correct (so, offsets the join *as if* it needed to match different tzs vs. the same one)
tz(bigc_light.df$local_datetime)
tz(bigc.df$local_datetime)
# Note that model_datetime is assigned the same tz as local_datetime

```

###### Visualize, select and clean BIGC data
```{r - BIGC visualize data, select days, add sat. light}

##### Visualize data and select days

yr <- 2024
mo <- 6
plottitle <- "BIGC, Jun 2024"

quartz(width=6.5, height=6.5)
#quartz(width=10, height=4)
bigc.df %>%
  filter(year(local_datetime) == yr) %>%
  filter(month(local_datetime) == mo) %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,3) +
  facet_wrap(~Jday) + 
  ggtitle(plottitle) +
  theme_bw()


# bigc_dy <- bigc.df %>%
  # select(local_datetime, surfWaterNitrateMean)

# Dygraph for interactive chart w slider
dygraph(data=bigc_dy) %>%
  dyRangeSelector()


##### ID MODEL_JDAYS to save for model - this makes later steps work better (determining daily light, for example). ID the actual (local time) days, but code for model jdays
#     visual check to select 120+ complete-appearing days to model from the 2.5 year dataset
#     BIGC: COMMENTS?


# ID Jdays: 

##    2021: 56, 57, 58, 59, 60, 61, 63, 64, 65, 128, 129, 130, 133, 139, 140, 149, 150, 151, 152, 153, [154], 155, 156, 157, 158, 159, 160, 175, 176, [177], 178, [179], 197, 198, 202, 203, 204, 225, 226, 227, 228, 229, 230, 243, 245, 246, 247, 248, 249, 250, 251, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 274, 275, 276, [337,338, 339, 340 - ampl. may be too low]

##    2022:  22, 23, 24, 25, 26, 27, 28, 38, 41, 42, 48, 49, 51, 52, 58, 59, 68, 69, 70, 71, 72, 82, 83, 84, 85, 91, 92, 93, 94, 95, 100, 103, 104, 105, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 147, 148, 155, 160, 170, 195, 196, 204, 205, 206, 221, 222, 230, 231, 232, 233, 234, 237, 238, 239, 244, 247, 251, 252, 257, 258, 259, 266, 267, 268 

##    2023: 33, 34, 41, 43, 44, 46, 47, 48, 49, 50, 51, 91, 92, 96, 101, 291, 292, 293, 294, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 313, 314, 315, 317, 318, 325, 326, 327, 328, 329, 330, 336, 337, 338, 339, 340, 341, 344, 345, 346, 347, 348, 349, 350

# We may not use 2024, but it's easiest to ID good days Jan-June now
##    2024: 24, 25, 26, 27, 28, 29, 30, 31, 74, 75, 76, 80, 81, 82, 93, 94, 99, 100, 137, 138, 142, 143, 146, 153, 154, 155, 163, 168


# create objects from selected jdays for each year
list.21 <- c(56, 57, 58, 59, 60, 61, 63, 64, 65, 128, 129, 130, 133, 139, 140, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 175, 176, 177, 178, 179, 197, 198, 202, 203, 204, 225, 226, 227, 228, 229, 230, 243, 245, 246, 247, 248, 249, 250, 251, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 274, 275, 276, 337, 338, 339, 340)

list.22 <- c(22, 23, 24, 25, 26, 27, 28, 38, 41, 42, 48, 49, 51, 52, 58, 59, 68, 69, 70, 82, 83, 84, 85, 91, 92, 93, 94, 95, 100, 103, 104, 105, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 147, 148, 155, 160, 170, 195, 196, 204, 205, 206, 221, 222, 230, 231, 232, 233, 234, 237, 238, 239, 244, 247, 251, 252, 257, 258, 259, 266, 267, 268)

list.23 <- c(33, 34, 41, 43, 44, 46, 47, 48, 49, 50, 51, 91, 92, 96, 101, 291, 292, 293, 294, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 313, 314, 315, 317, 318, 325, 326, 327, 328, 329, 330, 336, 337, 338, 339, 340, 341, 344, 345, 346, 347, 348, 349, 350)


# Use the objects to create dataframes of the selected *model* jdays (ensures complete 24-h model days, i.e. from 4a-3:59a)
bigc.df.21 <- bigc.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(model_jday %in% list.21)
  
bigc.df.22 <- bigc.df %>%  
  filter(year(local_datetime) == 2022) %>%
  filter(model_jday %in% list.22)

bigc.df.23 <- bigc.df %>%  
  filter(year(local_datetime) == 2023) %>%
  filter(model_jday %in% list.23)

# Check for complete days: should have 96 obs/ day
bigc.df.21 %>% count(model_jday) #70
bigc.df.22 %>% count(model_jday) #86, after removing 71 and 72 (92 obs each)
bigc.df.23 %>% count(model_jday) #53, after removing 308 (100 obs)

# check for days where obs != 96 (often leap years, or some sensor irregularity)

remove.21 <- bigc.df.21 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.22 <- bigc.df.22 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.23 <- bigc.df.23 %>% 
  count(model_jday) %>%
  filter(n != 96)

# View days with alternate n values
# bigc22_310 <- bigc.df.22 %>% filter(model_jday == 310)
# bigc21_310 <- bigc.df.22 %>% filter(model_jday == 310)
# View(bigc73)
# View(bigc310)
# 
# bigc310 <- bigc.df.22 %>% filter(Jday == 310)
# View(bigc310)


##### Combine selected days into 1 df, select columns 
bigc.df.2123 <- bind_rows(bigc.df.21, bigc.df.22, bigc.df.23) %>%
  mutate(Year = year(model_datetime)) %>% 
  rename(UTC_startDateTime=startDateTime) %>%
  unite("yr_jday", Year:model_jday, remove=FALSE) %>%
  dplyr::select(siteID, yr_jday, model_datetime, surfWaterNitrateMean, surfWaterNitrateVariance, surfWaterNitrateStdErMean, rangeFailQM, UTC_startDateTime, local_datetime, Jday, model_jday, Year, domainID)

# check the # of days by dividing bigc.df.2123 observations by 96 
# 209 for bigc


##### Combine df with sat_light data for the selected model days
bigc.wlight.df.2123 <- left_join(x=bigc.df.2123,
                                 y=bigc_light.df, 
                                 by = "local_datetime") 


View(bigc.wlight.df.2123)

# check the 1st day in the df to make sure light joined correctly
jday21_56 <- bigc_light.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(month(local_datetime) == 2) %>%
  filter(day(local_datetime) == 25)


# save to keep progress
write_csv(bigc.wlight.df.2123,file=here("N_uptake_NEON/data/neon_data_joined/BIGC_wlight_2123_joined.csv"))

```

###### Fill any gaps in no3 data
```{r fill gaps}

##############################  Reload data if needed  #########################
# bigc.wlight.df.2123 <- read_csv(here("N_uptake_NEON/data/neon_data_joined/WALK_wlight_2123_joined.csv")) %>%
#   mutate(local_datetime = force_tz(local_datetime, tzone="US/Mountain"))


##############################  Where are the NAs?  ############################

no3NA <- which(is.na(bigc.wlight.df.2123$surfWaterNitrateMean))
no3NA

# 13 occurrences: 595   596   877  3191  3523  3766  8476 15953 15968 15969 17308 17601 19377

# When do these occur? (ID any particularly bad days to remove)
NAdays <- bigc.wlight.df.2123 %>%
  filter(is.na(surfWaterNitrateMean)) %>%   # Filter rows where 'no3mean' is NA
  select(yr_jday)                  # Select the 'Jday' column for those rows

NAdays  # 2021: 63(2), 128, 198, 204, 227
        # 2022: 70
        # 2023: 51(3), 304, 307, 342

# all are missing < 1h of data, which is fine for a gap fill 

lightNA <- which(is.na(bigc.wlight.df.2123$GHI_wm2))
lightNA  
# none, whew

##############################   SMALL GAPS  #####################################

# For smaller NA chunks, (8 or fewer timesteps (<= 2 hours)?) , we will interpolate using zoo()

#    first see where in the curve this gap falls: 
span <- 15945:15985
plot(bigc.wlight.df.2123$local_datetime[span], bigc.wlight.df.2123$surfWaterNitrateMean[span], type = 'l')  #OK, linear seems reasonable

# Then fill
maxgap <- 8  # set the maximum gap for zoo() to fill
bigc.wlight.df.2123$surfWaterNitrateMean <- na.approx(bigc.wlight.df.2123$surfWaterNitrateMean, maxgap = maxgap)  # maxgap = max # of NAs to fill



# re-check NAs
which(is.na(bigc.wlight.df.2123$surfWaterNitrateMean)) 

# no NAs remain

N_e <- mean(bigc.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)  #1.811533
N_sd <- sd(bigc.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)   #0.7407876

## Select hourly no3 measurements to match what we've been doing  => CHANGE THIS LATER
bigc.wlight.df.h <- bigc.wlight.df.2123 %>%
  filter(minute(local_datetime) == 0) 
# Mean of selected dataset
N_e <- mean(bigc.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE) #1.811583
N_sd <- sd(bigc.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE)  #0.7408184



```


###### Visualize cleaned data

```{r - visualize BIGC clean}

# jday distribution plot (do we have even coverage across a generic year?)
hist(bigc.wlight.df.h$model_jday)



```



###### Save cleaned site df
```{r - Save cleaned df}

############################  Save cleaned site DF ###############################

path <- here("N_uptake_NEON/data/neon_data_clean/bigc_clean.csv")
path_h <- here("N_uptake_NEON/data/neon_data_clean/bigc_hourly_clean.csv")
write_csv(bigc.wlight.df.2123, path)
write_csv(bigc.wlight.df.h, path_h)


##### Reload data-in-progress as needed
# bigc.wlight.df.2122 <- read_csv(path)
# bigc.wlight.df.h <- read_csv(path_h)

```



#### Blacktail Deer Creek, Park, WY
nitrate sensor lat-long: 44.953606	-110.589396	
elevation (m): 2023.27
tz="US/Mountain"

###### Load data

```{r - BLDE}
#| output: false
#| message: false

##### Create object for BLDE => West St Louis Creek, Grand, CO

blde.df <- selectRdata(data=no3_data_sensor, site="BLDE", tz="US/Mountain", span=2021:2024)

blde_light.df <- read_csv(here("N_uptake_NEON/data/NSRDB_lightdata_clean/BLDE_satlight_all.csv")) %>%
  mutate(local_datetime = force_tz(local_datetime, tzone="US/Mountain"))

# make sure the tz assigned correctly - otherwise the join doesn't work correctly, because R/ tidyverse assumes the assigned tz is correct (so, offsets the join *as if* it needed to match different tzs vs. the same one)
tz(blde_light.df$local_datetime)
tz(blde.df$local_datetime)
# Note that model_datetime is assigned the same tz as local_datetime

```

###### Visualize, select and clean BLDE data
```{r - BLDE visualize data, select days, add sat. light}

##### Visualize data and select days

yr <- 2024
mo <- 6
plottitle <- "BLDE, Jun 2024"

quartz(width=6.5, height=6.5)
#quartz(width=10, height=4)
blde.df %>%
  filter(year(local_datetime) == yr) %>%
  filter(month(local_datetime) == mo) %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,3) +
  facet_wrap(~Jday) + 
  ggtitle(plottitle) +
  theme_bw()


# blde_dy <- blde.df %>%
  # select(local_datetime, surfWaterNitrateMean)

# Dygraph for interactive chart w slider
dygraph(data=blde_dy) %>%
  dyRangeSelector()


##### ID MODEL_JDAYS to save for model - this makes later steps work better (determining daily light, for example). ID the actual (local time) days, but code for model jdays
#     visual check to select 120+ complete-appearing days to model from the 2.5 year dataset
#     BLDE: COMMENTS?


# ID Jdays: 

##    2021: 56, 57, 58, 59, 60, 61, 63, 64, 65, 128, 129, 130, 133, 139, 140, 149, 150, 151, 152, 153, [154], 155, 156, 157, 158, 159, 160, 175, 176, [177], 178, [179], 197, 198, 202, 203, 204, 225, 226, 227, 228, 229, 230, 243, 245, 246, 247, 248, 249, 250, 251, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 274, 275, 276, [337,338, 339, 340 - ampl. may be too low]

##    2022:  22, 23, 24, 25, 26, 27, 28, 38, 41, 42, 48, 49, 51, 52, 58, 59, 68, 69, 70, 71, 72, 82, 83, 84, 85, 91, 92, 93, 94, 95, 100, 103, 104, 105, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 147, 148, 155, 160, 170, 195, 196, 204, 205, 206, 221, 222, 230, 231, 232, 233, 234, 237, 238, 239, 244, 247, 251, 252, 257, 258, 259, 266, 267, 268 

##    2023: 33, 34, 41, 43, 44, 46, 47, 48, 49, 50, 51, 91, 92, 96, 101, 291, 292, 293, 294, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 313, 314, 315, 317, 318, 325, 326, 327, 328, 329, 330, 336, 337, 338, 339, 340, 341, 344, 345, 346, 347, 348, 349, 350

# We may not use 2024, but it's easiest to ID good days Jan-June now
##    2024: 24, 25, 26, 27, 28, 29, 30, 31, 74, 75, 76, 80, 81, 82, 93, 94, 99, 100, 137, 138, 142, 143, 146, 153, 154, 155, 163, 168


# create objects from selected jdays for each year
list.21 <- c(56, 57, 58, 59, 60, 61, 63, 64, 65, 128, 129, 130, 133, 139, 140, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 175, 176, 177, 178, 179, 197, 198, 202, 203, 204, 225, 226, 227, 228, 229, 230, 243, 245, 246, 247, 248, 249, 250, 251, 257, 258, 259, 260, 261, 262, 263, 264, 267, 268, 269, 270, 274, 275, 276, 337, 338, 339, 340)

list.22 <- c(22, 23, 24, 25, 26, 27, 28, 38, 41, 42, 48, 49, 51, 52, 58, 59, 68, 69, 70, 82, 83, 84, 85, 91, 92, 93, 94, 95, 100, 103, 104, 105, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 147, 148, 155, 160, 170, 195, 196, 204, 205, 206, 221, 222, 230, 231, 232, 233, 234, 237, 238, 239, 244, 247, 251, 252, 257, 258, 259, 266, 267, 268)

list.23 <- c(33, 34, 41, 43, 44, 46, 47, 48, 49, 50, 51, 91, 92, 96, 101, 291, 292, 293, 294, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 313, 314, 315, 317, 318, 325, 326, 327, 328, 329, 330, 336, 337, 338, 339, 340, 341, 344, 345, 346, 347, 348, 349, 350)


# Use the objects to create dataframes of the selected *model* jdays (ensures complete 24-h model days, i.e. from 4a-3:59a)
blde.df.21 <- blde.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(model_jday %in% list.21)
  
blde.df.22 <- blde.df %>%  
  filter(year(local_datetime) == 2022) %>%
  filter(model_jday %in% list.22)

blde.df.23 <- blde.df %>%  
  filter(year(local_datetime) == 2023) %>%
  filter(model_jday %in% list.23)

# Check for complete days: should have 96 obs/ day
blde.df.21 %>% count(model_jday) #70
blde.df.22 %>% count(model_jday) #86, after removing 71 and 72 (92 obs each)
blde.df.23 %>% count(model_jday) #53, after removing 308 (100 obs)

# check for days where obs != 96 (often leap years, or some sensor irregularity)

remove.21 <- blde.df.21 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.22 <- blde.df.22 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.23 <- blde.df.23 %>% 
  count(model_jday) %>%
  filter(n != 96)

# View days with alternate n values
# blde22_310 <- blde.df.22 %>% filter(model_jday == 310)
# blde21_310 <- blde.df.22 %>% filter(model_jday == 310)
# View(blde73)
# View(blde310)
# 
# blde310 <- blde.df.22 %>% filter(Jday == 310)
# View(blde310)


##### Combine selected days into 1 df, select columns 
blde.df.2123 <- bind_rows(blde.df.21, blde.df.22, blde.df.23) %>%
  mutate(Year = year(model_datetime)) %>% 
  rename(UTC_startDateTime=startDateTime) %>%
  unite("yr_jday", Year:model_jday, remove=FALSE) %>%
  dplyr::select(siteID, yr_jday, model_datetime, surfWaterNitrateMean, surfWaterNitrateVariance, surfWaterNitrateStdErMean, rangeFailQM, UTC_startDateTime, local_datetime, Jday, model_jday, Year, domainID)

# check the # of days by dividing blde.df.2123 observations by 96 
# 209 for blde


##### Combine df with sat_light data for the selected model days
blde.wlight.df.2123 <- left_join(x=blde.df.2123,
                                 y=blde_light.df, 
                                 by = "local_datetime") 


View(blde.wlight.df.2123)

# check the 1st day in the df to make sure light joined correctly
jday21_56 <- blde_light.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(month(local_datetime) == 2) %>%
  filter(day(local_datetime) == 25)


# save to keep progress
write_csv(blde.wlight.df.2123,file=here("N_uptake_NEON/data/neon_data_joined/BLDE_wlight_2123_joined.csv"))

```

###### Fill any gaps in no3 data
```{r fill gaps}

##############################  Reload data if needed  #########################
# blde.wlight.df.2123 <- read_csv(here("N_uptake_NEON/data/neon_data_joined/WALK_wlight_2123_joined.csv")) %>%
#   mutate(local_datetime = force_tz(local_datetime, tzone="US/Mountain"))


##############################  Where are the NAs?  ############################

no3NA <- which(is.na(blde.wlight.df.2123$surfWaterNitrateMean))
no3NA

# 13 occurrences: 595   596   877  3191  3523  3766  8476 15953 15968 15969 17308 17601 19377

# When do these occur? (ID any particularly bad days to remove)
NAdays <- blde.wlight.df.2123 %>%
  filter(is.na(surfWaterNitrateMean)) %>%   # Filter rows where 'no3mean' is NA
  select(yr_jday)                  # Select the 'Jday' column for those rows

NAdays  # 2021: 63(2), 128, 198, 204, 227
        # 2022: 70
        # 2023: 51(3), 304, 307, 342

# all are missing < 1h of data, which is fine for a gap fill 

lightNA <- which(is.na(blde.wlight.df.2123$GHI_wm2))
lightNA  
# none, whew

##############################   SMALL GAPS  #####################################

# For smaller NA chunks, (8 or fewer timesteps (<= 2 hours)?) , we will interpolate using zoo()

#    first see where in the curve this gap falls: 
span <- 15945:15985
plot(blde.wlight.df.2123$local_datetime[span], blde.wlight.df.2123$surfWaterNitrateMean[span], type = 'l')  #OK, linear seems reasonable

# Then fill
maxgap <- 8  # set the maximum gap for zoo() to fill
blde.wlight.df.2123$surfWaterNitrateMean <- na.approx(blde.wlight.df.2123$surfWaterNitrateMean, maxgap = maxgap)  # maxgap = max # of NAs to fill



# re-check NAs
which(is.na(blde.wlight.df.2123$surfWaterNitrateMean)) 

# no NAs remain

N_e <- mean(blde.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)  #1.811533
N_sd <- sd(blde.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)   #0.7407876

## Select hourly no3 measurements to match what we've been doing  => CHANGE THIS LATER
blde.wlight.df.h <- blde.wlight.df.2123 %>%
  filter(minute(local_datetime) == 0) 
# Mean of selected dataset
N_e <- mean(blde.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE) #1.811583
N_sd <- sd(blde.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE)  #0.7408184



```


###### Visualize cleaned data

```{r - visualize BLDE clean}

# jday distribution plot (do we have even coverage across a generic year?)
hist(blde.wlight.df.h$model_jday)



```



###### Save cleaned site df
```{r - Save cleaned df}

############################  Save cleaned site DF ###############################

path <- here("N_uptake_NEON/data/neon_data_clean/blde_clean.csv")
path_h <- here("N_uptake_NEON/data/neon_data_clean/blde_hourly_clean.csv")
write_csv(blde.wlight.df.2123, path)
write_csv(blde.wlight.df.h, path_h)


##### Reload data-in-progress as needed
# blde.wlight.df.2122 <- read_csv(path)
# blde.wlight.df.h <- read_csv(path_h)

```



#### Blue River, Johnston, OK
nitrate sensor lat-long: 34.448448	-96.622796	** overhang sensor - the other sensor info says 'not used'
elevation (m): 288.31
tz="US/Central"


```{r - BLUE}

##### Create object for BLUE => Blue River, Johnston, OK

blue.df <- selectRdata(data=no3_data_sensor, site="BLUE", tz="US/Central", span=2021:2024)


# blue.df <- no3_data_sensor %>%
#   filter(siteID == "BLUE") %>%
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Central"),
#          Jday = yday(local_datetime)) %>%
#   filter(year(local_datetime) == 2021:2023)


##### Visualize, select and clean BLUE data

yr <- 2023
# mo <- 1:2

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
blue.df %>%
  filter(year(local_datetime) == yr) %>%
  # filter(month(local_datetime) == mo) %>%
  # ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,4) +
  # facet_wrap(~Jday) + 
  ggtitle("BLUE, 2023") +
  theme_bw()

##### ID Jdays to save for model




##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/blue_df.csv")
write_csv(blue.df, path)

```

#### Black Warrior River, Greene, AL
nitrate sensor lat-long: 32.542478	-87.797972	    ** sensor buoy (river)
elevation (m): 23
tz="US/Central"


```{r - BLWA}

##### Create object for BLWA => Black Warrior River, Greene, AL

blwa.df <- selectRdata(data=no3_data_sensor, site="BLWA", tz="US/Central", span=2021:2024)


# blwa.df <- no3_data_sensor %>%
#   filter(siteID == "BLWA") %>%
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Central"),
#          Jday = yday(local_datetime)) %>%
#    filter(year(local_datetime) == 2021:2023)


##### Visualize, select and clean BLWA data

yr <- 2023
# mo <- 1:2

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
blwa.df %>%
  filter(year(local_datetime) == yr) %>%
  # filter(month(local_datetime) == mo) %>%
  # ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,4) +
  # facet_wrap(~Jday) + 
  ggtitle("BLWA, 2023") +
  theme_bw()


##### ID Jdays to save for model




##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/blwa_df.csv")
write_csv(blwa.df, path)

```


#### Caribou Creek, Fairbanks North Star, AK
nitrate sensor lat-long: 65.153076	-147.502004	
elevation (m): 225.41
tz="US/Alaska"

###### Load data

```{r - CARI}
#| output: false
#| message: false

##### Create object for CARI => West St Louis Creek, Grand, CO

cari.df <- selectRdata(data=no3_data_sensor, site="CARI", tz="US/Alaska", span=2021:2024) %>%
  mutate(model_datetime = local_datetime - hours(2),  # adjusting model dates because #midsummer days start 2:45a, end 1a the next day; 2-hour offset ensures the start of the day is in darkness
         model_jday = yday(model_datetime))

# needed to reset Jday and model_jday values for some reason; they were not matching the actual jday and model jday.
cari_light.df <- read_csv(here("N_uptake_NEON/data/NSRDB_lightdata_clean/CARI_neonlight30m_all.csv")) %>%
  mutate(local_datetime = with_tz(endDateTime, tz="US/Alaska"),
         Jday = yday(local_datetime),
         model_datetime = local_datetime - hours(2),
         model_jday = yday(model_datetime)) %>%
  select(siteID, local_datetime, Jday, model_datetime, model_jday, PARMean, PARMinimum, PARMaximum, startDateTime, endDateTime, domainID)

# make sure the tz assigned correctly - otherwise the join doesn't work correctly, because R/ tidyverse assumes the assigned tz is correct (so, offsets the join *as if* it needed to match different tzs vs. the same one)
tz(cari_light.df$local_datetime)
tz(cari.df$local_datetime)
# Note that model_datetime is assigned the same tz as local_datetime

```

###### Visualize, select and clean CARI data
```{r - CARI visualize data, select days, add sat. light}

##### Visualize data and select days

yr <- 2024
mo <- 6
plottitle <- "CARI, June 2024"

quartz(width=6.5, height=6.5)
#quartz(width=10, height=4)
cari.df %>%
  filter(year(local_datetime) == yr) %>%
  filter(month(local_datetime) == mo) %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(22,30) +
  facet_wrap(~Jday) + 
  ggtitle(plottitle) +
  theme_bw()


# cari_dy <- cari.df %>%
  # select(local_datetime, surfWaterNitrateMean)

# Dygraph for interactive chart w slider
# dygraph(data=cari_dy) %>%
#   dyRangeSelector()


##### ID MODEL_JDAYS to save for model - this makes later steps work better (determining daily light, for example). ID the actual (local time) days, but code for model jdays
#     visual check to select 120+ complete-appearing days to model from the 2.5 year dataset
#     CARI: COMMENTS - Low appears to be ~ 8-10p (!!) in May,  - and that *is* local time. Daylight hours = 5:15a-10:20p in early May, 2:45a-25:00a (1a next day) mid-June...

#########   CARI MODEL DAY STARTS AT 2A

# ID Jdays: 

##    2021: [140-147; 149-151; 163-170; 172-175; 177-183; 185-187; 191-196; 198-201; 223-226; 246-265; 267-277] - Neq at around 5-10a

# 2021: REMOVE PAR NAdays if applicable: 2021: 119, 120, 121, 122, 123, 124, 154, 189, 353, 354 (none included)

##    2022: [130-131; 135-142; 144-147; 149-159; 161-166; 169-170; 174-183; 186; 188-190; 194-195; 198-199; 202-205; 208-212; 215-218; 223-226; 229-243; 246-254; 260-269; 271-282] Neq at around 5-10a

# 2022: REMOVE PAR NAdays if applicable: 2022: 116, 171, 206, 207, 227-235, 244, 258, 312, 313, 315, 316, 317, 336, 338-345, 348, 357 
# RM 229-235

##    2023: [132-134; 139-141; 158-167; 170-173; 175-185; 191-194; 196-206; 208-221; 223-224; 252-259; 263-271; 274-277; 279-285; 287-288; 290-291] Neq at around 5-10a

# 2023: REMOVE PAR NAdays if applicable: 16, 125, 156, 157, 167, 195, 206, 255
# 2023, RM 167, 206, 255

# We may not use 2024, but it's easiest to ID good days Jan-June now
##    2024: [111-113; 121-126; 137-142; 148-149; 153-156; 160-163; 172-176; ]
# 2024: REMOVE PAR NAdays if applicable: RM 120

# create objects from selected jdays for each year
list.21 <- c(140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 163, 164, 165, 166, 167, 168, 169, 170, 172, 713, 174, 175, 177, 178, 179, 180, 181, 182, 183, 185, 186, 187, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 223, 224, 225, 226, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277)

list.22 <- c(130, 131, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 169, 170, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 186, 188, 189, 190, 194, 195, 198, 199, 202, 203, 204, 205, 208, 209, 210, 211, 212, 215, 216, 217, 218, 223, 224, 225, 226, 236, 237, 238, 239, 240, 241, 242, 243, 246, 247, 248, 249, 250, 251, 252, 253, 254, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282)


list.23 <- c(132, 133, 134, 139, 140, 141, 158, 159, 160, 161, 162, 163, 164, 165, 166, 170, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 208, 209, 210, 211, 212, 213,  214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 252, 253, 254, 256, 257, 258, 259, 263, 264, 265, 266, 267, 268, 269, 270, 271, 274, 275, 276, 277, 279, 280, 281, 282, 283, 284, 285, 287, 288, 290, 291)

# [132-134; 139-141; 158-167; 170-173; 175-185; 191-194; 196-206; 208-221; 223-224; 252-259; 263-271; 274-277; 279-285; 287-288; 290-291] 

# Use the objects to create dataframes of the selected *model* jdays (ensures complete 24-h model days, i.e. from 4a-3:59a)
cari.df.21 <- cari.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(model_jday %in% list.21)
  
cari.df.22 <- cari.df %>%  
  filter(year(local_datetime) == 2022) %>%
  filter(model_jday %in% list.22)

cari.df.23 <- cari.df %>%  
  filter(year(local_datetime) == 2023) %>%
  filter(model_jday %in% list.23)

# Check for complete days: should have 96 obs/ day
cari.df.21 %>% count(model_jday) # 77
cari.df.22 %>% count(model_jday) # 107
cari.df.23 %>% count(model_jday) # 91

# check for days where obs != 96 (often leap years, or some sensor irregularity)

remove.21 <- cari.df.21 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.22 <- cari.df.22 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.23 <- cari.df.23 %>% 
  count(model_jday) %>%
  filter(n != 96)

# View days with alternate n values
# cari22_310 <- cari.df.22 %>% filter(model_jday == 310)
# cari21_310 <- cari.df.22 %>% filter(model_jday == 310)
# View(cari73)
# View(cari310)
# 
# cari310 <- cari.df.22 %>% filter(Jday == 310)
# View(cari310)


##### Combine selected days into 1 df, select columns 
cari.df.2123 <- bind_rows(cari.df.21, cari.df.22, cari.df.23) %>%
  mutate(Year = year(model_datetime)) %>% 
  rename(UTC_startDateTime=startDateTime) %>%
  unite("yr_jday", Year:model_jday, remove=FALSE) %>%
  dplyr::select(siteID, yr_jday, model_datetime, surfWaterNitrateMean, surfWaterNitrateVariance, surfWaterNitrateStdErMean, rangeFailQM, UTC_startDateTime, local_datetime, Jday, model_jday, Year, domainID)

# check the # of days by dividing cari.df.2123 observations by 96 
# 275 for cari

######################  DIFFERENT PROCESS: SAVE HERE, THEN DROP TO HOURLY AND COMBINE THEN MERGE 30M PAR DATA  ################################################


### Save the 15-min NO3 df
write_csv(cari.df.2123,file=here("N_uptake_NEON/data/neon_data_joined/CARI_2123_15m.csv"))

# convert to hourly NO3 df
cari.df.2123.h <- cari.df.2123 %>%
  filter(minute(local_datetime)==0)

View(cari.df.2123.h)

## sum hourly light
cari_light.df.h <- cari_light.df %>%
  mutate(model_datetime = ceiling_date(model_datetime, unit = "hour")) %>%
  group_by(model_datetime) %>%
  summarise(PARMean.h = sum(PARMean), 
            PARMax.h = max(PARMaximum), 
            PARMin.h = min(PARMinimum)) %>%
  ungroup() %>%
  mutate(local_datetime = model_datetime + hours(2))

View(cari_light.df.h)

##### Combine df with sat_light data for the selected model days
cari.wlight.df.2123.h <- left_join(x=cari.df.2123.h,
                                 y=cari_light.df.h, 
                                 by = "local_datetime") 


View(cari.wlight.df.2123.h)

# NB, this provides NA data on the :15 and :45 min spots. Ultimately may want to calculate hourly data and add *that* to the hourly dataset. 

# with CARI PAR data, need to check the model_jdays etc for the match... 
checkdays <- cari.wlight.df.2123 %>%
  filter(Jday.x != Jday.y)

checkdays2 <- cari.wlight.df.2123 %>%
  filter(model_jday.x != model_jday.y)

checkdays3 <- cari.wlight.df.2123.h %>%
  filter(model_datetime.x != model_datetime.y)

# check the 1st day in the df to make sure light joined correctly
jday21_56 <- cari_light.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(month(local_datetime) == 2) %>%
  filter(day(local_datetime) == 25)


# save to keep progress
# write_csv(cari.wlight.df.2123,file=here("N_uptake_NEON/data/neon_data_joined/CARI_wlight_2123_joined.csv"))

```

###### Fill any gaps in no3 data
```{r fill gaps}

##############################  Reload data if needed  #########################
# cari.wlight.df.2123 <- read_csv(here("N_uptake_NEON/data/neon_data_joined/WALK_wlight_2123_joined.csv")) %>%
#   mutate(local_datetime = force_tz(local_datetime, tzone="US/Mountain"))


##############################  Where are the NAs?  ############################

no3NA <- which(is.na(cari.wlight.df.2123.h$surfWaterNitrateMean))
no3NA

# 6 occurrences:  106 2140 3095 4239 5290 6179

# When do these occur? (ID any particularly bad days to remove)
NAdays <- cari.wlight.df.2123 %>%
  filter(is.na(surfWaterNitrateMean)) %>%   # Filter rows where 'no3mean' is NA
  select(yr_jday)                  # Select the 'Jday' column for those rows

NAdays  # 2021: 144(2), 255, 
        # 2022: 137, 146, 202(4), 275, 277,  
        # 2023: 139(3), 162, 196, 198(5), 201, 204, 252, 269(6), 283

# all are missing <= 1.5h of data, which is fine for a gap fill 

# lightNA <- which(is.na(cari.wlight.df.2123$PARMean))
# lightNA  
# none, whew

##############################   SMALL GAPS  #####################################

# For smaller NA chunks, (8 or fewer timesteps (<= 2 hours)?) , we will interpolate using zoo()

#    first see where in the curve this gap falls: 
span <- 15945:15985
plot(cari.wlight.df.2123$local_datetime[span], cari.wlight.df.2123$surfWaterNitrateMean[span], type = 'l')  #OK, linear seems reasonable

# Then fill the hourly data
maxgap <- 2  # set the maximum gap for zoo() to fill; 2h or less
cari.wlight.df.2123.h$surfWaterNitrateMean <- na.approx(cari.wlight.df.2123.h$surfWaterNitrateMean, maxgap = maxgap)  # maxgap = max # of NAs to fill



# re-check NAs
which(is.na(cari.wlight.df.2123.h$surfWaterNitrateMean)) 

# no NAs remain

N_e <- mean(cari.wlight.df.2123.h$surfWaterNitrateMean, na.rm = TRUE)  #28.1843
N_sd <- sd(cari.wlight.df.2123.h$surfWaterNitrateMean, na.rm = TRUE)   #4.415503

# ## Select hourly no3 measurements to match what we've been doing  => CHANGE THIS LATER
# cari.wlight.df.h <- cari.wlight.df.2123 %>%
#   filter(minute(local_datetime) == 0) 
# # Mean of selected dataset
# N_e <- mean(cari.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE) #1.811583
# N_sd <- sd(cari.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE)  #0.7408184
# 


```


###### Visualize cleaned data

```{r - visualize CARI clean}

# jday distribution plot (do we have even coverage across a generic year?)
hist(cari.wlight.df.2123.h$model_jday)

#good distribution from ~ April-October

```



###### Save cleaned site df
```{r - Save cleaned df}

############################  Save cleaned site DF ###############################

# path <- here("N_uptake_NEON/data/neon_data_clean/cari_clean.csv")
path_h <- here("N_uptake_NEON/data/neon_data_clean/cari_hourly_clean.csv")
# write_csv(cari.wlight.df.2123, path)
write_csv(cari.wlight.df.2123.h, path_h)


##### Reload data-in-progress as needed
# cari.wlight.df.2122 <- read_csv(path)
# cari.wlight.df.h <- read_csv(path_h)

```



#### Como Creek, Boulder CO
nitrate sensor lat-long: 40.034934	-105.544417	
elevation (m): 3022.86
tz="US/Mountain"


```{r - COMO}

##### Create object for COMO => Como Creek, Boulder CO

como.df <- selectRdata(data=no3_data_sensor, site="COMO", tz="US/Mountain", span=2021:2024)

# como.df <- no3_data_sensor %>%
#   filter(siteID == "COMO") %>% 
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Mountain"),
#          Jday = yday(local_datetime)) %>%
#   filter(year(local_datetime) == 2021:2023)


##### Visualize, select and clean COMO data

yr <- 2023
mo <- 1:2

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
como.df %>%
  filter(year(local_datetime) == yr) %>%
  filter(month(local_datetime) == mo) %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  ylim(2,8) +
  facet_wrap(~Jday) +
  ggtitle("COMO, 2023: Jan-Feb") +
  theme_bw()

##### ID Jdays to save for model




##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/como_df.csv")
write_csv(como.df, path)

```


#### Rio Cupeyes, San German Municipio, PR
nitrate sensor lat-long: 18.110265	-66.986411	
elevation (m): 149.9
tz = "America/Puerto_Rico"

###### Load data
```{r - CUPE}
#| output: false
#| message: false

##### Create object for CUPE => Rio Cupeyes, San German Municipio, PR

cupe.df <- selectRdata(data=no3_data_sensor, site="CUPE", tz="America/Puerto_Rico", span=2021:2024)

cupe_light.df <- read_csv(here("N_uptake_NEON/data/NSRDB_lightdata_clean/CUPE_satlight_all.csv")) %>%
  mutate(local_datetime = force_tz(local_datetime, tzone="America/Puerto_Rico"))

# make sure the tz assigned correctly - otherwise the join doesn't work correctly, because R/ tidyverse assumes the assigned tz is correct (so, offsets the join *as if* it needed to match different tzs vs. the same one)
tz(cupe_light.df$local_datetime)
tz(cupe.df$local_datetime)
# Note that model_datetime is assigned the same tz as local_datetime

```

###### Visualize, select and clean CUPE data
```{r - CUPE visualize data, select days, add sat. light}

##### Visualize data and select days

yr <- 2024
mo <- 6
plottitle <- "CUPE, Jun 2024"
# ylim(20,30) for Jan-Jun 21, Jan-Jun 23; (20, 33) for ~ Aug-Dec 2023, Jan- 2024; (17,27) for Jul-Dec 21, most of 2022, Jul 23, Jan- 2024;    : (25,35) for Oct-Dec 22

quartz(width=6.5, height=6.5)
#quartz(width=10, height=4)
cupe.df %>%
  filter(year(local_datetime) == yr) %>%
  filter(month(local_datetime) == mo) %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  ylim(17, 27) +
  facet_wrap(~Jday) + 
  ggtitle(plottitle) +
  theme_bw()


## Dygraph for interactive chart w slider - meh, doesn't work all that well for my purposes.
# cupe_dy <- cupe.df %>%
# select(local_datetime, surfWaterNitrateMean)
# 
# dygraph(data=cupe_dy) %>%
#   dyRangeSelector()


##### ID MODEL_JDAYS to save for model
# doing this early this makes later steps work better (determining daily light, for example). ID the actual (local time) days, but code for model jdays

# visual check to select 120+ complete-appearing days to model from the 2.5 year dataset
# CUPE: COMMENTS?  High NO3-N (20-35 range), very clear signals unless the data is disrupted. Several extended (hydrologic?) issues, especially in 2022. TONS of days selected. 


# ID Jdays: 

##    2021: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, [52, 53 - odd shapes], 54, 55, 60, 61, 62, 63, 63, 65, 66, 68, 69, 72, 73, 74, 75, 91, 92, 94, 95, 96, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 115, 116, 120, [128 - slanted bseline], 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 148, 149, 150, 151, 154, 155, 156, 158, 162, 163, 164, 165, 166, 169, 170, 171, 179, 180, 185, 186, 187, 188, 189, 191, 192, 193, 197, 198, 199, 200, 201, 202, 203, 204, 210, 211, 212, 215, 216, 217, 221, 222, 223, 225, 226, 229, 232, 239, 240, 241, 242, 244, 246, 247, 281, 282, 283, 285, [286 - lower ampl], 287, 295, 296, 298, 299, 300, 301, 302, 303, 304, 307, 308, 309, 310, 311, 312, 313, 314, 316, 323, 324, 325, 335, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 363, 364

##    2022: 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 38, 39, 84, 85, 86, 89, 90, 91, 92, 93, 94, 95, 96, 99, 100, 101, 102, 103, 105, 106, 107, 110, 111, 113, 114, 116, 118, 119, 121, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, [141 - slight wobble], 149, 150, 155, 156, 157, 165, 166, 167, 168, 169, 170, 171, 172, 175, 176, 177, 178, 184, 187, 188, 189, 192, 193, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 209, 211, 212, [221, 222, 238, 239, 240, 249, 251, 252, 258, 293 - low ampl - don't use unless testing something later] 298, 321, 327, 328, 329, 330, 331,  - mb include even tho relatively low?], 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347, 348, 349, 352, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362 

##    2023: 1, 2, 3, 4, 5, 6, 9, 20, 21, 25, 26, 27, 28, 29, 30, 31, 37, 39, 40, 41, 42, 43, 45, 46, 48, 50, 52, 53, 54, 55, 56, 60, 62, 63, 64, 65, 66, 70, 71, 72, 77, 78, 79, 85, 86, 87, 88, 89, 91, 92, 99, 100, 101, 102, 111, 112, 113, 115, 119, 120, 121, 122, 126, 127, 128, 129, 130, 134, 135, 136, 142, 143, 144, 145, 146, 151, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 182, 183, 184, 186, 187, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 208, 209, 210, 211, 212, 213, 216, 217, 218, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 236, 245, 246, 249, 250, 257, 270, 274, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 301, 302, 303, 304, 309, 310, 311, 312, 318, 319, 321, 322, 323, 326, 327, 328, 329, [335, 336, 338, 339, 340, 341, 342, 343, 346, 347, may be too low... ], 351, 352, 535, 356, 357, 358, 359, 362, 363, 364, 365


# We may not use 2024, but it's easiest to ID good days Jan-June now
##    2024: 1, 2, 3, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20, 26, 27, 28, 32, 33, 34, 35, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 59, 71, 72, 76, 77, 79, 86, 87, 93, 94, 97, 98, 99, 100, 101, 105, 106, 107, 109, [125 - low?], 126, 140, 141, 142, 143, 144, 149, 150, 151, 152, 153, 156, 157, 158, 159, 160, 161, 162, 163, 171, 177, 178, [182 - partial final day in June bc of model_day use]

# create objects from selected jdays for each year
list.21 <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 54, 55, 60, 61, 62, 63, 63, 65, 66, 68, 69, 72, 73, 74, 75, 91, 92, 94, 95, 96, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 115, 116, 120, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 148, 149, 150, 151, 154, 155, 156, 158, 162, 163, 164, 165, 166, 169, 170, 171, 179, 180, 185, 186, 187, 188, 189, 191, 192, 193, 197, 198, 199, 200, 201, 202, 203, 204, 210, 211, 212, 215, 216, 217, 221, 222, 223, 225, 226, 229, 232, 239, 240, 241, 242, 244, 246, 247, 281, 282, 283, 285, 287, 295, 296, 298, 299, 300, 301, 302, 303, 304, 307, 308, 309, 310, 311, 312, 313, 314, 316, 323, 324, 325, 335, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 363, 364)

list.22 <- c(14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 38, 39, 84, 85, 86, 89, 90, 91, 92, 93, 94, 95, 96, 99, 100, 101, 102, 103, 105, 106, 107, 110, 111, 113, 114, 116, 118, 119, 121, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 149, 150, 155, 156, 157, 165, 166, 167, 168, 169, 170, 171, 172, 175, 176, 177, 178, 184, 187, 188, 189, 192, 193, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 209, 211, 212, 298, 321, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347, 348, 349, 352, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362)

list.23 <- c(1, 2, 3, 4, 5, 6, 9, 20, 21, 25, 26, 27, 28, 29, 30, 31, 37, 39, 40, 41, 42, 43, 45, 46, 48, 50, 52, 53, 54, 55, 56, 60, 62, 63, 64, 65, 66, 70, 71, 72, 77, 78, 79, 85, 86, 87, 88, 89, 91, 92, 99, 100, 101, 102, 111, 112, 113, 115, 119, 120, 121, 122, 126, 127, 128, 129, 130, 134, 135, 136, 142, 143, 144, 145, 146, 151, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 182, 183, 184, 186, 187, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 208, 209, 210, 211, 212, 213, 216, 217, 218, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 236, 245, 246, 249, 250, 257, 270, 274, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 301, 302, 303, 304, 309, 310, 311, 312, 318, 319, 321, 322, 323, 326, 327, 328, 329, 351, 352, 535, 356, 357, 358, 359, 362, 363, 364, 365)


# Use the objects to create dataframes of the selected *model* jdays (ensures complete 24-h model days, i.e. from 4a-3:59a)
cupe.df.21 <- cupe.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(model_jday %in% list.21)
  
cupe.df.22 <- cupe.df %>%  
  filter(year(local_datetime) == 2022) %>%
  filter(model_jday %in% list.22)

cupe.df.23 <- cupe.df %>%  
  filter(year(local_datetime) == 2023) %>%
  filter(model_jday %in% list.23)

# Check for complete days: should have 96 obs/ day
cupe.df.21 %>% count(model_jday) #188
cupe.df.22 %>% count(model_jday) #132
cupe.df.23 %>% count(model_jday) #180

# check for days where obs != 96 (often leap years, or some sensor irregularity)

remove.21 <- cupe.df.21 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.22 <- cupe.df.22 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.23 <- cupe.df.23 %>% 
  count(model_jday) %>%
  filter(n != 96)

# All days, all 3 years had 96 obs. 

# View days with alternate n values
# cupe22_310 <- cupe.df.22 %>% filter(model_jday == 310)
# cupe21_310 <- cupe.df.22 %>% filter(model_jday == 310)
# View(cupe73)
# View(cupe310)
# 
# cupe310 <- cupe.df.22 %>% filter(Jday == 310)
# View(cupe310)


##### Combine selected days into 1 df, select columns 
cupe.df.2123 <- bind_rows(cupe.df.21, cupe.df.22, cupe.df.23) %>%
  mutate(Year = year(model_datetime)) %>%  #changed to be consistent w. model_jday => important in CUPE where there are year-round good days!
  rename(UTC_startDateTime=startDateTime) %>%
  unite("yr_jday", Year:model_jday, remove=FALSE) %>%
  dplyr::select(siteID, yr_jday, model_datetime, surfWaterNitrateMean, surfWaterNitrateVariance, surfWaterNitrateStdErMean, rangeFailQM, UTC_startDateTime, local_datetime, Jday, model_jday, Year, domainID)

# check the # of days by dividing cupe.df.2123 observations by 96 
# 500 for cupe


##### Combine df with sat_light data for the selected model days
cupe.wlight.df.2123 <- left_join(x=cupe.df.2123,
                                 y=cupe_light.df, 
                                 by = "local_datetime") 


View(cupe.wlight.df.2123)

plot(x= cupe.wlight.df.2123$)
# check the 1st day in the df to make sure light joined correctly
jday21_01 <- cupe_light.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(month(local_datetime) == 1) %>%
  filter(day(local_datetime) == 1)

View(jday21_01)

# save to keep progress
write_csv(cupe.wlight.df.2123,file=here("N_uptake_NEON/data/neon_data_joined/CUPE_wlight_2123_joined.csv"))

```

###### Fill any gaps in no3 data
```{r fill gaps}

##############################  Reload data if needed  #########################
# cupe.wlight.df.2123 <- read_csv(here("N_uptake_NEON/data/neon_data_joined/WALK_wlight_2123_joined.csv")) %>%
#   mutate(local_datetime = force_tz(local_datetime, tzone="US/Mountain"))


##############################  Where are the NAs?  ############################

### Set low/ clearly sensor-error values to NA: 

# First, ID whether there are any
sensorfaildays <- cupe.wlight.df.2123 %>%
  filter(surfWaterNitrateMean < 10)  # all are less than 5

# then change the values to NA
cupe.wlight.df.2123 <- cupe.wlight.df.2123 %>%
  mutate(surfWaterNitrateMean = replace(surfWaterNitrateMean, surfWaterNitrateMean < 5, NA))


no3NA <- which(is.na(cupe.wlight.df.2123$surfWaterNitrateMean))
no3NA

# 46 occurrences:  888  1304  5839  5840  5844  5846  5847  5850  5854  5856  9618  9619  9620 11823 11824 12983 12984 12985 13786 15965 15966 15967 15988 21327 21328 21340 21709 24208 24442 31425 31736 31830 31831 32206 32877 39208 39209 40065 40066 40067 40068 40069 44076 44499 47174 47177

# Then, removing low values added 39 more: 
# 8848  8849 10488 10489 10867 10868 13264 13265 14419 14420 21143 21144 26513 28050 28052 28053 28054 28055 28056 28819 28820 28821 29585 29586 30545 30546 36515 37762 40834 42272 42273 43811 43812 43813 44867 44868 46114 46115 47175

# When do these occur? (ID any particularly bad days to remove)
NAdays <- cupe.wlight.df.2123 %>%
  filter(is.na(surfWaterNitrateMean)) %>%   # Filter rows where 'no3mean' is NA
  select(yr_jday)                  # Select the 'Jday' column for those rows

NAdays  # 2021: 10, 15, 75(8 - not continuous), 158(3), 201(2), 223(3), 242, 312(4)
        # 2022: 103(3), 110, 156, 165
        # 2023: 20, 26, 27(2), 31, 45, 166(2), 184(5), 257, 280, 352(2)

# all are missing < 1h of data, which is fine for a gap fill 

lightNA <- which(is.na(cupe.wlight.df.2123$GHI_wm2))
lightNA  
# none, whew

##############################   SMALL GAPS  #####################################

# For smaller NA chunks, (8 or fewer timesteps (<= 2 hours)?) , we will interpolate using zoo()

#    first see where in the curve this gap falls: 5830:5855, 40060:40075
# print(cupe.wlight.df.2123[5830:5860,])

span <- 40060:40075
plot(cupe.wlight.df.2123$local_datetime[span], cupe.wlight.df.2123$surfWaterNitrateMean[span], type = 'l')  #OK, linear seems reasonable for both gaps


# Then fill
maxgap <- 8  # set the maximum gap for zoo() to fill
cupe.wlight.df.2123$surfWaterNitrateMean <- na.approx(cupe.wlight.df.2123$surfWaterNitrateMean, maxgap = maxgap)  # maxgap = max # of NAs to fill



# re-check NAs
which(is.na(cupe.wlight.df.2123$surfWaterNitrateMean)) 
# no NAs remain

N_e <- mean(cupe.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)  #24.42304 then 24.4445 after removing very low/ neg #s
N_e
N_sd <- sd(cupe.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)   #2.786831 then 2.701031 after removing very low/ neg #s
N_sd

## Select hourly no3 measurements to match what we've been doing  => CHANGE THIS LATER
cupe.wlight.df.h <- cupe.wlight.df.2123 %>%
  filter(minute(local_datetime) == 0) 
# Mean of selected dataset
N_e <- mean(cupe.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE) #24.4242 then 24.44639 after removing very low/ neg #s
N_e
N_sd <- sd(cupe.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE)  #2.785572 then 2.701461 after removing very low/ neg #s
N_sd



```


###### Visualize cleaned data

```{r - visualize CUPE clean}

# jday distribution plot (do we have even coverage across a generic year?)
hist(cupe.wlight.df.h$model_jday)

clean.cupe.plot <- ggplot(cupe.wlight.df.h, aes(x=yr_jday, y=surfWaterNitrateMean)) + 
  geom_point() + 
  ggtitle("CUPE - cleaned and filled data") +
  theme_bw()

  quartz(height=6, width=7.5)
  clean.cupe.plot

```



###### Save cleaned site df
```{r - Save cleaned df}

############################  Save cleaned site DF ###############################

path <- here("N_uptake_NEON/data/neon_data_clean/cupe_clean.csv")
path_h <- here("N_uptake_NEON/data/neon_data_clean/cupe_hourly_clean.csv")
write_csv(cupe.wlight.df.2123, path)
write_csv(cupe.wlight.df.h, path_h)


##### Reload data-in-progress as needed
# cupe.wlight.df.2123 <- read_csv(path)
# cupe.wlight.df.h <- read_csv(path_h)

```


Will not model Flint River, Baker, GA - no hydraulic z/ Aho
nitrate sensor lat-long: 31.184609	-84.438438	** sensor buoy (river)
elevation (m): 30
tz = "US/Eastern"

#### Ro Yahuecas, Adjuntas Municipio, PR
nitrate sensor lat-long: 18.174069	-66.799688	
elevation (m): 546.37
tz = "America/Puerto_Rico"


```{r - GUIL}

##### Create object for GUIL => Ro Yahuecas, Adjuntas Municipio, PR

guil.df <- selectRdata(data=no3_data_sensor, site="GUIL", tz="America/Puerto_Rico", span=2021:2024)


# guil.df <- no3_data_sensor %>%
#   filter(siteID == "GUIL") %>% 
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Puerto_Rico"),
#          Jday = yday(local_datetime)) %>%
#   filter(year(local_datetime) == 2021:2023)


##### Visualize, select and clean GUIL data

yr <- 2023
# mo <- 2

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
guil.df %>%
  filter(year(local_datetime) == yr) %>%
  # filter(month(local_datetime) == mo) %>%
  # ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,4) +
  # facet_wrap(~Jday) + 
  ggtitle("GUIL, 2023") +
  theme_bw()


##### ID Jdays to save for model



##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/guil_df.csv")
write_csv(guil.df, path)

```

#### Lower Hop Brook, Franklin, MA
nitrate sensor lat-long: 42.473264	-72.330924	
elevation (m): 214.2
tz="US/Eastern"

overhang sensor location: 42.471752	-72.330641	elevation: 208.31

```{r - HOPB}

##### Create object for HOPB => Lower Hop Brook, Franklin, MA

hopb.df <- selectRdata(data=no3_data_sensor, site="HOPB", tz="US/Eastern", span=2021:2024)


# hopb.df <- no3_data_sensor %>%
#   filter(siteID == "HOPB") %>% 
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Eastern"),
#          Jday = yday(local_datetime)) %>%
#   filter(year(local_datetime) == 2021:2023)


##### Visualize, select and clean HOPB data

yr <- 2023
# mo <- 1:2

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
hopb.df %>%
  filter(year(local_datetime) == yr) %>%
  # filter(month(local_datetime) == mo) %>%
  # ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,4) +
  # facet_wrap(~Jday) + 
  ggtitle("HOPB, 2023") +
  theme_bw()

##### ID Jdays to save for model




##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/hopb_df.csv")
write_csv(hopb.df, path)

```

#### Kings Creek, Riley, KS
nitrate sensor lat-long: 39.105188	-96.603584	
elevation (m): 324.26
tz="US/Central"


```{r - KING}

##### Create object for KING => Kings Creek, Riley, KS

king.df <- selectRdata(data=no3_data_sensor, site="KING", tz="US/Central", span=2021:2024)


# king.df <- no3_data_sensor %>%
#   filter(siteID == "KING") %>% 
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Central"),
#          Jday = yday(local_datetime)) %>%
#   filter(year(local_datetime) == 2021:2023)

##### Visualize, select and clean KING data

yr <- 2023
# mo <- 1:2

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
king.df %>%
  filter(year(local_datetime) == yr) %>%
  # filter(month(local_datetime) == mo) %>%
  # ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,4) +
  # facet_wrap(~Jday) + 
  ggtitle("KING, 2023") +
  theme_bw()

##### ID Jdays to save for model




##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/king_df.csv")
write_csv(king.df, path)





##________________________________________________________________________________

##### filter data by selected year(s)
king.df.2021 <- king.df %>%
  filter(year(local_datetime) == 2021) %>%
  dplyr::select(siteID, local_datetime, Jday, startDateTime:surfWaterNitrateStdErMean)

king.df.2022 <-  king.df %>%
  filter(year(local_datetime) == 2022) %>%
  dplyr::select(siteID, local_datetime, Jday, startDateTime:surfWaterNitrateStdErMean)

king.df.2023 <-  king.df %>%
  filter(year(local_datetime) == 2023) %>%
  dplyr::select(siteID, local_datetime, Jday, startDateTime:surfWaterNitrateStdErMean)


##### save dfs
path <- here("N_uptake_NEON/data/neon_data_by_year/king.df_2021.csv")
write_csv(king.df.2021, path)

path <- here("N_uptake_NEON/data/neon_data_by_year/king.df_2022.csv")
write_csv(king.df.2022, path)

path <- here("N_uptake_NEON/data/neon_data_by_year/king.df_2023.csv")
write_csv(king.df.2023, path)



```


#### LeConte Creek, Sevier, TN
nitrate sensor lat-long: 35.69222	-83.504409	
elevation (m): 538.09
tz="US/Eastern"

overhang sensor location: 35.692205	-83.504421	elevation: 537.52

```{r -  LECO}

##### Create object for LECO => LeConte Creek, Sevier, TN

leco.df <- selectRdata(data=no3_data_sensor, site="LECO", tz="US/Eastern", span=2021:2024) # LECO is in Sevier County = US Eastern

# leco.df <- no3_data_sensor %>%
#   filter(siteID == "LECO") %>% 
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Eastern"),
#          Jday = yday(local_datetime)) %>% # Sevier County = US Eastern
#   filter(year(local_datetime) == 2021:2023)


##### Visualize, select and clean LECO data

yr <- 2023
mo <- 3:4

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
leco.df %>%
  filter(year(local_datetime) == yr) %>%
  filter(month(local_datetime) == mo) %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,4) +
  facet_wrap(~Jday) +
  ggtitle("LECO, 2023: Mar-Apr") +
  theme_bw()

##### ID Jdays to save for model




##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/leco_df.csv")
write_csv(leco.df, path)

```

[Did NOT model Lewis Run [LEWI], Clarke, VA]

#### Martha Creek, Skamania, WA
nitrate sensor lat-long: 45.792321	-121.929418	
elevation (m): 337.16
tz="US/Pacific"

overhang sensor location: 45.792318	-121.92943	elevation: 337.02


###### Load data

```{r - MART}
#| output: false
#| message: false

##### Create object for MART => Pringle Creek, Wise Co., TX

mart.df <- selectRdata(data=no3_data_sensor, site="MART", tz="US/Pacific", span=2021:2024)

mart_light.df <- read_csv(here("N_uptake_NEON/data/NSRDB_lightdata_clean/MART_satlight_all.csv")) %>%
  mutate(local_datetime = force_tz(local_datetime, tzone="US/Pacific"))

#tz(mart_light.df$local_datetime) <- "US/Mountain"

# make sure the tz assigned correctly - otherwise the join doesn't work correctly, because R/ tidyverse assumes the assigned tz is correct (so, offsets the join *as if* it needed to match different tzs vs. the same one)
tz(mart_light.df$local_datetime)
tz(mart.df$local_datetime)
# Note that model_datetime is assigned the same tz as local_datetime

# The function does this now: 
# mart.df <- no3_data_sensor %>%
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Mountain"),
#         Jday = yday(local_datetime)) %>%
#   filter(siteID == "MART", 
#          year(local_datetime) %in% 2021:2023) 
#   
```

###### Visualize, select and clean MART data
```{r - MART visualize data, select days, add sat. light}

##### Visualize data and select days

yr <- 2024
mo <- 6
plottitle <- "MART, Jun 2024"

quartz(width=6.5, height=6.5)
#quartz(width=10, height=4)
mart.df %>%
  filter(year(local_datetime) == yr) %>%
  filter(month(local_datetime) == mo) %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  ylim(10, 30) +
  facet_wrap(~Jday) +
  ggtitle(plottitle) +
  theme_bw()


mart_dy <- mart.df %>%
  select(local_datetime, surfWaterNitrateMean)

# Dygraph for interactive chart w slider
dygraph(data=mart_dy) %>%
  dyRangeSelector()


##### ID MODEL_JDAYS to save for model - this makes later steps work better (determining daily light, for example). ID the actual (local time) days, but code for model jdays
#     visual check to select 120+ complete-appearing days to model from the 2.5 year dataset
#     MART: comments?

# Jdays: 
##    2021: 

##    2022: 

##    2023: 

##    2024: 


list.21 <- c()

list.22 <- c()

list.23 <- c()

# Creating and visualizing dataframes from the selected days - 
#   MART looked really ... wobby and odd, so making sure these seem like reasonable model options

mart.df.21 <- mart.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(model_jday %in% list.21)

quartz(6.5, 6.5)
mart.df.21 %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(10, 30) +
  facet_wrap(~Jday) +
  ggtitle("MART 2021") +
  theme_bw()
  
mart.df.22 <- mart.df %>%  
  filter(year(local_datetime) == 2022) %>%
  filter(model_jday %in% list.22)

quartz(6.5, 6.5)
mart.df.22 %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(10, 30) +
  facet_wrap(~Jday) +
  ggtitle("MART 2022") +
  theme_bw()

mart.df.23 <- mart.df %>%  
  filter(year(local_datetime) == 2023) %>%
  filter(model_jday %in% list.23)

quartz(6.5, 6.5)
mart.df.23 %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(10, 30) +
  facet_wrap(~Jday) +
  ggtitle("MART 2023") +
  theme_bw()

# Check for complete days: should have 96 obs/ day

# check for days where obs != 96 (often leap years, or some sensor irregularity)

remove.21 <- mart.df.21 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.22 <- mart.df.22 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.23 <- mart.df.23 %>% 
  count(model_jday) %>%
  filter(n != 96)


length(mart.df.21$model_jday)/96 #35
length(mart.df.22$model_jday)/96 #57
length(mart.df.23$model_jday)/96 #48

# mart.df.21 %>% count(model_jday) # 35
# mart.df.22 %>% count(model_jday) # 57 (after removing jday 121 because of no3 spikes at end of model day) 
# mart.df.23 %>% count(model_jday) # 48 (after removing jday 71, w. only 92 obs)


##### Combine selected days into 1 df 
mart.df.2123 <- bind_rows(mart.df.21, mart.df.22, mart.df.23) %>%
  mutate(Year = year(model_datetime)) %>% 
  rename(UTC_startDateTime=startDateTime) %>%
  unite("yr_jday", Year:model_jday, remove=FALSE) %>%
  dplyr::select(siteID, yr_jday, model_datetime, surfWaterNitrateMean, surfWaterNitrateVariance, surfWaterNitrateStdErMean, rangeFailQM, UTC_startDateTime, local_datetime, Jday, model_jday, Year, domainID)


##### Combine df with sat_light data for the selected model days
mart.wlight.df.2123 <- left_join(x=mart.df.2123,
                                 y=mart_light.df, 
                                 by = "local_datetime") 


View(mart.wlight.df.2123)

# # check the 1st day in the df to make sure light joined correctly 
# jday31 <- mart_light.df %>% 
#   filter(year(local_datetime) == 2021) %>%
#   filter(month(local_datetime) == 1) %>% 
#   filter(day(local_datetime) == 31)


# save to keep progress
write_csv(mart.wlight.df.2123,file=here("N_uptake_NEON/data/neon_data_joined/MART_wlight_2123_joined.csv"))

```

###### Fill any gaps in no3 data
```{r fill gaps}

##############################  Reload data if needed  #########################
# mart.wlight.df.2122 <- read_csv(here("N_uptake_NEON/data/neon_data_joined/WALK_wlight_2122_joined.csv")) %>%
#   mutate(local_datetime = force_tz(local_datetime, tzone="US/Mountain"))


##############################  Where are the NAs?  ############################

no3NA <- which(is.na(mart.wlight.df.2123$surfWaterNitrateMean))
no3NA
# 20 NAs:  188   189  7651  7652  9053  9054  9055  9056 10914 10915 10916 10917 11338 12498 12499 12500 12501 12502 12503 12504

# 
# When do these occur? (ID any particularly bad days to remove)
NAdays <- mart.wlight.df.2123 %>%
  filter(is.na(surfWaterNitrateMean)) %>%   # Filter rows where 'no3mean' is NA
  select(yr_jday)                  # Select the 'Jday' column for those rows

NAdays # 2021: 184(2)
       # 2022: 132(2)
       # 2023:  72(4), 109(4), 121, 143(7)


lightNA <- which(is.na(mart.wlight.df.2123$GHI_wm2))
lightNA  
# none, whew

##############################   SMALL GAPS  ###################################

# For smaller NA chunks, (8 or fewer timesteps (<= 2 hours)?) , we will interpolate using zoo()

#    first see where in the curve this gap falls: 
span <- 12495:12510
plot(mart.wlight.df.2123$local_datetime[span], mart.wlight.df.2123$surfWaterNitrateMean[span], type = 'l')  #OK, linear seems reasonable

# Then fill
maxgap <- 8  # set the maximum gap for zoo() to fill
mart.wlight.df.2123$surfWaterNitrateMean <- na.approx(mart.wlight.df.2123$surfWaterNitrateMean, maxgap = maxgap)  # maxgap = max # of NAs to fill



# re-check NAs
which(is.na(mart.wlight.df.2123$surfWaterNitrateMean)) 

# no NAs remain

N_e <- mean(mart.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)  #15.567
N_sd <- sd(mart.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)   #6.575

## Select hourly no3 measurements to match what we've been doing  => CHANGE THIS LATER
mart.wlight.df.h <- mart.wlight.df.2123 %>%
  filter(minute(local_datetime) == 0) 
# Mean of selected dataset
N_e <- mean(mart.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE) #15.576
N_sd <- sd(mart.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE)  #6.581



```

###### Visualize cleaned data
```{r - visualize MART clean}

# jday distribution plot (do we have even coverage across a generic year?)
hist(mart.wlight.df.h$model_jday)

# big gaps in the middle of the year - things really seemed to wash out then (flashy stream?)

```


###### Save cleaned site df
```{r - Save cleaned df}

############################  Save cleaned site DF ###############################

path <- here("N_uptake_NEON/data/neon_data_clean/mart_clean.csv")
path_h <- here("N_uptake_NEON/data/neon_data_clean/mart_hourly_clean.csv")
write_csv(mart.wlight.df.2123, path)
write_csv(mart.wlight.df.h, path_h)


##### Reload data-in-progress as needed
# mart.wlight.df.2123 <- read_csv(path)
# mart.wlight.df.h <- read_csv(path_h)

```



[Did NOT model: Mayfield Creek [MAYF], Bibb, AL; McDiffett Creek [MCDI], Wabaunsee, KS; McRae Creek [MCRA], Linn, OR]

#### Oksrukuyik Creek, North Slope, AK
nitrate sensor lat-long: 68.669769	-149.142847	
elevation (m): 767.19
tz="US/Alaska"


```{r - OKSR}

##### Create object for OKSR => Oksrukuyik Creek, North Slope, AK

oksr.df <- selectRdata(data=no3_data_sensor, site="OKSR", tz="US/Alaska", span=2021:2024)


# oksr.df <- no3_data_sensor %>%
#   filter(siteID == "OKSR") %>% 
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Alaska"),
#          Jday = yday(local_datetime)) %>%
#   filter(year(local_datetime) == 2021:2023)

##### Visualize, select and clean OKSR data

yr <- 2023
# mo <- 1:2

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
oksr.df %>%
  filter(year(local_datetime) == yr) %>%
  # filter(month(local_datetime) == mo) %>%
  # ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  ylim(0,10) +
  # facet_wrap(~Jday) + 
  ggtitle("OKSR, 2023") +
  theme_bw()

##### ID Jdays to save for model




##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/oksr_df.csv")
write_csv(oksr.df, path)

```


#### Posey Creek, Warren, VA 
nitrate sensor lat-long: 38.895193	-78.147862	
elevation (m): 271.86
tz="US/Eastern"


```{r - POSE}

##### Create object for POSE => Posey Creek, Warren, VA 

pose.df <- selectRdata(data=no3_data_sensor, site="POSE", tz="US/Eastern", span=2021:2024)


# pose.df <- no3_data_sensor %>%
#   filter(siteID == "POSE") %>% 
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Eastern"),
#          Jday = yday(local_datetime)) %>%
#   filter(year(local_datetime) == 2021:2023)


##### Visualize, select and clean POSE data

yr <- 2023
# mo <- 1:2

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
pose.df %>%
  filter(year(local_datetime) == yr) %>%
  # filter(month(local_datetime) == mo) %>%
  # ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,50) +
  # facet_wrap(~Jday) + 
  ggtitle("POSE, 2023") +
  theme_bw()

##### ID Jdays to save for model



##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/pose_df.csv")
write_csv(pose.df, path)

```




#### Pringle Creek, Wise County, TX
nitrate sensor lat-long: 33.37836	-97.78134	
elevation (m): 251.34
tz="US/Central"

###### Load data

```{r - PRIN}
#| output: false
#| message: false

##### Create object for PRIN => Pringle Creek, Wise Co., TX

prin.df <- selectRdata(data=no3_data_sensor, site="PRIN", tz="US/Central", span=2021:2024)

prin_light.df <- read_csv(here("N_uptake_NEON/data/NSRDB_lightdata_clean/PRIN_satlight_all.csv")) %>%
  mutate(local_datetime = force_tz(local_datetime, tzone="US/Central"))

#tz(prin_light.df$local_datetime) <- "US/Mountain"

# make sure the tz assigned correctly - otherwise the join doesn't work correctly, because R/ tidyverse assumes the assigned tz is correct (so, offsets the join *as if* it needed to match different tzs vs. the same one)
tz(prin_light.df$local_datetime)
tz(prin.df$local_datetime)
# Note that model_datetime is assigned the same tz as local_datetime

# The function does this now: 
# prin.df <- no3_data_sensor %>%
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Mountain"),
#         Jday = yday(local_datetime)) %>%
#   filter(siteID == "PRIN", 
#          year(local_datetime) %in% 2021:2023) 
#   
```

###### Visualize, select and clean PRIN data
```{r - PRIN visualize data, select days, add sat. light}

##### Visualize data and select days

yr <- 2024
mo <- 6
plottitle <- "PRIN, Jun 2024"

quartz(width=6.5, height=6.5)
#quartz(width=10, height=4)
prin.df %>%
  filter(year(local_datetime) == yr) %>%
  filter(month(local_datetime) == mo) %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  ylim(10, 30) +
  facet_wrap(~Jday) +
  ggtitle(plottitle) +
  theme_bw()



prin_dy <- prin.df %>%
  select(local_datetime, surfWaterNitrateMean)

# Dygraph for interactive chart w slider
dygraph(data=prin_dy) %>%
  dyRangeSelector()


##### ID MODEL_JDAYS to save for model - this makes later steps work better (determining daily light, for example). ID the actual (local time) days, but code for model jdays
#     visual check to select 120+ complete-appearing days to model from the 2.5 year dataset
#     PRIN: lots of disturbance, April-May seem to be the best months for the autotrophic cycle (except 2024, where June was *lovely*); maybe a tough sensor placement? Often sudden peaks from ~ 8-noon, centered at 10a: sharper than other sites I've seen

# Jdays: 
##    2021: 181, 184, 186, 188, 234-237, 240-244, 249, 251, 253, 254, 255, 261, 265-269, 294, 295, 304, 318, 319, 324, 343, 344, 355, 356, 357, 

# 2021: maybes: [127-129, 170-171 -MESSY] [185-outliers]

##    2022: 4, 5, 6, 10, 11, 12, 13, 16, 17, 18, 19, 25, 27, 29, 30, 31, 38-41, 44, 45, 61, 68, 74-78, 84-87, 90, 91, 104-107, 110-112, 121, 130-132, 134-140, 182, 304, 305, 306, 310

# 2022-62 has declining Neq throughout day; 97-100(? - slanty baseline; april)]; [186, 187, weird sharp bump up ~ 10a]

##    2023: 55, 71, 72, 75, 77, 84, 85, 86, 88, 90, 94, 95, 96, 97, 100, 102-104, 106-111, 113, 115, 121-126, 137-143, 146-148, 149, 150, 151, 152, 328, 329, 337

# 2024: 48, 61, 69, 71, 72, 73, 74, 80, 95, 97, 118, 139, 140, 141, 142, 144, 145, 147, 159, 160, 161, 165-175, 177, 179


list.21 <- c(181, 184, 186, 188, 234, 235, 236, 237, 240, 241, 242, 243, 244, 249, 251, 253, 254, 255, 261, 265, 266, 267, 268, 269, 294, 295, 304, 318, 319, 324, 343, 344, 355, 356, 357)

list.22 <- c(4, 5, 6, 10, 11, 12, 13, 16, 17, 18, 19, 25, 27, 29, 30, 31, 38, 39, 40, 41, 44, 45, 61, 68, 74, 75, 76, 77, 78, 84, 85, 86, 87, 90, 91, 104, 105, 106, 107, 110, 111, 112, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 182, 304, 305, 306, 310)

list.23 <- c(55, 72, 75, 77, 84, 85, 86, 88, 90, 94, 95, 96, 97, 100, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 115, 121, 122, 123, 124, 125, 126, 137, 138, 139, 140, 141, 142, 143, 146, 147, 148, 149, 150, 151, 152, 328, 329, 337)

# Creating and visualizing dataframes from the selected days - 
#   PRIN looked really ... wobby and odd, so making sure these seem like reasonable model options

prin.df.21 <- prin.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(model_jday %in% list.21)

quartz(6.5, 6.5)
prin.df.21 %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(10, 30) +
  facet_wrap(~Jday) +
  ggtitle("PRIN 2021") +
  theme_bw()
  
prin.df.22 <- prin.df %>%  
  filter(year(local_datetime) == 2022) %>%
  filter(model_jday %in% list.22)

quartz(6.5, 6.5)
prin.df.22 %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(10, 30) +
  facet_wrap(~Jday) +
  ggtitle("PRIN 2022") +
  theme_bw()

prin.df.23 <- prin.df %>%  
  filter(year(local_datetime) == 2023) %>%
  filter(model_jday %in% list.23)

quartz(6.5, 6.5)
prin.df.23 %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(10, 30) +
  facet_wrap(~Jday) +
  ggtitle("PRIN 2023") +
  theme_bw()

# Check for complete days: should have 96 obs/ day

# check for days where obs != 96 (often leap years, or some sensor irregularity)

remove.21 <- prin.df.21 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.22 <- prin.df.22 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.23 <- prin.df.23 %>% 
  count(model_jday) %>%
  filter(n != 96)


length(prin.df.21$model_jday)/96 #35
length(prin.df.22$model_jday)/96 #57
length(prin.df.23$model_jday)/96 #48

# prin.df.21 %>% count(model_jday) # 35
# prin.df.22 %>% count(model_jday) # 57 (after removing jday 121 because of no3 spikes at end of model day) 
# prin.df.23 %>% count(model_jday) # 48 (after removing jday 71, w. only 92 obs)


##### Combine selected days into 1 df 
prin.df.2123 <- bind_rows(prin.df.21, prin.df.22, prin.df.23) %>%
  mutate(Year = year(model_datetime)) %>% 
  rename(UTC_startDateTime=startDateTime) %>%
  unite("yr_jday", Year:model_jday, remove=FALSE) %>%
  dplyr::select(siteID, yr_jday, model_datetime, surfWaterNitrateMean, surfWaterNitrateVariance, surfWaterNitrateStdErMean, rangeFailQM, UTC_startDateTime, local_datetime, Jday, model_jday, Year, domainID)


##### Combine df with sat_light data for the selected model days
prin.wlight.df.2123 <- left_join(x=prin.df.2123,
                                 y=prin_light.df, 
                                 by = "local_datetime") 


View(prin.wlight.df.2123)

# # check the 1st day in the df to make sure light joined correctly 
# jday31 <- prin_light.df %>% 
#   filter(year(local_datetime) == 2021) %>%
#   filter(month(local_datetime) == 1) %>% 
#   filter(day(local_datetime) == 31)


# save to keep progress
write_csv(prin.wlight.df.2123,file=here("N_uptake_NEON/data/neon_data_joined/PRIN_wlight_2123_joined.csv"))

```

###### Fill any gaps in no3 data
```{r fill gaps}

##############################  Reload data if needed  #########################
# prin.wlight.df.2122 <- read_csv(here("N_uptake_NEON/data/neon_data_joined/WALK_wlight_2122_joined.csv")) %>%
#   mutate(local_datetime = force_tz(local_datetime, tzone="US/Mountain"))


##############################  Where are the NAs?  ############################

no3NA <- which(is.na(prin.wlight.df.2123$surfWaterNitrateMean))
no3NA
# 20 NAs:  188   189  7651  7652  9053  9054  9055  9056 10914 10915 10916 10917 11338 12498 12499 12500 12501 12502 12503 12504

# 
# When do these occur? (ID any particularly bad days to remove)
NAdays <- prin.wlight.df.2123 %>%
  filter(is.na(surfWaterNitrateMean)) %>%   # Filter rows where 'no3mean' is NA
  select(yr_jday)                  # Select the 'Jday' column for those rows

NAdays # 2021: 184(2)
       # 2022: 132(2)
       # 2023:  72(4), 109(4), 121, 143(7)


lightNA <- which(is.na(prin.wlight.df.2123$GHI_wm2))
lightNA  
# none, whew

##############################   SMALL GAPS  ###################################

# For smaller NA chunks, (8 or fewer timesteps (<= 2 hours)?) , we will interpolate using zoo()

#    first see where in the curve this gap falls: 
span <- 12495:12510
plot(prin.wlight.df.2123$local_datetime[span], prin.wlight.df.2123$surfWaterNitrateMean[span], type = 'l')  #OK, linear seems reasonable

# Then fill
maxgap <- 8  # set the maximum gap for zoo() to fill
prin.wlight.df.2123$surfWaterNitrateMean <- na.approx(prin.wlight.df.2123$surfWaterNitrateMean, maxgap = maxgap)  # maxgap = max # of NAs to fill



# re-check NAs
which(is.na(prin.wlight.df.2123$surfWaterNitrateMean)) 

# no NAs remain

N_e <- mean(prin.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)  #15.567
N_sd <- sd(prin.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)   #6.575

## Select hourly no3 measurements to match what we've been doing  => CHANGE THIS LATER
prin.wlight.df.h <- prin.wlight.df.2123 %>%
  filter(minute(local_datetime) == 0) 
# Mean of selected dataset
N_e <- mean(prin.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE) #15.576
N_sd <- sd(prin.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE)  #6.581



```

###### Visualize cleaned data
```{r - visualize PRIN clean}

# jday distribution plot (do we have even coverage across a generic year?)
hist(prin.wlight.df.h$model_jday)

# big gaps in the middle of the year - things really seemed to wash out then (flashy stream?)

```


###### Save cleaned site df
```{r - Save cleaned df}

############################  Save cleaned site DF ###############################

path <- here("N_uptake_NEON/data/neon_data_clean/prin_clean.csv")
path_h <- here("N_uptake_NEON/data/neon_data_clean/prin_hourly_clean.csv")
write_csv(prin.wlight.df.2123, path)
write_csv(prin.wlight.df.h, path_h)


##### Reload data-in-progress as needed
# prin.wlight.df.2123 <- read_csv(path)
# prin.wlight.df.h <- read_csv(path_h)

```



#### Red Butte Creek, Salt Lake, UT
nitrate sensor lat-long: 40.78366	-111.79811	
elevation (m): 1689.47
tz="US/Mountain"


```{r - REDB}

##### Create object for REDB => Red Butte Creek, Salt Lake, UT

redb.df <- selectRdata(data=no3_data_sensor, site="REDB", tz="US/Mountain", span=2021:2024)


# redb.df <- no3_data_sensor %>%
#   filter(siteID == "REDB") %>% 
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Mountain"),
#          Jday = yday(local_datetime)) %>%
#   filter(year(local_datetime) == 2021:2023)

##### Visualize, select and clean REDB data

yr <- 2024
# mo <- 1:2

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
redb.df %>%
  filter(year(local_datetime) == yr) %>%
  # filter(month(local_datetime) == mo) %>%
   # ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,4) +
  # facet_wrap(~Jday) + 
  ggtitle("REDB, 2023") +
  theme_bw()


##### ID Jdays to save for model



##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/redb_df.csv")
write_csv(redb.df, path)

```



#### Sycamore Creek, Maricopa, AZ
nitrate sensor lat-long: 33.751675	-111.508603	
elevation (m): 643
tz="US/Arizona"

###### Load data
```{r - SYCA}
#| output: false
#| message: false

##### Create object for SYCA => Rio Cupeyes, San German Municipio, PR

syca.df <- selectRdata(data=no3_data_sensor, site="SYCA", tz="US/Arizona", span=2021:2024)

syca_light.df <- read_csv(here("N_uptake_NEON/data/NSRDB_lightdata_clean/SYCA_satlight_all.csv")) %>%
  mutate(local_datetime = force_tz(local_datetime, tzone="US/Arizona"))

# make sure the tz assigned correctly - otherwise the join doesn't work correctly, because R/ tidyverse assumes the assigned tz is correct (so, offsets the join *as if* it needed to match different tzs vs. the same one)
tz(syca_light.df$local_datetime)
tz(syca.df$local_datetime)
# Note that model_datetime is assigned the same tz as local_datetime

```

###### Visualize, select and clean SYCA data
```{r - SYCA visualize data, select days, add sat. light}

##### Visualize data and select days

yr <- 2021
mo <- 7
plottitle <- "SYCA, Jul 2021"
# ylim(20,30) for Jan-Jun 21, Jan-Jun 23; (20, 33) for ~ Aug-Dec 2023, Jan- 2024; (17,27) for Jul-Dec 21, most of 2022, Jul 23, Jan- 2024;    : (25,35) for Oct-Dec 22

quartz(width=6.5, height=6.5)
#quartz(width=10, height=4)
syca.df %>%
  filter(year(local_datetime) == yr) %>%
  filter(month(local_datetime) == mo) %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(5, 10) +
  facet_wrap(~Jday) + 
  ggtitle(plottitle) +
  theme_bw()


## Dygraph for interactive chart w slider - meh, doesn't work all that well for my purposes.
# syca_dy <- syca.df %>%
# select(local_datetime, surfWaterNitrateMean)
# 
# dygraph(data=syca_dy) %>%
#   dyRangeSelector()


##### ID MODEL_JDAYS to save for model
# doing this early this makes later steps work better (determining daily light, for example). ID the actual (local time) days, but code for model jdays

# visual check to select 120+ complete-appearing days to model from the 2.5 year dataset
# SYCA: COMMENTS  Clear diel no3 cycling; pattern shoots UP to pointy peak ~ 5-9a, then drops off sharply. Especially noticeable Feb 2021, March 2022, . 
# No data 2021: Jan, Jun-Dec; 2022: Jan; 2023: Jan (crazy-high data (90-120 umol/L), likely sensor error), Feb, [most of] Mar, Nov, Dec; 2024: Jan, Feb, 
# Apr-Nov 2022 - disturbance, and shifts so pattern peaks in afternoon (~10-17), w. trough in AM (~ 5-10); goes back to afternoon trough in Dec. 2022; same in June 2023

# ID Jdays: 

##    2021: 28, 32, 33, 36, 37, 41, 43, 47, 49, 50-70, 79-81, 87-90, 96-98, 103-109, 111-114, 116, 141, 142

##    2022: 34-46, 49-53, 56-62, 64-71, [72-78 - odd shape and 'nipple' at ~ 6-7:30a; nipple also in 64-71 but less evident], 350-352, 356-360

##    2023: 101-106 (a little nipple-y), 125-129, 132, 138, 139, 143-148, 150, [257, 263, 266, 274, 278, 279, 280, 281, 282, - VERY low (<1) but clear cycles]

# We may not use 2024, but it's easiest to ID good days Jan-June now
##    2024: 64, 97-100, 103-105, [112-116, nipple-y], 119-121, 125-129, 131, 138, 142, 145-147, 150-152, 155, 156


# create objects from selected jdays for each year
list.21 <- c(28, 32, 33, 36, 37, 41, 43, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 79, 80, 81, 87, 88, 89, 90, 96, 97, 98, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 116, 141, 142)

list.22 <- c(34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 350, 351, 352, 356, 357, 358, 359, 360)

list.23 <- c(101, 102, 103, 104, 105, 106, 125, 126, 127, 128, 129, 132, 138, 139, 143, 144, 145, 146, 147, 148, 150, 257, 263, 266, 274, 278, 279, 280, 281, 282)


# Use the objects to create dataframes of the selected *model* jdays (ensures complete 24-h model days, i.e. from 4a-3:59a)
syca.df.21 <- syca.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(model_jday %in% list.21)
  
syca.df.22 <- syca.df %>%  
  filter(year(local_datetime) == 2022) %>%
  filter(model_jday %in% list.22)

syca.df.23 <- syca.df %>%  
  filter(year(local_datetime) == 2023) %>%
  filter(model_jday %in% list.23)

# Check for complete days: should have 96 obs/ day
# causes of n < 96 are often time change days, or some sensor irregularity

remove.21 <- syca.df.21 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.22 <- syca.df.22 %>% 
  count(model_jday) %>%
  filter(n != 96)

remove.23 <- syca.df.23 %>% 
  count(model_jday) %>%
  filter(n != 96)

# All days, all 3 years had 96 obs. 

# Double-check for even days, and get count per df
length(syca.df.21$model_jday)/96 #54
length(syca.df.22$model_jday)/96 #41
length(syca.df.23$model_jday)/96 #30


##### Combine selected days into 1 df, select columns 
syca.df.2123 <- bind_rows(syca.df.21, syca.df.22, syca.df.23) %>%
  mutate(Year = year(model_datetime)) %>%  #changed to be consistent w. model_jday => important in SYCA where there are year-round good days!
  rename(UTC_startDateTime=startDateTime) %>%
  unite("yr_jday", Year:model_jday, remove=FALSE) %>%
  dplyr::select(siteID, yr_jday, model_datetime, surfWaterNitrateMean, surfWaterNitrateVariance, surfWaterNitrateStdErMean, rangeFailQM, UTC_startDateTime, local_datetime, Jday, model_jday, Year, domainID)

# check the # of days by dividing syca.df.2123 observations by 96 
# XXX for syca


##### Combine df with sat_light data for the selected model days
syca.wlight.df.2123 <- left_join(x=syca.df.2123,
                                 y=syca_light.df, 
                                 by = "local_datetime") 


View(syca.wlight.df.2123)


# check the 1st day in the df to make sure light joined correctly
jday21_28 <- syca_light.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(month(local_datetime) == 1) %>%
  filter(day(local_datetime) == 28)

View(jday21_28)

# save to keep progress
write_csv(syca.wlight.df.2123,file=here("N_uptake_NEON/data/neon_data_joined/SYCA_wlight_2123_joined.csv"))

```

###### Fill any gaps in no3 data
```{r fill gaps}

##############################  Reload data if needed  #########################
# syca.wlight.df.2123 <- read_csv(here("N_uptake_NEON/data/neon_data_joined/WALK_wlight_2123_joined.csv")) %>%
#   mutate(local_datetime = force_tz(local_datetime, tzone="US/Mountain"))


##############################  Where are the NAs?  ############################

### Set low/ clearly sensor-error values to NA: 

# First, ID whether there are any measurements <0
sensorfaildays <- syca.wlight.df.2123 %>%
  filter(surfWaterNitrateMean < 0)  ### 778! yikes. 


max(sensorfaildays$surfWaterNitrateMean) #-0.1
min(sensorfaildays$surfWaterNitrateMean) #-1
which(min(sensorfaildays$surfWaterNitrateMean) == -1)

unique(sensorfaildays$yr_jday) #24:  "2021_28" - OK  "2021_32" - kinda low  "2021_33"- OK  "2021_87"  "2021_88"  "2021_89"  "2021_90"  "2021_96"  "2021_97" "2021_98"  "2021_103" "2021_104" "2021_105" "2021_106" "2021_107" "2021_108" "2021_109" "2021_111" "2021_112" "2021_113" "2021_114" "2021_116" "2021_141" "2021_142"

# re-check the images: is this likely to be 0, or is it a sensor blip?  The curves look good, but seems like the sensor is reading low.. check flags

# then change the applicable <0 values to 0
# syca.wlight.df.2123 <- syca.wlight.df.2123 %>%
#   mutate(surfWaterNitrateMean = replace(surfWaterNitrateMean, surfWaterNitrateMean < 0, 0))


no3NA <- which(is.na(syca.wlight.df.2123$surfWaterNitrateMean))
no3NA

# 21 occurrences:  3497  3498  3687  4357  4451  4452  4453  4454  4455  4457  4841  4918  7229  7259  8831  9031  9816  10534 11179 11180 11181



# When do these occur? (ID any particularly bad days to remove)
NAdays <- syca.wlight.df.2123 %>%
  filter(is.na(surfWaterNitrateMean)) %>%   # Filter rows where 'no3mean' is NA
  select(yr_jday)                  # Select the 'Jday' column for those rows

NAdays  # 2021: 90(2), 97, 108, 109(6), 114, 116
        # 2022: 59(2), 357, 360, 
        # 2023: 126, 143, 257(3)

# all are missing < 2h of data, which is fine for a gap fill 

lightNA <- which(is.na(syca.wlight.df.2123$GHI_wm2))
lightNA  
# none, whew

##############################   SMALL GAPS  #####################################

# For smaller NA chunks, (8 or fewer timesteps (<= 2 hours)?) , we will interpolate using zoo()

#    first see where in the curve this gap falls: 5830:5855, 40060:40075
# print(syca.wlight.df.2123[5830:5860,])

span <- 4445:4460
plot(syca.wlight.df.2123$local_datetime[span], syca.wlight.df.2123$surfWaterNitrateMean[span], type = 'l')  #OK, linear seems reasonable for both gaps


# Then fill
maxgap <- 8  # set the maximum gap for zoo() to fill
syca.wlight.df.2123$surfWaterNitrateMean <- na.approx(syca.wlight.df.2123$surfWaterNitrateMean, maxgap = maxgap)  # maxgap = max # of NAs to fill



# re-check NAs
which(is.na(syca.wlight.df.2123$surfWaterNitrateMean)) 
# no NAs remain

N_e <- mean(syca.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)  #24.42304 then 24.4445 after removing very low/ neg #s
N_e
N_sd <- sd(syca.wlight.df.2123$surfWaterNitrateMean, na.rm = TRUE)   #2.786831 then 2.701031 after removing very low/ neg #s
N_sd

## Select hourly no3 measurements to match what we've been doing  => CHANGE THIS LATER
syca.wlight.df.h <- syca.wlight.df.2123 %>%
  filter(minute(local_datetime) == 0) 
# Mean of selected dataset
N_e <- mean(syca.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE) #24.4242 then 24.44639 after removing very low/ neg #s
N_e
N_sd <- sd(syca.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE)  #2.785572 then 2.701461 after removing very low/ neg #s
N_sd



```


###### Visualize cleaned data

```{r - visualize SYCA clean}

# jday distribution plot (do we have even coverage across a generic year?)
hist(syca.wlight.df.h$model_jday)

clean.syca.plot <- ggplot(syca.wlight.df.h, aes(x=yr_jday, y=surfWaterNitrateMean)) + 
  geom_point() + 
  ggtitle("SYCA - cleaned and filled data") +
  theme_bw()

  quartz(height=6, width=7.5)
  clean.syca.plot

```



###### Save cleaned site df
```{r - Save cleaned df}

############################  Save cleaned site DF ###############################

path <- here("N_uptake_NEON/data/neon_data_clean/syca_clean.csv")
path_h <- here("N_uptake_NEON/data/neon_data_clean/syca_hourly_clean.csv")
write_csv(syca.wlight.df.2123, path)
write_csv(syca.wlight.df.h, path_h)


##### Reload data-in-progress as needed
# syca.wlight.df.2123 <- read_csv(path)
# syca.wlight.df.h <- read_csv(path_h)

```




#### Teakettle Creek - Watershed 2, Fresno, CA
nitrate sensor lat-long: 36.955228	-119.023553	
elevation (m): 2002.75
tz="US/Pacific"


```{r - TECR}

##### Create object for TECR => Teakettle Creek - Watershed 2, Fresno, CA

tecr.df <- selectRdata(data=no3_data_sensor, site="TECR", tz="US/Pacific", span=2021:2024)


# tecr.df <- no3_data_sensor %>%
#   filter(siteID == "TECR") %>%
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Pacific"),
#          Jday = yday(local_datetime)) %>%
#   filter(year(local_datetime) == 2021:2023)


##### Visualize, select and clean TECR data

yr <- 2021
# mo <- 5:6

quartz(width=6.5, height=6.5)
# quartz(width=10, height=4)
tecr.df %>%
 filter(year(local_datetime) == yr) %>%
  # filter(month(local_datetime) == mo) %>%
  # ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(0,4) +
  # facet_wrap(~Jday) +
  ggtitle("TECR, 2021") +
  theme_bw()

##### ID Jdays to save for model




##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/tecr_df.csv")
write_csv(tecr.df, path)

```

[Did NOT model Lower Tombigbee River [TOMB], Choctaw, AL]

#### Walker Branch, TN
nitrate sensor lat-long: 35.957219	-84.279206	
elevation (m): 261.76
tz="US/Eastern"


```{r - WALK}

##### Create object for WALK  => Walker Branch, TN

walk.df <- selectRdata(data=no3_data_sensor, site="WALK", tz="US/Eastern", span=2021:2024) ## in Knox Co. = Eastern Time

# walk.df <- no3_data_sensor %>%
#   filter(siteID == "WALK") %>%
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Eastern"),
#          Jday = yday(local_datetime)) %>%  ## in Knox Co. = Eastern Time
#   filter(year(local_datetime) == 2021:2023)


##### Visualize, select and clean WALK data

yr <- 2023
mo <- 5:6

quartz(width=6.5, height=6.5)
#quartz(width=10, height=4)
walk.df %>%
  filter(year(local_datetime) == yr) %>%
  filter(month(local_datetime) == mo) %>%
  ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  # ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  #ylim(0,6) +
  facet_wrap(~Jday) +
  ggtitle("WALK, May-Jun 2023") +
  theme_bw()

##### ID Jdays to save for model




##### save site df
path <- here("N_uptake_NEON/data/neon_data_clean/walk_df.csv")
write_csv(walk.df, path)

```

#### West St Louis Creek, Grand, CO
nitrate sensor lat-long: 39.890673	-105.911297	
elevation (m): 2900.74
tz="US/Mountain"


###### Load data

```{r - WLOU}
#| output: false
#| message: false

##### Create object for WLOU => West St Louis Creek, Grand, CO

wlou.df <- selectRdata(data=no3_data_sensor, site="WLOU", tz="US/Mountain", span=2021:2023)

wlou_light.df <- read_csv(here("N_uptake_NEON/data/NSRDB_lightdata_clean/WLOU_satlight_all.csv")) %>%
  mutate(local_datetime = force_tz(local_datetime, tzone="US/Mountain"))

#tz(wlou_light.df$local_datetime) <- "US/Mountain"

# make sure the tz assigned correctly - otherwise the join doesn't work correctly, because R/ tidyverse assumes the assigned tz is correct (so, offsets the join *as if* it needed to match different tzs vs. the same one)
tz(wlou_light.df$local_datetime)
tz(wlou.df$local_datetime)
# Note that model_datetime is assigned the same tz as local_datetime

# The function does this now: 
# wlou.df <- no3_data_sensor %>%
#   mutate(local_datetime = with_tz(startDateTime, tzone="US/Mountain"),
#         Jday = yday(local_datetime)) %>%
#   filter(siteID == "WLOU", 
#          year(local_datetime) %in% 2021:2023) 
#   
```

###### Visualize, select and clean WLOU data
```{r - WLOU visualize data, select days, add sat. light}

##### Visualize data and select days

yr <- 2024
# mo <- 7:12

quartz(width=6.5, height=6.5)
#quartz(width=10, height=4)
wlou.df %>%
  filter(year(local_datetime) == yr) %>%
  # filter(month(local_datetime) == mo) %>%
  # ggplot(aes(x=hour(local_datetime), y = surfWaterNitrateMean)) +
  ggplot(aes(x=local_datetime, y=surfWaterNitrateMean)) +
  geom_point() + 
  geom_line() +
  # ylim(6,7) +
  # facet_wrap(~Jday) + 
  ggtitle("WLOU, 2024") +
  theme_bw(local_datetime, s)


wlou_dy <- wlou.df %>%
  select(local_datetime, surfWaterNitrateMean)

# Dygraph for interactive chart w slider
dygraph(data=wlou_dy) %>%
  dyRangeSelector()


##### ID MODEL_JDAYS to save for model - this makes later steps work better (determining daily light, for example). ID the actual (local time) days, but code for model jdays
#     visual check to select 120+ complete-appearing days to model from the 2.5 year dataset
#     WLOU: summer is both more disturbed and appears to have low autotrophic uptake (Deciduous shading? Cottonwoods?)

# Jdays: 
##    2021 - 32, 34:37, 40:59, 68:72, 74, (80:90 if only 1h missing), 95:113, 153:160 :164?), 238, 240, 241, 265, 268:270, 276:278, 285, 287:294, 299:307, 309:311, 313:321; 340:344  

##    2022 - 43, 53:56, 87:107, 116:123, 171, 173, 178, 179, 182:185, 202, 203, 207, 221:224, 235:240, 242, 244:255 (Xed 249 for missingness), 267:273, 279:283, 305:310, 316:318, 340, 343, 360, 363, 364

# 2024 looks like it has a great string of days ~ Feb-April

# Removed days in 2021 and 22 for either too few or too many measurements/day: these could not be interpolated /filled/ deleted due to the 45-min data structure that proceeded from the resumed measurement time. Noticed this when looking at the Jday count, below. Excising them seemed like the simplest solution. 
# Removed 2021-73 (n=30), 2021-311 (n=34), and 2022-310 (n=33)

# PAD for model day stuff - add a day on each end of a string of consecutive days. Original selections retained in 'orig.list's

list.21 <- c(32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 68, 69, 70, 71, 74, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 238, 240, 241, 265, 268, 269, 270, 276, 277, 278, 285, 287, 288, 289, 290, 291, 292, 293, 294, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 311, 313, 314, 315, 316, 317, 318, 319, 320, 321, 340, 341, 342, 343, 344)

# padded.list.21 <- c(31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 67, 68, 69, 70, 71, 72, 73, 74, 75, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 237, 238, 239, 240, 241, 242, 264, 265, 266, 267, 268, 269, 270, 271, 272, 275, 276, 277, 278, 279, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 339, 340, 341, 342, 343, 344)

list.22 <- c(43, 53, 54, 55, 56, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 116, 117, 118, 119, 120, 121, 122, 123, 171, 173, 178, 179, 182, 183, 184, 185, 202, 203, 207, 221, 222, 223, 224, 235, 236, 237, 238, 239, 240, 242, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 267, 268, 269, 270, 271, 272, 273, 279, 280, 281, 282, 283, 305, 306, 307, 308, 310, 316, 317, 318, 340, 343, 360, 363, 364)

# padded.list.22 <- c(42, 43, 44, 52, 53, 54, 55, 56, 57, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 170, 171, 172, 173, 174, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 201, 202, 203, 204, 206, 207, 208, 220, 221, 222, 223, 224, 225, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 266, 267, 268, 269, 270, 271, 272, 273, 274, 278, 279, 280, 281, 282, 283, 283, 304, 305, 306, 307, 308, 309, 310, 215, 316, 317, 318, 319, 339, 340, 341, 342, 343, 344, 359, 360, 361, 363, 363, 364)

wlou.df.21 <- wlou.df %>%
  filter(year(local_datetime) == 2021) %>%
  filter(model_jday %in% list.21)
  
wlou.df.22 <- wlou.df %>%  
  filter(year(local_datetime) == 2022) %>%
  filter(model_jday %in% list.22)

wlou.df.21 %>% count(model_jday) #106: after removing Jday73 (n=30 w. 45-min data) and Jday311 (n=34)  
# model_jday => #72 and 73 have 92 meas and # 310 has 100 (time change?)
wlou.df.22 %>% count(model_jday) #92: after removing Jday310 (n=33 w. 45-min data)
# model_jday => rm 309, which has 100 meas  

# wlou22_310 <- wlou.df.22 %>% filter(model_jday == 310)
# wlou21_310 <- wlou.df.22 %>% filter(model_jday == 310)
# View(wlou73)
# View(wlou310)
# 
# wlou310 <- wlou.df.22 %>% filter(Jday == 310)
# View(wlou310)


##### Combine selected days into 1 df 
wlou.df.2122 <- bind_rows(wlou.df.21, wlou.df.22) %>%
  mutate(Year = year(model_datetime)) %>% 
  rename(UTC_startDateTime=startDateTime) %>%
  unite("yr_jday", Year:model_jday, remove=FALSE) %>%
  dplyr::select(siteID, yr_jday, model_datetime, surfWaterNitrateMean, surfWaterNitrateVariance, surfWaterNitrateStdErMean, rangeFailQM, UTC_startDateTime, local_datetime, Jday, model_jday, Year, domainID)

# check the # of days - CLUNKY
#unique(wlou.df.2122$yr_jday) #198


##### Combine df with sat_light data for the selected model days
wlou.wlight.df.2122 <- left_join(x=wlou.df.2122,
                                 y=wlou_light.df, 
                                 by = "local_datetime") 


View(wlou.wlight.df.2122)

# # check the 1st day in the df to make sure light joined correctly 
# jday31 <- wlou_light.df %>% 
#   filter(year(local_datetime) == 2021) %>%
#   filter(month(local_datetime) == 1) %>% 
#   filter(day(local_datetime) == 31)


# save to keep progress
write_csv(wlou.wlight.df.2122,file=here("N_uptake_NEON/data/neon_data_joined/WLOU_wlight_2122_joined.csv"))

```

###### Fill any gaps in no3 data
```{r fill gaps}

##############################  Reload data if needed  #########################
# wlou.wlight.df.2122 <- read_csv(here("N_uptake_NEON/data/neon_data_joined/WALK_wlight_2122_joined.csv")) %>%
#   mutate(local_datetime = force_tz(local_datetime, tzone="US/Mountain"))


##############################  Where are the NAs?  ############################

no3NA <- which(is.na(wlou.wlight.df.2122$surfWaterNitrateMean))
no3NA
# 32 occurrences: 3145  3241  3259  3337  3433  3529  3625  3721  3817  3913  4009  4105  4201 4297  4300  4393  4489  4585  4681  4777  4873  5425  5426  6029 14359 14360 16476 17035 17036 17037 17038 18856

# 33 occurrences, using model days: [1]  2966  3033  3129  3147  3225  3321  3417  3513  3609  3705  3801  3897  3993  4089  4185  4188  4281 4377  4473  4569  4665  4761  5313  5314  5917 14247 14248 16364 16923 16924 16925 16926 18744

# When do these occur? (ID any particularly bad days to remove)
NAdays <- wlou.wlight.df.2122 %>%
  filter(is.na(surfWaterNitrateMean)) %>%   # Filter rows where 'no3mean' is NA
  select(yr_jday)                  # Select the 'Jday' column for those rows

NAdays # 2021: 74, 95, 96, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 113, 158, 158, 164, ## 2022: 202, 202, 253, 270, 270, 270, 270, 360

# 270 is missing 1 hour of data, that's fine for a gap fill. 

lightNA <- which(is.na(wlou.wlight.df.2122$GHI_wm2))
lightNA  
# none, whew

##############################   SMALL GAPS  #####################################

# For smaller NA chunks, (8 or fewer timesteps (<= 2 hours)?) , we will interpolate using zoo()

#    first see where in the curve this gap falls: 
span <- 16900:16930
plot(wlou.wlight.df.2122$local_datetime[span], wlou.wlight.df.2122$surfWaterNitrateMean[span], type = 'l')  #OK, linear seems reasonable

# Then fill
maxgap <- 8  # set the maximum gap for zoo() to fill
wlou.wlight.df.2122$surfWaterNitrateMean <- na.approx(wlou.wlight.df.2122$surfWaterNitrateMean, maxgap = maxgap)  # maxgap = max # of NAs to fill



# re-check NAs
which(is.na(wlou.wlight.df.2122$surfWaterNitrateMean)) 

# no NAs remain

N_e <- mean(wlou.wlight.df.2122$surfWaterNitrateMean, na.rm = TRUE)  #3.028714
N_sd <- sd(wlou.wlight.df.2122$surfWaterNitrateMean, na.rm = TRUE)   #1.337295

## Select hourly no3 measurements to match what we've been doing  => CHANGE THIS LATER
wlou.wlight.df.h <- wlou.wlight.df.2122 %>%
  filter(minute(local_datetime) == 0) 
# Mean of selected dataset
N_e <- mean(wlou.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE) #3.028401
N_sd <- sd(wlou.wlight.df.h$surfWaterNitrateMean, na.rm = TRUE)  #1.33781



```
###### Visualize cleaned data

```{r - visualize WLOU clean}

# jday distribution plot (do we have even coverage across a generic year?)
hist(wlou.wlight.df.h$model_jday)



```



###### Save cleaned site df
```{r - Save cleaned df}

############################  Save cleaned site DF ###############################

path <- here("N_uptake_NEON/data/neon_data_clean/wlou_clean.csv")
path_h <- here("N_uptake_NEON/data/neon_data_clean/wlou_hourly_clean.csv")
write_csv(wlou.wlight.df.2122, path)
write_csv(wlou.wlight.df.h, path_h)


##### Reload data-in-progress as needed
# wlou.wlight.df.2122 <- read_csv(path)
# wlou.wlight.df.h <- read_csv(path_h)

```


