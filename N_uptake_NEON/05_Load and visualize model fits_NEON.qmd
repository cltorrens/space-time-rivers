---
title: "Load and visualize fits for NEON NO3 models"
author: "Christa Torrens"
format: 
  html: 
    code-fold: true #enables collapsible code blocks
    code-summary: "Show the code"
    self-contained: true #easier to share, no supplemental files
editor: visual
execute:
    message: false
---

# Intro

This document uses tidybayes to extract and visualize model fit params for the NEON data/ autotrophic uptake project

First created 2/25/2025 by Christa Torrens, with ongoing amendments

### Load needed libraries and model fits

```{r - load libraries}
#| warning: false
#| output: false

# Load libraries
library(tidyverse)
library(tidybayes)
library(bayesplot)
library(rstanarm)
library(here)
library(plotly)
library(sf)
library(MetBrewer)
library(PNWColors)
library(ggpubr)
library(cowplot)
library(ragg)
library(latex2exp)


```

### Plotting functions

This keeps all of the core plotting code in 1 place, and allows site-level customization (mostly dataframes and titles)

```{r - plotting functions}

##### Model fit - Npred + Nobs over time

plot_Nmodelfit_ts <- function(data, start_jday, end_jday, plot_title) {
  plot <- data %>%
    filter(model_jday >= start_jday & model_jday <= end_jday) %>%
    ggplot(aes(x = mod_hours)) +
    geom_point(aes(y = N_conc)) + 
    geom_line(aes(y = N_pred), col = 'red') +
    # labs(                                       ## ggplot only
    #   x = "Time (h)",
    #   y = expression("N" ~ (mmol ~ m^-3))
    # ) +
    labs(                                         ## better ggplot
      x = "Time (h)",
      y = latex2exp::TeX("N (mmol~m^{-3})")
      
    ) + 
    ggtitle(plot_title) +
    facet_wrap(~yr_jday) +
    theme_bw()
}

plot_Nmodelfit_tsCI <- function(data, start_jday, end_jday, plot_title) {
  plot <- data %>%
    filter(model_jday >= start_jday & model_jday <= end_jday) %>%
    ggplot(aes(x = mod_hours)) +
    geom_ribbon(aes(ymin=N_pred.lower, ymax=N_pred.upper), alpha = 0.7, fill = "slategray") +
    geom_point(aes(y = N_conc)) + 
    geom_line(aes(y = N_pred), col = 'red') +
    # labs(                                       ## ggplot only
    #   x = "Time (h)",
    #   y = expression("N" ~ (mmol ~ m^-3))
    # ) +
    labs(                                         ## better ggplot
      x = "Time (h)",
      y = latex2exp::TeX("N (mmol~m$^{-3}$)")
      
    ) + 
    ggtitle(plot_title) +
    facet_wrap(~yr_jday) +
    theme_bw()
}

##### Model fit - Npred v Nobs w. 1:1 line

plot_Npred_v_Nobs <- function(data, plot_title) {
  plot <- ggplot(data=data, aes(x=N_conc, y=N_pred, 
                                text = paste("DateTime:", model_datetime, "<br>Year-jday", yr_jday) # for plotly 
                                )) +
  geom_point() + 
  # labs( x=expression("measured N" ~(mmol~m^-3)), # OR (mu*mol~L^-1)             ## ggplot only
  #       y=expression("predicted N" ~(mmol~m^-3))  # ~ = a space, * = no space
  #     ) + 
    
  # labs( x=latex2exp::TeX("measured N//(mmol~m^${-3})"), # OR (mu*mol~L^-1)        ## better ggplot
  #       y=latex2exp::TeX("predicted N//(mmol~m$^{-3}$)")  # ~ = a space, * = no space
  #     ) + 
  #   
    labs(
      x = "measured N (mmol m^-3)",
      y = "predicted N (mmol m^-3)",
      title = plot_title
    ) +
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  theme_bw()
}

##### Equifinality checks
# added error bars to all equifinality checks, 4/16/25

## K vs U

plot_KvU <- function(data, plot_title) {
  plot <- ggplot(data=data, aes(y=K, x=U, 
                                text = paste("Date:", model_date, "<br>Year-jday", yr_jday) # for plotly
                                )) +
   geom_errorbar(aes(ymin = K.lower, ymax = K.upper), 
                 width = 0.2, color='grey70') + # CI bars
   geom_point() + 
   # labs(y=expression("K[nit]"~(d^-1)),                   ## ggplot only
   #     x=expression("U[auto]"~(mmol~m^-2~d^-1))
   #      ) +
    
   # labs(y=latex2exp::TeX("$K_{nit}~(d^{-1})$"),                  ## better ggplot
   #     x=latex2exp::TeX("$U_{auto}~(mmol~m^{-2}~d^{-1})$")
   #      ) +
    
   labs(y = "$K_{nit}~(d^{-1})$",                         ## for plotly - plotly interprets MathJax
        x = "$U_{auto}~(mmol~m^{-2}~d^{-1})$"
        ) +
   ggtitle(plot_title) + 
  # geom_abline(intercept = 0, slope = 1, color = "red") + # doesn't show up - off the charts!
   theme_bw()

}
  

## K vs N_e

plot_KvNe <- function(data, plot_title) {
  plot <- ggplot(data=data, aes(y=K, x=N_e,
                                text = paste("Date:", model_date, "<br>Year-jday", yr_jday) # for plotly
                                )) +
   geom_errorbar(aes(ymin = K.lower, ymax = K.upper), 
                 width = 0.2, color='grey70') + # CI bars
   geom_point() + 
   # labs(y=expression("modeled K"~(d^-1)),               ## ggplot only
   #      x=expression("modeled N_e"~(mmol~m^-3)) 
   #      ) + 
    
   # labs(y=latex2exp::TeX("$K_{nit}~(d^{-1})$"),         ## better ggplot
   #      x=latex2exp::TeX("$N_{eq}~(mmol~m^{-3})$")
   #      ) + 
   
   labs(x = "$K_{nit}~(d^{-1})$",                          ## for plotly - plotly interprets MathJax
        y = "$N_{eq}~(mmol~m^{-3})$"
        ) + 
   ggtitle(plot_title) + 
   # geom_abline(intercept = 0, slope = 1, color = "red") + # doesn't show up - off the charts!
   theme_bw()
}


## U vs N_e
# Includes CI bars because this relationship tends to look a bit linear - checking whether that holds even w ci values in place

plot_UvNe <- function(data, plot_title) {
  plot <- ggplot(data=data, aes(y=U, x=N_e,
                                text = paste("Date:", model_date, "<br>Year-jday", yr_jday) # for plotly
                                )) +
   geom_errorbar(aes(ymin = U.lower, ymax = U.upper), 
                width = 0.2, color='grey70') + # CI bars
   geom_point() + 
   # labs(y=expression("modeled U"~(mmol~m^-2~d^-1)),           ## gglot only
   #      x=expression("modeled N_e"~(mmol~m^-3)) 
   #      ) + 
  
   # labs(y=latex2exp::TeX("$U_{auto}~(mmol~m^{-2}~d^{-1})$"),    ## better ggplot
   #      x=latex2exp::TeX("$N_{eq}~(mmol~m^{-3})$") 
   #      ) +
    
    labs(x = "$U_{auto}~(mmol~m^{-2}~d^{-1})$",           ## for plotly - plotly interprets MathJax
         y = "$N_{eq}~(mmol~m^{-3})$"
         ) + 

   ggtitle(plot_title) + 
   theme_bw()
}


##### K over time

plot_K_ts <- function(data, plot_title) {
  plot <- ggplot(data=data, aes(x=d, y=K, 
                                text = paste("Date:", model_date, "<br>Year-jday", yr_jday) # for plotly
                                )) +
   geom_errorbar(aes(ymin = K.lower, ymax = K.upper),
                 width = 0.2, color='grey70') + # CI bars
   geom_point() +
  # geom_point(y = sumlight.real, color = 'gold') +
    
   # labs( x= "Index",                                                    ## ggplot only
   #       y=expression("modeled K"~(d^-1)) # ~ = a space, * = no space
   #    ) +
    
   # labs( x= "Index",                                                      ## better ggplot
   #       y=latex2exp::TeX("$K_{nit}~(d^{-1})$")  # ~ = a space, * = no space
   #    ) +
   #  
   labs( x = "Index",                                      ## for plotly - plotly interprets MathJax
         y = "$K_{nit}~(d^{-1})$"   # ~ = a space, * = no space
      ) +
  
   ggtitle(plot_title) +
   theme_bw()
}

  
##### U over time

plot_U_ts <- function(data, plot_title) {
  ggplot(data=data, aes(x=d, y=U,
                        text = paste("Date:", model_date, "<br>Year-jday", yr_jday) # for plotly
                        )) +
   geom_errorbar(aes(ymin = U.lower, ymax = U.upper), 
                 width = 0.2, color='grey70') + # CI bars
   geom_point() + 
  # geom_point(y = sumlight.real_wlou, color = 'gold') +
  # ADD IN HIGH AND LOW CIs
   # labs( x= "Index",                                                    ## ggplot only
   #      y=expression("modeled U"~(mmol~m^-2~d^-1)) # ~ = a space, * = no space
   #    ) +
    
   # labs( x= "Index",                                                      ## better ggplot
   #      y=latex2exp::TeX("$U_{auto}~(mmol~m^{-2}~d^{-1})$") # ~ = a space, * = no space
   #    ) +

   labs( x= "Index",                                     ## for plotly - plotly interprets MathJax
        y="$U_{auto}~(mmol~m^{-2}~d^{-1})$" # ~ = a space, * = no space
      ) +
    
   ggtitle(plot_title) +
   theme_bw()
}


##### U vs. real sumlight

plot_UvLight <- function(data, plot_title) {
  plot <- ggplot(data=data, aes(x=sumlightReal, y=U)) +
  geom_point() + 
  # labs( x=expression("measured daily light"~(w~m^-2)),             ## ggplot only
  #       y=expression("modeled U"~(mmol~m^-2~d^-1))                  # ~ = a space, * = no space
  #     ) + 
  
  # labs( x=latex2exp::TeX("measured daily light~(w~m$^{-2}$)"),      ## better ggplot
  #       y=latex2exp::TeX("$U_{auto}~(mmol~m^{-2}~d^{-1})$")          # ~ = a space, * = no space
  #     ) + 
    
  labs( x="measured daily light~(w~m$^{-2}$)",             ## better ggplot
        y="$U_{auto}~(mmol~m^{-2}~d^{-1})$"                # ~ = a space, * = no space
      ) +
    
  scale_x_log10() + scale_y_log10() + 
  ggtitle(plot_title) +
  theme_bw()
}


##### U + daily (or hourly?) sumlight over time



##### Autotrophic U vs total U

plot_autoUperc <- function(data, plot_title) {
  plot <- ggplot(data=data, aes(x=d, y=U_auto_perc, 
                                 text = paste("Date:", model_date, "<br>Year-jday", yr_jday) # for plotly
                                )) +
  geom_point() + 
  xlab("Index") + ylab("Percent autotrophic uptake") + 
  ggtitle(plot_title) + 
  theme_bw()
}


```

### Load GPP dataset (full)

```{r load gpp}

gpp.df <- read_csv(file=here("N_uptake_NEON/GPP_model/neon_metab_data.csv")) 

gpp.df <- as_tibble(gpp.df) %>%
  mutate(site = toupper(site))  # capitalizing to match the other dataframes
# Split by siteID
site_list <- split(gpp.df, gpp.df$site)
# Add '.q' suffix to each name
names(site_list) <- paste0(names(site_list), ".gpp")
# Create objects in global environment
list2env(site_list, envir = .GlobalEnv)

# Only WLOU and CARI foverlap for 2021-2023 data


```

### Load LINX II Vf data

```{r LINX II data}

linx.df <- read_csv(here("N_uptake_NEON/data/LINXII_data/LINX_II_raw_data.csv")) %>%
  dplyr::select(state, stream, Date_15Nexpt, water_velocity_m_min,Q_L_s,NO3conc_ugN_L, GPP_gO2_m2_d, width_m, depth_m, Vf_tot_no3_cm_s, Vfdentot_cm_s, Vf_assim_cm_s) %>%
  rename(Date = Date_15Nexpt, 
         v_m_min = water_velocity_m_min, 
         Q_Ls = Q_L_s)

```

### Load model fits and data by site (stored as RDS files), then extract parameters

```{r, error notes}
#| echo: false

# Hmmm: error notes when loading the rds files:
# 
# The legacy packages maptools, rgdal, and rgeos, underpinning the sp package, which was just loaded, will retire in October 2023. Please refer to R-spatial evolution reports for details, especially <https://r-spatial.org/r/2023/05/15/evolution4.html.> It may be desirable to make the sf package available; package maintainers should consider adding sf to Suggests:. The sp package is now running under evolution status 2 (status 2 uses the sf package in place of rgdal)

# I've added sf() to the library block

```

#### BIGC

Outputs and visualizations for Upper Big Creek, Fresno, CA

###### Load model fits and data

```{r load BIGC model fit and data}
#| output: false
#| message: false
#| warning: false

########## Load model fit ______________________________________________________

# bigc.fit <- readRDS(here("N_uptake_NEON/data/model_fits/bigc.fit.rds"))

bigc.fit <- readRDS(here("N_uptake_NEON/data/model_fits/pooledK_logK/bigc.fit2.rds"))

# if working from model run
# bigc.fit <- fit.bigc


########## Load model data _____________________________________________________

bigc.modeldata <- readRDS(here("N_uptake_NEON/data/model_datalist/bigc.data.rds"))


########## Load full daily and hourly datasets _________________________________
# these include datatime info, and the model data does not)

# # 15-min dataset
# path <- here("N_uptake_NEON/data/neon_data_clean/bigc_clean.csv")
# bigc.df <- read_csv(path) %>%
#   mutate(local_datetime = with_tz(local_datetime, tzone="US/Pacific"), 
#          model_datetime = local_datetime - hours(4))  


# hourly dataset
path_h <- here("N_uptake_NEON/data/neon_data_clean/bigc_hourly_clean.csv")
bigc.df.h <- read_csv(path_h) %>%
  mutate(local_datetime = with_tz(local_datetime, tzone="US/Pacific"), 
         model_datetime = local_datetime - hours(4))  

# daily dataseset for Q, v, z
bigc.qvz <- readRDS(file=here("N_uptake_NEON/data/neon_data_clean/bigc.q.daily.rds"))
  
bigc.qvz <- bigc.qvz %>%
  dplyr::filter(yr_jday %in% bigc.df.h$yr_jday)

```

###### Extract model parameters and real-data values

First, check the fit for sigmas and betas ("sigma", "sigma_U", "b0", "b1"), since these are notoriously tough to fit. Then put the spread and mean/ median/ quartile info for K, U, and N_e into a BIGC tibble (to be saved in one large csv at the end of the document). Finally, assemble tibbles for the visualizations, which include both modeled parameters and real data.

```{r - bigc: extract model params and real-data values}
#| echo: false
#| message: false
#| warning: false

# get list of variables for the model fit
# get_variables(bigc.fit) - too many to see all, but informs re: structure [hour,day] for hourly variables

########## Check the fit for sigmas and betas (tough to fit): __________________
# print(bigc.fit, pars=c("sigma", "sigma_U", "b0", "b1"))
print(bigc.fit, pars=c("sigma", "sigma_U", "sigma_logK", "logK_mean", "b0", "b1"))


########## Summarize daily variables (K, U, N_e) _______________________________

# First, use spread_draws for the daily parameters - this gives each parameter its own column
# d = days (198), chain = 4, iteration = 1K, draw = 4K (1 per iteration per chain)
bigc.spread <- spread_draws(bigc.fit, K[d], U[d], U_tot[d], Vf_tot[d], N_e[d]) 

# Then get the mean/median + 95% QI (CI) for each parameter/day
# median + QI
bigc.medqi <- bigc.spread %>%
  # dplyr::group_by(d) %>%  # Unnecessary: tidybayes automatically groups by the selected spread_draws group [in this case, 'd']
  median_qi() %>%
  add_column(site = "BIGC")

# mean + QI
bigc.meanqi <- bigc.spread %>%
  mean_qi() %>%
  add_column(site = "BIGC")

### Add median param estimate info to the meanqi df
bigc.meanqi$K.median <- bigc.medqi$K
bigc.meanqi$U.median <- bigc.medqi$U
bigc.meanqi$U_tot.median <- bigc.medqi$U_tot
bigc.meanqi$Vf_tot.median <- bigc.medqi$Vf_tot
bigc.meanqi$N_e.median <- bigc.medqi$N_e



##### Get draws for b1 and b0 (for plotting later)
bigc.betas <- spread_draws(bigc.fit, b0, b1) %>%
  sample_n(max(bigc.medqi$d))

########## Summarize the hourly variable (conc_pred) ____________________________

# Again, using spread_draws for the hourly parameter, conc_pred (predicted N concentration)
bigc.spread.h <- spread_draws(bigc.fit, conc_pred[h,d])
# View(bigc.spread.h)

# Getting the mean and 95% credible interval values (= 0.95 QI)
bigc.meanqi.h <- bigc.spread.h %>%
  dplyr::group_by(d, h) %>%  # This command IS needed here: tidybayes *did* have h + d column outputs, but the values were both different and less precise than when grouping by (d, h). 
  mean_qi() %>%
  add_column(site = "BIGC")

# median + 0.95 QI
bigc.medqi.h <- bigc.spread.h %>%
  dplyr::group_by(d, h) %>% #again, explicit grouping is needed here
  median_qi() %>%
  add_column(site = "BIGC")

### Add median param info to the hourly meanqi df
bigc.meanqi.h$conc_pred.median <- bigc.medqi.h$conc_pred


########## Compile dataframes ___________________________________________________

####### Hourly dataframe: N and N_pred #############

# start w. the hourly dataframe and add mean, QI, modeldata info
N_output_bigc.df <- bigc.df.h %>%  
  add_column(N_pred = bigc.meanqi.h$conc_pred, 
         N_pred.lower = bigc.meanqi.h$.lower, 
         N_pred.upper = bigc.meanqi.h$.upper, 
         z=as.vector(bigc.modeldata$zMA), 
         site = "BIGC") %>%
  rename(N_conc = surfWaterNitrateMean, 
         lightReal = GHI_wm2) %>%
   mutate(hours = hour(local_datetime), 
         mod_hours = hour(model_datetime)) %>%
  dplyr::select(site, local_datetime, model_datetime, N_conc, N_pred, N_pred.lower, N_pred.upper, lightReal, z, Year, yr_jday, Jday, model_jday, hours, mod_hours) 

####### Daily dataframe: U, K, N_e, b0, b1 ##################

# Create a domainID vector length(nday) (may be handy later)
nday <- bigc.modeldata$D
domainID <- bigc.df.h$domainID[1:nday]

# start w. the meanqi dataframe and add modeldata and datetime info as needed
daily_output_bigc.df <- bigc.meanqi %>%
  add_column(yr_jday = unique(bigc.df.h$yr_jday), 
             sumlightReal = bigc.modeldata$sumlightReal, 
             sumlightIdeal = bigc.modeldata$sumlightIdeal,
             Q = bigc.qvz$Q_cms,
             v = bigc.qvz$v,
             z = colMeans(bigc.modeldata$zMA), # daily depth = avg of hourly depth
             b0 = bigc.betas$b0,
             b1 = bigc.betas$b1,
             domainID = domainID) %>%
  separate(yr_jday, into = c("Year", "model_jday"), sep = "_", convert = TRUE, remove = FALSE) %>% # makes the new columns numeric if possible, and retains the old column
  mutate(model_date = ymd(paste0(Year, "-01-01")) + days(model_jday - 1)) %>%
  dplyr::select(site, yr_jday, K, K.lower, K.upper, K.median, U, U.lower, U.upper, U.median, U_tot, U_tot.lower, U_tot.upper, U_tot.median, Vf_tot, Vf_tot.lower, Vf_tot.upper, Vf_tot.median, N_e, N_e.lower, N_e.upper, N_e.median, sumlightReal, sumlightIdeal, Q, v, z, b0,b1, model_date, model_jday, Year, domainID, d) #d is the index from the meanqi object - used for plotting 'ts' by index
         
# Check means for all 3 params
# mean(daily_output_bigc.df$K) # 3.225407
# mean(daily_output_bigc.df$U) # 0.1986689
# mean(daily_output_bigc.df$N_e) # 3.395281

```

###### Create visualizations

```{r - bigc: create visualizations}
#| echo: true

########## N Model fit to data __________________________________________________

######  N model fit over time - predicted N vs measured N  #######

N_Npred_ts_bigc <- plot_Nmodelfit_ts(data=N_output_bigc.df, start_jday = 331, end_jday = 365, plot_title = "NEON N data + model predictions: Upper Big Creek, CA")

quartz(width=6,height=6)
N_Npred_ts_bigc

### w. credible interval ribbon
N_Npred_tsCI_bigc <- plot_Nmodelfit_tsCI(data=N_output_bigc.df, start_jday = 35, end_jday = 60, plot_title = "NEON N data + model predictions: Upper Big Creek, CA")

# quartz(width=6,height=6)
N_Npred_tsCI_bigc

######  predicted N vs measured N + 1:1 line  #######

Npred_v_N_bigc <- plot_Npred_v_Nobs(data=N_output_bigc.df, plot_title="Predicted N vs. measured N, Upper Big Creek, CA")

# quartz(width=6, height=6)
Npred_v_N_bigc


########## Equifinality checks _________________________________________________

####### Check equifinality!! K vs U  #######  

KvU_bigc <- plot_KvU(data=daily_output_bigc.df, plot_title = "Equifinality check, Upper Big Creek, CA: K_nit vs. U_auto")
  
quartz(width=7.5, height=6)
KvU_bigc

ggplotly(KvU_bigc, tooltip = "text")
  

####### Check equifinality!! K vs N_e  #######  

KvNe_bigc <- plot_KvNe(data=daily_output_bigc.df, plot_title = "Equifinality check, Upper Big Creek, CA:  K_nit vs. equilibrium N")

quartz(width=7.5, height=6)
KvNe_bigc

ggplotly(KvNe_bigc, tooltip = "text")

####### Check equifinality!! U vs N_e  #######  

UvNe_bigc <- plot_UvNe(data=daily_output_bigc.df, plot_title = "Equifinality check, Upper Big Creek, CA: U_auto vs. equilibrium N")
 
quartz(width=7.5, height=6)
UvNe_bigc

ggplotly(UvNe_bigc, tooltip = "text")


########## Other parameter visualizations _______________________________________

#######  K over time  #######  

K_v_time_bigc <- plot_K_ts(data=daily_output_bigc.df, plot_title = "K_nit over time, Upper Big Creek, CA")

quartz(width=6.5, height=6)
K_v_time_bigc

ggplotly(K_v_time_bigc, tooltip = "text")

##### U over time  #######  

U_time_bigc <- plot_U_ts(data=daily_output_bigc.df, plot_title="U_auto, Upper Big Creek, CA")

quartz(width=6, height=6)
U_time_bigc

ggplotly(U_time_bigc, tooltip = "text")

###### U vs sumlight  #######  

U_vs_light_bigc <- plot_UvLight(data=daily_output_bigc.df, plot_title = "Scatterplot of U-auto and daily light: Upper Big Creek, CA")

quartz(width=6.5, height=6)
U_vs_light_bigc


```

###### Compare U_total and U_auto

What percentage of total uptake is the autotrophic uptake?

```{r - bigc: percent autotrophic NO3 uptake}

# NON-AUTOTROPHIC UPTAKE = Equilibrium nitrate * K *z
# TOTAL UPTAKE = U + (Equilibrium nitrate * K)  
# Since U_auto is very small, there won't be much difference between 'non-autotrophic uptake' and 'total uptake'

daily_output_bigc.df <- daily_output_bigc.df %>%
  mutate(U_other = N_e*K*z,      
         U_tot1 = U_tot + U,
         U_tot2 = U_other + U,
         U_auto_perc = 100*U/U_tot1) 

plot(daily_output_bigc.df$U_tot, daily_output_bigc.df$U_other) # 1:1

U_auto_avg <- mean(daily_output_bigc.df$U_auto_perc) #12.5% on average

Uauto_perc_bigc <- plot_autoUperc(data=daily_output_bigc.df, plot_title = "Daily  autotrophic uptake (as % of total uptake), Upper Big Creek, CA")

# quartz(width=7.5, height=6)
Uauto_perc_bigc 

#### Interactive w. ggplotly, but only gives the x,y info... maybe if I added datetime-associated labels?
ggplotly(Uauto_perc_bigc,  tooltip = "text")

```

###### Save updated model output datasets

```{r bigc: save updated datasets}
#| output: false
#| message: false
#| warning: false

path = here("N_uptake_NEON/data/model_output/bigc_output_daily.rds")
saveRDS(daily_output_bigc.df, file=path)
# 
path.h = here("N_uptake_NEON/data/model_output/bigc_output_hourly.rds")
saveRDS(N_output_bigc.df, file=path.h)


# clean environment (retain functions)
# all_objects <- ls(envir = .GlobalEnv)
all_functions <- Filter(function(x) is.function(get(x, envir = .GlobalEnv)), ls(envir = .GlobalEnv))
rm(list = setdiff(ls(envir = .GlobalEnv), all_functions), envir = .GlobalEnv)

```

#### CARI

Outputs and visualizations for Caribou Creek, Fairbanks North Star, AK

###### Load model fits and data

```{r load CARI model fit and data}
#| output: false
#| message: false
#| warning: false


########## Load model fit ______________________________________________________

cari.fit <- readRDS(here("N_uptake_NEON/data/model_fits/pooledK_logK/cari.fit2.rds"))

# if working from model run
# cari.fit <- fit.cari


########## Load model data _____________________________________________________

cari.modeldata <- readRDS(here("N_uptake_NEON/data/model_datalist/cari.data.rds"))


########## Load full hourly datasets __________________________________
# these include datatime info, and the model data does not)

# hourly dataset
path_h <- here("N_uptake_NEON/data/neon_data_clean/cari_hourly_clean.csv")
cari.df.h <- read_csv(path_h) %>%
  mutate(local_datetime = with_tz(local_datetime, tzone="US/Alaska"), 
         model_datetime = local_datetime - hours(2))# 2-hr offset because it starts getting light at 02:45 in midsummer...

# daily dataseset for Q, v, z
cari.qvz <- readRDS(file=here("N_uptake_NEON/data/neon_data_clean/cari.q.daily.rds"))

```

###### Extract model parameters and real-data values

First, check the fit for sigmas and betas ("sigma", "sigma_U", "b0", "b1"), since these are notoriously tough to fit. Then put the spread and mean/ median/ quartile info for K, U, and N_e into a CARI tibble (to be saved in one large csv at the end of the document). Finally, assemble tibbles for the visualizations, which include both modeled parameters and real data.

```{r - cari: extract model params and real-data values}
#| echo: false
#| message: false
#| warning: false

# get list of variables for the model fit
# get_variables(cari.fit) - too many to see all, but informs re: structure [hour,day] for hourly variables

########## Check the fit for sigmas and betas (tough to fit): ___________________
print(cari.fit, pars=c("sigma", "sigma_U", "sigma_K", "K_mean", "b0", "b1"))

########## Summarize daily variables (K, U, N_e) ________________________________

# First, use spread_draws for the daily parameters - this gives each parameter its own column
# d = days (198), chain = 4, iteration = 1K, draw = 4K (1 per iteration per chain)
cari.spread <- spread_draws(cari.fit, K[d], U[d], U_tot[d], Vf_tot[d], N_e[d]) 


# Then get the mean/median + 95% QI (CI) for each parameter/day

### median + CI
cari.medqi <- cari.spread %>%
  # dplyr::group_by(d) %>%  # Unnecessary: tidybayes automatically groups by the selected spread_draws group [in this case, 'd']
  median_qi() %>%
  # summarise_draws() %>%
  # pivot_wider(names_from = variable, values_from = mean, median, sd, mad, q5, q95, rhat, ess_bulk, ess_tail) %>%
  add_column(site = "CARI")

### mean + QI
cari.meanqi <- cari.spread %>%
  mean_qi() %>%
  add_column(site = "CARI")

### Add median param info to the meanqi df
cari.meanqi$K.median <- cari.medqi$K
cari.meanqi$U.median <- cari.medqi$U
cari.meanqi$U_tot.median <- cari.medqi$U_tot
cari.meanqi$Vf_tot.median <- cari.medqi$Vf_tot
cari.meanqi$N_e.median <- cari.medqi$N_e

##### Get draws for b1 and b0 (for plotting later)
cari.betas <- spread_draws(cari.fit, b0, b1) %>%
  sample_n(max(cari.medqi$d))



########## Summarize the hourly variable (conc_pred) ____________________________

# Again, using spread_draws for the hourly parameter, conc_pred (predicted N concentration)
cari.spread.h <- spread_draws(cari.fit, conc_pred[h,d])
# View(cari.spread.h)

# Getting the mean and 95% credible interval values ( = 0.95 QI)
cari.meanqi.h <- cari.spread.h %>%
  dplyr::group_by(d, h) %>%  # This command IS needed here: tidybayes *did* have h + d column outputs, but the values were both different and less precise than when grouping by (d, h). 
  mean_qi() %>%
  add_column(site = "CARI")

# Median + 0.95 QI
cari.medqi.h <- cari.spread.h %>%
  dplyr::group_by(d, h) %>% #again, explicit grouping is needed here
  median_qi() %>%
  add_column(site = "CARI")

### Add median param info to the hourly meanqi df
cari.meanqi.h$conc_pred.median <- cari.medqi.h$conc_pred


########## Compile dataframes __________________________________________________

####### Hourly dataframe: N and N_pred #############

# start w. the hourly dataframe and add mean, QI, modeldata info
N_output_cari.df <- cari.df.h %>%  
  add_column(N_pred = cari.meanqi.h$conc_pred, 
         N_pred.lower = cari.meanqi.h$.lower, 
         N_pred.upper = cari.meanqi.h$.upper, 
         z=as.vector(cari.modeldata$zMA), 
         site = "CARI") %>%
  rename(N_conc = surfWaterNitrateMean, 
         lightReal = PAR) %>%         # CHANGE this line for CARI and OKSR
   mutate(hours = hour(local_datetime), 
         mod_hours = hour(model_datetime), 
         lightReal = lightReal/4.6) %>%     # CONVERT (for now) to W m-2 to match others (evt will change all to umol m-2 s-1)
  dplyr::select(site, local_datetime, model_datetime, N_conc, N_pred, N_pred.lower, N_pred.upper, lightReal, z, Year, yr_jday, Jday, model_jday, hours, mod_hours) 
  

####### Daily dataframe: U, K, N_e ##################

# Create a domainID vector length(nday)
nday <- cari.modeldata$D
domainID <- cari.df.h$domainID[1:nday]

# start w. the meanqi dataframe and add modeldata and datetime info as needed
daily_output_cari.df <- cari.meanqi %>%
  add_column(yr_jday = unique(cari.df.h$yr_jday), 
             sumlightReal = cari.modeldata$sumlightReal, 
             sumlightIdeal = cari.modeldata$sumlightIdeal, 
             Q = cari.qvz$Q_cms,
             v = cari.qvz$v,
             z = colMeans(cari.modeldata$zMA), # daily depth = average of hourly depth
             b0 = cari.betas$b0, 
             b1 = cari.betas$b1,
             domainID = domainID) %>%
  separate(yr_jday, into = c("Year", "model_jday"), sep = "_", convert = TRUE, remove = FALSE) %>%   # makes the new columns numeric if possible, and retains the old column
  mutate(model_date = ymd(paste0(Year, "-01-01")) + days(model_jday - 1), 
         sumlightReal = sumlightReal/4.6) %>%     # CONVERT (for now) to W m-2 to match others (evt will change all to umol m-2 s-1)
  dplyr::select(site, yr_jday, K, K.lower, K.upper, K.median, U, U.lower, U.upper, U.median, U_tot, U_tot.lower, U_tot.upper, U_tot.median, Vf_tot, Vf_tot.lower, Vf_tot.upper, Vf_tot.median, N_e, N_e.lower, N_e.upper, N_e.median, sumlightReal, sumlightIdeal, Q, v, z, b0,b1, model_date, model_jday, Year, domainID, d) #d is the index from the meanqi object - used for plotting 'ts' by index
         
# Check means for all 3 params
# mean(daily_output_cari.df$K) # 3.225407
# mean(daily_output_cari.df$U) # 0.1986689
# mean(daily_output_cari.df$N_e) # 3.395281


```

###### Create visualizations - Caribou Creek, AK

```{r - cari: create visualizations}
#| echo: true

########## N Model fit to data _________________________________________________

######  N model fit over time - predicted N vs measured N  #######

N_Npred_ts_cari <- plot_Nmodelfit_ts(data=N_output_cari.df, start_jday = 301, end_jday = 365, plot_title = "NEON N data + model predictions: Caribou Creek, AK")

quartz(width=6,height=6)
N_Npred_ts_cari



### w. credible interval ribbon
N_Npred_tsCI_cari <- plot_Nmodelfit_tsCI(data=N_output_cari.df, start_jday = 170, end_jday = 185, plot_title = "NEON N data + model predictions: Caribou Creek, AK")


quartz(width=6,height=6)
N_Npred_tsCI_cari


######  predicted N vs measured N + 1:1 line  #######

Npred_v_N_cari <- plot_Npred_v_Nobs(data=N_output_cari.df, plot_title="Predicted N vs. measured N, Caribou Creek, AK")

quartz(width=6, height=6)
Npred_v_N_cari


########## Equifinality checks ________________________________________________

####### Check equifinality!! K vs U  #######  

KvU_cari <- plot_KvU(data=daily_output_cari.df, plot_title = "Equifinality check, Caribou Creek, AK: modeled K vs modeled U")
  
quartz(width=7.5, height=6)
KvU_cari

ggplotly(KvU_cari, tooltip = "text")

####### Check equifinality!! K vs N_e  #######  

KvNe_cari <- plot_KvNe(data=daily_output_cari.df, plot_title = "Equifinality check, Caribou Creek, AK: modeled K vs modeled equilibrium N")

quartz(width=7.5, height=6)
KvNe_cari

ggplotly(KvNe_cari, tooltip = "text")

####### Check equifinality!! U vs N_e  #######  

UvNe_cari <- plot_UvNe(data=daily_output_cari.df, plot_title = "Equifinality check, Caribou Creek, AK: modeled U vs modeled equilibrium N")
 
quartz(width=7.5, height=6)
UvNe_cari

ggplotly(UvNe_cari, tooltip = "text")

########## Other parameter visualizations ______________________________________

#######  K over time  #######  

K_v_time_cari <- plot_K_ts(data=daily_output_cari.df, plot_title = "modeled K over time, Caribou Creek, AK")

quartz(width=6.5, height=6)
K_v_time_cari

ggplotly(K_v_time_cari, tooltip = "text")

##### U over time  #######  

U_time_cari <- plot_U_ts(data=daily_output_cari.df, plot_title="Diel nitrate uptake (modeled), Caribou Creek, AK")

quartz(width=6, height=6)
U_time_cari

ggplotly(U_time_cari, tooltip = "text")

###### U vs sumlight  #######  

U_vs_light_cari <- plot_UvLight(data=daily_output_cari.df, plot_title = "Scatterplot of NO3 uptake and daily light: Caribou Creek, AK")

quartz(width=6.5, height=6)
U_vs_light_cari

ggplotly(U_vs_light_cari, tooltip = "text")

```

###### Compare U_total and U_auto

What percentage of total uptake is the autotrophic uptake?

```{r - cari: percent autotrophic NO3 uptake}

# NON-AUTOTROPHIC UPTAKE = Equilibrium nitrate * K
# TOTAL UPTAKE = U + (Equilibrium nitrate * K)  
# Since U_auto is very small, there won't be much difference between 'non-autotrophic uptake' and 'total uptake'

daily_output_cari.df <- daily_output_cari.df %>%
  mutate(U_other = N_e*K*z,      
         U_tot1 = U_tot + U,
         U_tot2 = U_other + U,
         U_auto_perc = 100*U/U_tot1) 

plot(daily_output_cari.df$U_tot, daily_output_cari.df$U_other) # 1:1

U_auto_avg <- mean(daily_output_cari.df$U_auto_perc) #4.9% on average - now 13.9

Uauto_perc_cari <- plot_autoUperc(data=daily_output_cari.df, plot_title = "Daily  autotrophic uptake (as % of total uptake), Caribou Creek, AK")

quartz(width=7.5, height=6)
Uauto_perc_cari 

#### Interactive w. ggplotly, but only gives the x,y info... maybe if I added datetime-associated labels?
ggplotly(Uauto_perc_cari, tooltip = "text")

```

###### Save updated model output datasets

```{r cari:save updated datasets}
#| output: false
#| message: false
#| warning: false

path = here("N_uptake_NEON/data/model_output/cari_output_daily.rds")
saveRDS(daily_output_cari.df, file=path)
# 
path.h = here("N_uptake_NEON/data/model_output/cari_output_hourly.rds")
saveRDS(N_output_cari.df, file=path.h)

# clean environment (retain functions)
# all_objects <- ls(envir = .GlobalEnv)
all_functions <- Filter(function(x) is.function(get(x, envir = .GlobalEnv)), ls(envir = .GlobalEnv))
rm(list = setdiff(ls(envir = .GlobalEnv), all_functions), envir = .GlobalEnv)


```

#### CUPE

Outputs and visualizations for Rio Cupeyes, San German Municipio, PR

###### Load model fits and data

```{r load CUPE model fit and data}
#| output: false
#| message: false
#| warning: false

########## Load model fit ______________________________________________________

cupe.fit <- readRDS(here("N_uptake_NEON/data/model_fits/pooledK_logK/cupe.fit2.rds"))

# if working from model run
# cupe.fit <- fit.cupe


########## Load model data _____________________________________________________

cupe.modeldata <- readRDS(here("N_uptake_NEON/data/model_datalist/cupe.data.rds"))

########## Load full daily and hourly datasets __________________________________
# these include datatime info, and the model data does not)

# # 15-min dataset
# path <- here("N_uptake_NEON/data/neon_data_clean/cupe_clean.csv")
# cupe.df <- read_csv(path) %>%
#   mutate(local_datetime = with_tz(local_datetime, tzone="America/Puerto_Rico"), 
#          model_datetime = local_datetime - hours(4))  


# hourly dataset
path_h <- here("N_uptake_NEON/data/neon_data_clean/cupe_hourly_clean.csv")
cupe.df.h <- read_csv(path_h) %>%
  mutate(local_datetime = with_tz(local_datetime, tzone="America/Puerto_Rico"), 
         model_datetime = local_datetime - hours(4))  
  

# daily dataseset for Q, v, z
cupe.qvz <- readRDS(file=here("N_uptake_NEON/data/neon_data_clean/cupe.q.daily.rds"))
  
  
```

###### Extract model parameters and real-data values

First, check the fit for sigmas and betas ("sigma", "sigma_U", "b0", "b1"), since these are notoriously tough to fit. Then put the spread and mean/ median/ quartile info for K, U, and N_e into a CUPE tibble (to be saved in one large csv at the end of the document). Finally, assemble tibbles for the visualizations, which include both modeled parameters and real data.

```{r - cupe: extract model params and real-data values}
#| echo: false
#| message: false
#| warning: false

# get list of variables for the model fit
# get_variables(cupe.fit) - too many to see all, but informs re: structure [hour,day] for hourly variables

########## Check the fit for sigmas and betas (tough to fit): ___________________
print(cupe.fit, pars=c("sigma", "sigma_U", "sigma_K", "K_mean", "b0", "b1"))

########## Summarize daily variables (K, U, N_e) ________________________________

#### First, use spread_draws for the daily parameters - this gives each parameter its own column
# d = days (198), chain = 4, iteration = 1K, draw = 4K (1 per iteration per chain)
cupe.spread <- spread_draws(cupe.fit, K[d], U[d], U_tot[d], Vf_tot[d], N_e[d]) 


##### Then get the mean/median + 95% QI (CI) for each parameter/day

# NB: for the daily data, it is unnecessary to group by day using  dplyr::group_by(d): tidybayes automatically groups by the selected spread_draws group [in this case, 'd']

### median + CI
cupe.medqi <- cupe.spread %>%
  median_qi() %>%
  add_column(site = "CUPE")

### mean + QI
cupe.meanqi <- cupe.spread %>%
  mean_qi() %>%
  add_column(site = "CUPE")

### Add median param info to the meanqi df
cupe.meanqi$K.median <- cupe.medqi$K
cupe.meanqi$U.median <- cupe.medqi$U
cupe.meanqi$U_tot.median <- cupe.medqi$U_tot
cupe.meanqi$Vf_tot.median <- cupe.medqi$Vf_tot
cupe.meanqi$N_e.median <- cupe.medqi$N_e

##### Get draws for b1 and b0 (for plotting later)
cupe.betas <- spread_draws(cupe.fit, b0, b1) %>%
  sample_n(max(cupe.medqi$d))



########## Summarize hourly variable (conc_pred) _______________________________

# Again, using spread_draws for the hourly parameter, conc_pred (predicted N concentration)
cupe.spread.h <- spread_draws(cupe.fit, conc_pred[h,d])
# View(cupe.spread.h)

# Get the mean and 95% credible interval values (= 0.95 QI)
cupe.meanqi.h <- cupe.spread.h %>%
  dplyr::group_by(d, h) %>%  # This command IS needed here: tidybayes *did* have h + d column outputs, but the values were both different and less precise than when grouping by (d, h). 
  mean_qi() %>%
  add_column(site = "CUPE")

# Median + 0.95 QI
cupe.medqi.h <- cupe.spread.h %>%
  dplyr::group_by(d, h) %>% #again, explicit grouping is needed here
  median_qi() %>%
  add_column(site = "CUPE")

### Add median param info to the hourly meanqi df
cupe.meanqi.h$conc_pred.median <- cupe.medqi.h$conc_pred


########## Compile dataframes __________________________________________________

####### Hourly dataframe: N and N_pred #############

# start w. the hourly dataframe and add mean, QI, modeldata info
N_output_cupe.df <- cupe.df.h %>%  
  add_column(N_pred = cupe.meanqi.h$conc_pred, 
         N_pred.lower = cupe.meanqi.h$.lower, 
         N_pred.upper = cupe.meanqi.h$.upper, 
         z=as.vector(cupe.modeldata$zMA), 
         site = "CUPE") %>%
  rename(N_conc = surfWaterNitrateMean, 
         lightReal = GHI_wm2) %>%
   mutate(hours = hour(local_datetime), 
         mod_hours = hour(model_datetime)) %>%
  select(site, local_datetime, model_datetime, N_conc, N_pred, N_pred.lower, N_pred.upper, lightReal, z, Year, yr_jday, Jday, model_jday, hours, mod_hours) 
  

####### Daily dataframe: U, K, N_e ##################

# Create a domainID vector length(nday)
nday <- cupe.modeldata$D
domainID <- cupe.df.h$domainID[1:nday]

# start w. the meanqi dataframe and add modeldata and datetime info as needed
daily_output_cupe.df <- cupe.meanqi %>%
  add_column(yr_jday = unique(cupe.df.h$yr_jday), 
             sumlightReal = cupe.modeldata$sumlightReal, 
             sumlightIdeal = cupe.modeldata$sumlightIdeal, 
             Q = cupe.qvz$Q_cms,
             v = cupe.qvz$v,
             z = cupe.qvz$z,
             # z = colMeans(cupe.modeldata$zMA), # daily depth = average of hourly depth
             b0 = cupe.betas$b0,
             b1 = cupe.betas$b1,
             domainID = domainID) %>%
  separate(yr_jday, into = c("Year", "model_jday"), sep = "_", convert = TRUE, remove = FALSE) %>%  # makes the new columns numeric if possible, and retains the old column
  mutate(model_date = ymd(paste0(Year, "-01-01")) + days(model_jday - 1)) %>%
  dplyr::select(site, yr_jday, K, K.lower, K.upper, K.median, U, U.lower, U.upper, U.median, U_tot, U_tot.lower, U_tot.upper, U_tot.median, Vf_tot, Vf_tot.lower, Vf_tot.upper, Vf_tot.median, N_e, N_e.lower, N_e.upper, N_e.median, sumlightReal, sumlightIdeal, Q, v, z, b0,b1, model_date, model_jday, Year, domainID, d) #d is the index from the meanqi object - used for plotting 'ts' by index
         
# Check means for all 3 params
# mean(daily_output_cupe.df$K) # 3.225407
# mean(daily_output_cupe.df$U) # 0.1986689
# mean(daily_output_cupe.df$N_e) # 3.395281


```

###### Create visualizations - Rio Cupeyes, San German Municipio, PR

```{r - CUPE: create visualizations}
#| echo: true

########## N Model fit to data __________________________________________________

######  N model fit over time - predicted N vs measured N  #######

N_Npred_ts_cupe <- plot_Nmodelfit_ts(data=N_output_cupe.df, start_jday = 331, end_jday = 365, plot_title = "NEON N data + model predictions: Rio Cupeyes, PR")

quartz(width=6,height=6)
N_Npred_ts_cupe

### w. credible interval ribbon
N_Npred_tsCI_cupe <- plot_Nmodelfit_tsCI(data=N_output_cupe.df, start_jday = 35, end_jday = 60, plot_title = "NEON N data + model predictions: Rio Cupeyes, PR")

quartz(width=6,height=6)
N_Npred_tsCI_cupe

######  predicted N vs measured N + 1:1 line  #######

Npred_v_N_cupe <- plot_Npred_v_Nobs(data=N_output_cupe.df, plot_title="Predicted N vs. measured N, Rio Cupeyes, PR")

quartz(width=6, height=6)
Npred_v_N_cupe


########## Equifinality checks _________________________________________________

####### Check equifinality!! K vs U  #######  

KvU_cupe <- plot_KvU(data=daily_output_cupe.df, plot_title = "Equifinality check, Rio Cupeyes, PR: K_nit vs U_auto")
  
# quartz(width=7.5, height=6)
KvU_cupe

ggplotly(KvU_cupe, tooltip = "text")

####### Check equifinality!! K vs N_e  #######  

KvNe_cupe <- plot_KvNe(data=daily_output_cupe.df, plot_title = "Equifinality check, Rio Cupeyes, PR: K_nit vs equilibrium N")

# quartz(width=7.5, height=6)
KvNe_cupe

ggplotly(KvNe_cupe, tooltip = "text")


####### Check equifinality!! U vs N_e  #######  

UvNe_cupe <- plot_UvNe(data=daily_output_cupe.df, plot_title = "Equifinality check, Rio Cupeyes, PR: U_auto vs equilibrium N")
 
# quartz(width=7.5, height=6)
UvNe_cupe

ggplotly(UvNe_cupe, tooltip = "text")


########## Other parameter visualizations ______________________________________

#######  K over time  #######  

K_v_time_cupe <- plot_K_ts(data=daily_output_cupe.df, plot_title = "K_nit over time, Rio Cupeyes, PR")

# quartz(width=6.5, height=6)
K_v_time_cupe

ggplotly(K_v_time_cupe, tooltip = "text")

##### U over time  #######  

U_time_cupe <- plot_U_ts(data=daily_output_cupe.df, plot_title="Diel nitrate uptake (modeled), Rio Cupeyes, PR")

# quartz(width=6, height=6)
U_time_cupe

ggplotly(U_time_cupe, tooltip = "text")

###### U vs sumlight  #######  

U_vs_light_cupe <- plot_UvLight(data=daily_output_cupe.df, plot_title = "Scatterplot of NO3 uptake and daily light: Rio Cupeyes, PR")

# quartz(width=6.5, height=6)
U_vs_light_cupe

ggplotly(U_vs_light_cupe, tooltip = "text")

```

###### Compare U_total and U_auto

What percentage of total uptake is the autotrophic uptake?

```{r - cupe: percent autotrophic NO3 uptake}

# NON-AUTOTROPHIC UPTAKE = Equilibrium nitrate * K * z
# TOTAL UPTAKE = U + (Equilibrium nitrate * K * z)  
# Since U_auto is very small, there won't be much difference between 'non-autotrophic uptake' and 'total uptake'

daily_output_cupe.df <- daily_output_cupe.df %>%
  mutate(U_other = N_e*K*z,      
         U_tot1 = U_tot + U,
         U_tot2 = U_other + U,
         U_auto_perc = 100*U/U_tot1) 

plot(daily_output_cupe.df$U_tot, daily_output_cupe.df$U_other) # 1:1


U_auto_avg <- mean(daily_output_cupe.df$U_auto_perc) #4.05% on average

Uauto_perc_cupe <- plot_autoUperc(data=daily_output_cupe.df, plot_title = "Daily autotrophic uptake (as % of total uptake), Rio Cupeyes, PR")

# quartz(width=7.5, height=6)
Uauto_perc_cupe 

#### Interactive w. ggplotly, but only gives the x,y info... maybe if I added datetime-associated labels?
ggplotly(Uauto_perc_cupe, tooltip = "text")

```

###### Save updated model output datasets

```{r cupe: save updated datasets}
#| output: false
#| message: false
#| warning: false

path = here("N_uptake_NEON/data/model_output/cupe_output_daily.rds")
saveRDS(daily_output_cupe.df, file=path)
# 
path.h = here("N_uptake_NEON/data/model_output/cupe_output_hourly.rds")
saveRDS(N_output_cupe.df, file=path.h)


# clean environment (retain functions)
# all_objects <- ls(envir = .GlobalEnv)
all_functions <- Filter(function(x) is.function(get(x, envir = .GlobalEnv)), ls(envir = .GlobalEnv))
rm(list = setdiff(ls(envir = .GlobalEnv), all_functions), envir = .GlobalEnv)

```

#### PRIN

Outputs and visualizations for Pringle Creek, Wise County, TX

###### Load model fits and data

```{r load PRIN model fit and data}
#| output: false
#| message: false
#| warning: false

########## Load model fit ______________________________________________________

prin.fit <- readRDS(here("N_uptake_NEON/data/model_fits/pooledK_logK/prin.fit2.rds"))

# if working from model run
# prin.fit <- fit.prin


########## Load model data _____________________________________________________

prin.modeldata <- readRDS(here("N_uptake_NEON/data/model_datalist/prin.data.rds"))

########## Load full daily and hourly datasets __________________________________
# these include datatime info, and the model data does not)

# # 15-min dataset
# path <- here("N_uptake_NEON/data/neon_data_clean/prin_clean.csv")
# prin.df <- read_csv(path) %>%
#   mutate(local_datetime = with_tz(local_datetime, tzone="US/Central"), 
#          model_datetime = local_datetime - hours(4))  


# hourly dataset
path_h <- here("N_uptake_NEON/data/neon_data_clean/prin_hourly_clean.csv")
prin.df.h <- read_csv(path_h) %>%
  mutate(local_datetime = with_tz(local_datetime, tzone="US/Central"), 
         model_datetime = local_datetime - hours(4))  

# daily dataseset for Q, v, z
prin.qvz <- readRDS(file=here("N_uptake_NEON/data/neon_data_clean/prin.q.daily.rds"))
  
```

###### Extract model parameters and real-data values

First, check the fit for sigmas and betas ("sigma", "sigma_U", "b0", "b1"), since these are notoriously tough to fit. Then put the spread and mean/ median/ quartile info for K, U, and N_e into a PRIN tibble (to be saved in one large csv at the end of the document). Finally, assemble tibbles for the visualizations, which include both modeled parameters and real data.

```{r - prin: extract model params and real-data values}
#| echo: false
#| message: false
#| warning: false

# get list of variables for the model fit
# get_variables(prin.fit) - too many to see all, but informs re: structure [hour,day] for hourly variables

########## Check the fit for sigmas and betas (tough to fit): ___________________
print(prin.fit, pars=c("sigma", "sigma_U", "sigma_K", "K_mean", "b0", "b1"))

########## Summarize daily variables (K, U, N_e) ________________________________

#### First, use spread_draws for the daily parameters - this gives each parameter its own column
# d = days (198), chain = 4, iteration = 1K, draw = 4K (1 per iteration per chain)
prin.spread <- spread_draws(prin.fit, K[d], U[d], U_tot[d], Vf_tot[d], N_e[d]) 


##### Then get the mean/median + 95% QI (CI) for each parameter/day

# NB: for the daily data, it is unnecessary to group by day using  dplyr::group_by(d): tidybayes automatically groups by the selected spread_draws group [in this case, 'd']

### median + CI
prin.medqi <- prin.spread %>%
  median_qi() %>%
  add_column(site = "PRIN")

### mean + QI
prin.meanqi <- prin.spread %>%
  mean_qi() %>%
  add_column(site = "PRIN")

### Add median param info to the meanqi df
prin.meanqi$K.median <- prin.medqi$K
prin.meanqi$U.median <- prin.medqi$U
prin.meanqi$U_tot.median <- prin.medqi$U_tot
prin.meanqi$Vf_tot.median <- prin.medqi$Vf_tot
prin.meanqi$N_e.median <- prin.medqi$N_e

##### Get draws for b1 and b0 (for plotting later)
prin.betas <- spread_draws(prin.fit, b0, b1) %>%
  sample_n(max(prin.medqi$d))




########## Summarize hourly variable (conc_pred) _______________________________

# Again, using spread_draws for the hourly parameter, conc_pred (predicted N concentration)
prin.spread.h <- spread_draws(prin.fit, conc_pred[h,d])
# View(prin.spread.h)

# Get the mean and 95% credible interval values (= 0.95 QI)
prin.meanqi.h <- prin.spread.h %>%
  dplyr::group_by(d, h) %>%  # This command IS needed here: tidybayes *did* have h + d column outputs, but the values were both different and less precise than when grouping by (d, h). 
  mean_qi() %>%
  add_column(site = "PRIN")

# Median + 0.95 QI
prin.medqi.h <- prin.spread.h %>%
  dplyr::group_by(d, h) %>% #again, explicit grouping is needed here
  median_qi() %>%
  add_column(site = "PRIN")

### Add median param info to the hourly meanqi df
prin.meanqi.h$conc_pred.median <- prin.medqi.h$conc_pred


########## Compile dataframes __________________________________________________

####### Hourly dataframe: N and N_pred #############

# start w. the hourly dataframe and add mean, QI, modeldata info
N_output_prin.df <- prin.df.h %>%  
  add_column(N_pred = prin.meanqi.h$conc_pred, 
         N_pred.lower = prin.meanqi.h$.lower, 
         N_pred.upper = prin.meanqi.h$.upper, 
         z=as.vector(prin.modeldata$zMA), 
         site = "PRIN") %>%
  rename(N_conc = surfWaterNitrateMean, 
         lightReal = GHI_wm2) %>%
   mutate(hours = hour(local_datetime), 
         mod_hours = hour(model_datetime)) %>%
  select(site, local_datetime, model_datetime, N_conc, N_pred, N_pred.lower, N_pred.upper, lightReal, z, Year, yr_jday, Jday, model_jday, hours, mod_hours) 
  

####### Daily dataframe: U, K, N_e ##################

# Create a domainID vector length(nday)
nday <- prin.modeldata$D
domainID <- prin.df.h$domainID[1:nday]

# start w. the meanqi dataframe and add modeldata and datetime info as needed
daily_output_prin.df <- prin.meanqi %>%
  add_column(yr_jday = unique(prin.df.h$yr_jday), 
             sumlightReal = prin.modeldata$sumlightReal, 
             sumlightIdeal = prin.modeldata$sumlightIdeal, 
             Q = prin.qvz$Q_cms,
             v = prin.qvz$v,
             z = prin.qvz$z,
             #z = colMeans(prin.modeldata$zMA), # daily depth = avg of hourly depth
             b0 = prin.betas$b0,
             b1 = prin.betas$b1,
             domainID = domainID) %>%
  separate(yr_jday, into = c("Year", "model_jday"), sep = "_", convert = TRUE, remove = FALSE) %>%  # makes the new columns numeric if possible, and retains the old column
  mutate(model_date = ymd(paste0(Year, "-01-01")) + days(model_jday - 1)) %>%
  dplyr::select(site, yr_jday, K, K.lower, K.upper, K.median, U, U.lower, U.upper, U.median, U_tot, U_tot.lower, U_tot.upper, U_tot.median, Vf_tot, Vf_tot.lower, Vf_tot.upper, Vf_tot.median, N_e, N_e.lower, N_e.upper, N_e.median, sumlightReal, sumlightIdeal, Q, v, z, b0, b1, model_date, model_jday, Year, domainID, d) #d is the index from the meanqi object - used for plotting 'ts' by index
         
# Check means for all 3 params
# mean(daily_output_prin.df$K) #
# mean(daily_output_prin.df$U) # 
# mean(daily_output_prin.df$N_e) # 

# print("Pringle Creek mean K is" mean(daily_output_prin.df$K))", mean U is" mean(daily_output_prin.df$U) ", and mean equilibrium N is" mean(daily_output_prin.df$N_e)



```

###### Create visualizations - Pringle Creek, Wise County, TX

```{r - PRIN: create visualizations}
#| echo: true

########## N Model fit to data __________________________________________________

######  N model fit over time - predicted N vs measured N  #######

N_Npred_ts_prin <- plot_Nmodelfit_ts(data=N_output_prin.df, start_jday = 331, end_jday = 365, plot_title = "NEON N data + model predictions: Pringle Creek, TX")

quartz(width=6,height=6)
N_Npred_ts_prin



### w. credible interval ribbon
N_Npred_tsCI_prin <- plot_Nmodelfit_tsCI(data=N_output_prin.df, start_jday = 90, end_jday = 120, plot_title = "NEON N data + model predictions: Pringle Creek, TX")

quartz(width=6,height=6)
N_Npred_tsCI_prin

######  predicted N vs measured N + 1:1 line  #######

Npred_v_N_prin <- plot_Npred_v_Nobs(data=N_output_prin.df, plot_title="Predicted N vs. measured N, Pringle Creek, TX")

# quartz(width=6, height=6)
Npred_v_N_prin

# ggplotly(Npred_v_N_prin, tooltip = "text") - need to change for this to work: just wanted to check out the two curly "antennae" at the upper RH corner


########## Equifinality checks _________________________________________________

####### Check equifinality!! K vs U  #######  

KvU_prin <- plot_KvU(data=daily_output_prin.df, plot_title = "Equifinality check, Pringle Creek, TX: modeled K vs modeled U")
  
# quartz(width=7.5, height=6)
KvU_prin

ggplotly(KvU_prin, tooltip = "text") 
  

####### Check equifinality!! K vs N_e  #######  

KvNe_prin <- plot_KvNe(data=daily_output_prin.df, plot_title = "Equifinality check, Pringle Creek, TX: modeled K vs modeled equilibrium N")

# quartz(width=7.5, height=6)
KvNe_prin

ggplotly(KvNe_prin, tooltip = "text")

####### Check equifinality!! U vs N_e  #######  

UvNe_prin <- plot_UvNe(data=daily_output_prin.df, plot_title = "Equifinality check, Pringle Creek, TX: modeled U vs modeled equilibrium N")
 
# quartz(width=7.5, height=6)
UvNe_prin

ggplotly(UvNe_prin, tooltip = "text")

########## Other parameter visualizations ______________________________________

#######  K over time  #######  

K_v_time_prin <- plot_K_ts(data=daily_output_prin.df, plot_title = "modeled K over time, Pringle Creek, TX")

# quartz(width=6.5, height=6)
K_v_time_prin

ggplotly(K_v_time_prin, tooltip = "text")

##### U over time  #######  

U_time_prin <- plot_U_ts(data=daily_output_prin.df, plot_title="Diel nitrate uptake (modeled), Pringle Creek, TX")

# quartz(width=6, height=6)
U_time_prin

ggplotly(U_time_prin, tooltip = "text")

###### U vs sumlight  #######  

U_vs_light_prin <- plot_UvLight(data=daily_output_prin.df, plot_title = "Scatterplot of NO3 uptake and daily light: Pringle Creek, TX")

# quartz(width=6.5, height=6)
U_vs_light_prin

ggplotly(U_vs_light_prin, tooltip = "text")


```

###### Compare U_total and U_auto

What percentage of total uptake is the autotrophic uptake?

```{r - prin: percent autotrophic NO3 uptake}

# NON-AUTOTROPHIC UPTAKE = Equilibrium nitrate * K * z
# TOTAL UPTAKE = U + (Equilibrium nitrate * K * z)  
# Since U_auto is very small, there won't be much difference between 'non-autotrophic uptake' and 'total uptake'

daily_output_prin.df <- daily_output_prin.df %>%
  mutate(U_other = N_e*K*z,      
         U_tot1 = U_tot + U,
         U_tot2 = U_other + U,
         U_auto_perc = 100*U/U_tot1) 

plot(daily_output_prin.df$U_tot, daily_output_prin.df$U_other) # 1:1


U_auto_avg <- mean(daily_output_prin.df$U_auto_perc) #27.83% on average

Uauto_perc_prin <- plot_autoUperc(data=daily_output_prin.df, plot_title = "Daily autotrophic uptake (as % of total uptake), Pringle Creek, TX")

# quartz(width=7.5, height=6)
Uauto_perc_prin 

#### Interactive w. ggplotly, but only gives the x,y info... maybe if I added datetime-associated labels?
ggplotly(Uauto_perc_prin, tooltip = "text")

```

###### Save updated model output datasets

```{r prin: save updated datasets}
#| output: false
#| message: false
#| warning: false

path = here("N_uptake_NEON/data/model_output/prin_output_daily.rds")
saveRDS(daily_output_prin.df, file=path)

path.h = here("N_uptake_NEON/data/model_output/prin_output_hourly.rds")
saveRDS(N_output_prin.df, file=path.h)

# clean environment (retain functions)
# all_objects <- ls(envir = .GlobalEnv)
all_functions <- Filter(function(x) is.function(get(x, envir = .GlobalEnv)), ls(envir = .GlobalEnv))
rm(list = setdiff(ls(envir = .GlobalEnv), all_functions), envir = .GlobalEnv)

```

#### SYCA

Outputs and visualizations for Sycamore Creek, Maricopa County, AZ

###### Load model fits and data

```{r load SYCA model fit and data}
#| output: false
#| message: false
#| warning: false

########## Load model fit ______________________________________________________

syca.fit <- readRDS(here("N_uptake_NEON/data/model_fits/pooledK_logK/syca.fit2.rds"))

# if working from model run
# syca.fit <- fit.syca


########## Load model data _____________________________________________________
datapath <- here("N_uptake_NEON/data/model_datalist/syca.data.rds")
syca.modeldata <- readRDS(datapath)


########## Load full daily and hourly datasets __________________________________
# these include datatime info, and the model data does not)

# # 15-min dataset
# path <- here("N_uptake_NEON/data/neon_data_clean/syca_clean.csv")
# syca.df <- read_csv(path) %>%
#   mutate(local_datetime = with_tz(local_datetime, tzone="US/Central"), 
#          model_datetime = local_datetime - hours(4))  


# hourly dataset
path_h <- here("N_uptake_NEON/data/neon_data_clean/syca_hourly_clean.csv")
syca.df.h <- read_csv(path_h) %>%
  mutate(local_datetime = with_tz(local_datetime, tzone="US/Arizona"), 
         model_datetime = local_datetime - hours(4), 
         model_date = as.Date(model_datetime),
         model_jday = yday(model_datetime))  

# daily dataseset for Q, v, z
syca.qvz <- readRDS(file=here("N_uptake_NEON/data/neon_data_clean/syca.q.daily.rds"))
  
  
  
```

###### Extract model parameters and real-data values

First, check the fit for sigmas and betas ("sigma", "sigma_U", "b0", "b1"), since these are notoriously tough to fit. Then put the spread and mean/ median/ quartile info for K, U, and N_e into a SYCA tibble (to be saved in one large csv at the end of the document). Finally, assemble tibbles for the visualizations, which include both modeled parameters and real data.

```{r - syca: extract model params and real-data values}
#| echo: false
#| message: false
#| warning: false

# get list of variables for the model fit
# get_variables(syca.fit) - too many to see all, but informs re: structure [hour,day] for hourly variables

########## Check the fit for sigmas and betas (tough to fit): ___________________
# print(cari.fit, pars=c("sigma", "sigma_U", "sigma_K", "K_mean", "b0", "b1"))
print(syca.fit, pars=c("sigma", "sigma_U", "sigma_logK", "logK_mean", "b0", "b1"))


########## Summarize daily variables (K, U, N_e) ________________________________

#### First, use spread_draws for the daily parameters - this gives each parameter its own column
# d = days (198), chain = 4, iteration = 1K, draw = 4K (1 per iteration per chain)
syca.spread <- spread_draws(syca.fit, K[d], U[d], U_tot[d], Vf_tot[d], N_e[d]) 


##### Then get the mean/median + 95% QI (CI) for each parameter/day

# NB: for the daily data, it is unnecessary to group by day using  dplyr::group_by(d): tidybayes automatically groups by the selected spread_draws group [in this case, 'd']

### median + CI
syca.medqi <- syca.spread %>%
  median_qi() %>%
  add_column(site = "SYCA")

### mean + QI
syca.meanqi <- syca.spread %>%
  mean_qi() %>%
  add_column(site = "SYCA")

### Add median param info to the meanqi df
syca.meanqi$K.median <- syca.medqi$K
syca.meanqi$U.median <- syca.medqi$U
syca.meanqi$U_tot.median <- syca.medqi$U_tot
syca.meanqi$Vf_tot.median <- syca.medqi$Vf_tot
syca.meanqi$N_e.median <- syca.medqi$N_e

##### Get draws for b1 and b0 (for plotting later)
syca.betas <- spread_draws(syca.fit, b0, b1) %>%
  sample_n(max(syca.medqi$d))




########## Summarize hourly variable (conc_pred) _______________________________

# Again, using spread_draws for the hourly parameter, conc_pred (predicted N concentration)
syca.spread.h <- spread_draws(syca.fit, conc_pred[h,d])
# View(syca.spread.h)

# Get the mean and 95% credible interval values (= 0.95 QI)
syca.meanqi.h <- syca.spread.h %>%
  dplyr::group_by(d, h) %>%  # This command IS needed here: tidybayes *did* have h + d column outputs, but the values were both different and less precise than when grouping by (d, h). 
  mean_qi() %>%
  add_column(site = "SYCA")

# Median + 0.95 QI
syca.medqi.h <- syca.spread.h %>%
  dplyr::group_by(d, h) %>% #again, explicit grouping is needed here
  median_qi() %>%
  add_column(site = "SYCA")

### Add median param info to the hourly meanqi df
syca.meanqi.h$conc_pred.median <- syca.medqi.h$conc_pred


########## Compile dataframes __________________________________________________

####### Hourly dataframe: N and N_pred #############
# N_output_syca.df$z <- as.vector(syca.modeldata$zMA)
# start w. the hourly dataframe and add mean, QI, modeldata info
N_output_syca.df <- syca.df.h %>%  
  add_column(N_pred = syca.meanqi.h$conc_pred, 
         N_pred.lower = syca.meanqi.h$.lower, 
         N_pred.upper = syca.meanqi.h$.upper, 
         z=as.vector(syca.modeldata$zMA), 
         site = "SYCA") %>%
  rename(N_conc = surfWaterNitrateMean, 
         lightReal = GHI_wm2) %>%
   mutate(hours = hour(local_datetime), 
         mod_hours = hour(model_datetime), 
         model_jday = yday(model_datetime)) %>%  # added because somehow it was reading as a 3-digit character (artifact from creating the yr_jday col?)
  select(site, local_datetime, model_datetime, N_conc, N_pred, N_pred.lower, N_pred.upper, lightReal, z, Year, yr_jday, Jday, model_jday, hours, mod_hours) 
  

####### Daily dataframe: U, K, N_e ##################

# Create a domainID vector length(nday)
nday <- syca.modeldata$D  # 90
domainID <- syca.df.h$domainID[1:nday]

# start w. the meanqi dataframe and add modeldata and datetime info as needed
daily_output_syca.df <- syca.meanqi %>%
  add_column(yr_jday = unique(syca.df.h$yr_jday), 
             sumlightReal = syca.modeldata$sumlightReal, 
             sumlightIdeal = syca.modeldata$sumlightIdeal, 
             Q = syca.qvz$Q_cms,
             v = syca.qvz$v,
             z = syca.qvz$z,
             #z = colMeans(syca.modeldata$zMA), # daily depth = avg of hourly depth
             b0 = syca.betas$b0,
             b1 = syca.betas$b1,
             domainID = domainID) %>%
  separate(yr_jday, into = c("Year", "model_jday"), sep = "_", convert = TRUE, remove = FALSE) %>%  # makes the new columns numeric if possible, and retains the old column
  mutate(model_date = ymd(paste0(Year, "-01-01")) + days(model_jday - 1)) %>%
  dplyr::select(site, yr_jday, K, K.lower, K.upper, K.median, U, U.lower, U.upper, U.median, U_tot, U_tot.lower, U_tot.upper, U_tot.median, Vf_tot, Vf_tot.lower, Vf_tot.upper, Vf_tot.median, N_e, N_e.lower, N_e.upper, N_e.median, sumlightReal, sumlightIdeal, Q, v, z, b0,b1, model_date, model_jday, Year, domainID, d) #d is the index from the meanqi object - used for plotting 'ts' by index
         
# Check means for all 3 params
# mean(daily_output_syca.df$K) #
# mean(daily_output_syca.df$U) # 
# mean(daily_output_syca.df$N_e) # 

# sycat("Pringle Creek mean K is" mean(daily_output_syca.df$K))", mean U is" mean(daily_output_syca.df$U) ", and mean equilibrium N is" mean(daily_output_syca.df$N_e)



```

###### Create visualizations - Pringle Creek, Wise County, TX

```{r - SYCA: create visualizations}
#| echo: true

########## N Model fit to data __________________________________________________

######  N model fit over time - predicted N vs measured N  #######
#max model_jday is 153
N_Npred_ts_syca <- plot_Nmodelfit_ts(data=N_output_syca.df, start_jday = 300, end_jday = 365, plot_title = "NEON N data + model predictions: Sycamore Creek, AZ")

quartz(width=6,height=6)
N_Npred_ts_syca

## Weird, there are a couple of 1 and 2 observation days for SYCA: either something is wrong w the day selection OR with creating the yr_jday column (made from local_datetime and not model_datetime).

# jdaycount <- N_output_syca.df %>% count(model_jday)
# moddaycount <- N_output_syca.df %>% count(day(model_datetime))
datecount <-  N_output_syca.df %>% count(model_datetime) # uh-oh: 103 obs. - should be 82... 
yrjdaycount <- N_output_syca.df %>% count(yr_jday) %>% filter(n != 24)

#####
syca.df.h <- syca.df.h %>%
  mutate(model_datetime = as.POSIXct(model_datetime, tz="US/Arizona"))

wonkySYCAdays3 <- syca.df.h %>%
  group_by(model_date) %>%
  summarise(n_hours = n_distinct(model_datetime))
#####

wonkySYCAdays <- N_output_syca.df %>% count(as.Date(model_datetime)) %>% filter(n != 24)

print(wonkySYCAdays, n=42)

wonkySYCAdays2 <- syca.df.h %>% count(as.Date(model_datetime)) %>% filter(n != 24)
print(wonkySYCAdays2, n=42)


syca.df.h %>%
  filter(model_date == as.Date("2022-02-03")) %>%
  mutate(
    local_full  = format(local_datetime,  "%Y-%m-%d %H:%M:%OS6 %Z"),
    model_full  = format(model_datetime,  "%Y-%m-%d %H:%M:%OS6 %Z")
  ) %>%
  select(local_datetime, model_datetime, local_full, model_full) %>%
  print(n = Inf)


# ...BUT even weirder, the dataframe looks fine... 

### w. credible interval ribbon
N_Npred_tsCI_syca <- plot_Nmodelfit_tsCI(data=N_output_syca.df, start_jday = 100, end_jday = 120, plot_title = "NEON N data + model predictions: Sycamore Creek, AZ")

# quartz(width=6,height=6)
N_Npred_tsCI_syca

######  predicted N vs measured N + 1:1 line  #######

Npred_v_N_syca <- plot_Npred_v_Nobs(data=N_output_syca.df, plot_title="Predicted N vs. measured N, Sycamore Creek, AZ")

# quartz(width=6, height=6)
Npred_v_N_syca


########## Equifinality checks _________________________________________________

####### Check equifinality!! K vs U  #######  

KvU_syca <- plot_KvU(data=daily_output_syca.df, plot_title = "Equifinality check, Sycamore Creek, AZ: modeled K vs modeled U")
  
quartz(width=7.5, height=6)
KvU_syca

ggplotly(KvU_syca, tooltip = "text") 

####### Check equifinality!! K vs N_e  #######  

KvNe_syca <- plot_KvNe(data=daily_output_syca.df, plot_title = "Equifinality check, Sycamore Creek, AZ: modeled K vs modeled equilibrium N")

# quartz(width=7.5, height=6)
KvNe_syca


####### Check equifinality!! U vs N_e  #######  

UvNe_syca <- plot_UvNe(data=daily_output_syca.df, plot_title = "Equifinality check, Sycamore Creek, AZ: modeled U vs modeled equilibrium N")
 
# quartz(width=7.5, height=6)
UvNe_syca


########## Other parameter visualizations ______________________________________

#######  K over time  #######  

K_v_time_syca <- plot_K_ts(data=daily_output_syca.df, plot_title = "modeled K over time, Sycamore Creek, AZ")

quartz(width=6.5, height=6)
K_v_time_syca

ggplotly(K_v_time_syca, tooltip = "text")

##### U over time  #######  

U_time_syca <- plot_U_ts(data=daily_output_syca.df, plot_title="Diel nitrate uptake (modeled), Sycamore Creek, AZ")

quartz(width=6, height=6)
U_time_syca

ggplotly(U_time_syca, tooltip = "text")


# there is one isolated VERY high U, "2024-03-24"; what's the Q for that date?
syca.qvz %>%
  filter(model_date == as.Date("2024-03-24")) %>%
  pull(Q_cms)


sycaQ <- syca.qvz %>%
  ggplot(aes(x=model_date, y=Qmean_ls, 
         text = paste("Date:", model_date, "<br>Year-jday", yr_jday, "Q in L/s", Qmean_ls)
         )) +  # text for plotly
  geom_point() + 
     labs( x= "Date",                                     ## for plotly - plotly interprets MathJax
        y="Q ($L~s^{-1}$)" # ~ = a space, * = no space
      ) +
  ggtitle("Sycamore Creek Q in liters per second") +
  theme_bw()

quartz()
sycaQ

ggplotly(sycaQ, tooltip = "text")


###### U vs sumlight  #######  

U_vs_light_syca <- plot_UvLight(data=daily_output_syca.df, plot_title = "Scatterplot of NO3 uptake and daily light: Sycamore Creek, AZ")

# quartz(width=6.5, height=6)
U_vs_light_syca


```

###### Compare U_total and U_auto

What percentage of total uptake is the autotrophic uptake?

```{r - syca: percent autotrophic NO3 uptake}

# NON-AUTOTROPHIC UPTAKE = Equilibrium nitrate * K * z
# TOTAL UPTAKE = U + (Equilibrium nitrate * K * z)  
# Since U_auto is very small, there won't be much difference between 'non-autotrophic uptake' and 'total uptake'

daily_output_syca.df <- daily_output_syca.df %>%
  mutate(U_other = N_e*K*z,      
         U_tot1 = U_tot + U,
         U_tot2 = U_other + U,
         U_auto_perc = 100*U/U_tot1) 

plot(daily_output_syca.df$U_tot, daily_output_syca.df$U_other) # 1:1


U_auto_avg <- mean(daily_output_syca.df$U_auto_perc) #12.74% on average

Uauto_perc_syca <- plot_autoUperc(data=daily_output_syca.df, plot_title = "Daily autotrophic uptake (as % of total uptake), Sycamore Creek, AZ")

# quartz(width=7.5, height=6)
Uauto_perc_syca 

#### Interactive w. ggplotly, but only gives the x,y info... maybe if I added datetime-associated labels?
# ggplotly(Uauto_plot_syca)

```

###### Save updated model output datasets

```{r syca: save updated datasets}
#| output: false
#| message: false
#| warning: false

path = here("N_uptake_NEON/data/model_output/syca_output_daily.rds")
saveRDS(daily_output_syca.df, file=path)

path.h = here("N_uptake_NEON/data/model_output/syca_output_hourly.rds")
saveRDS(N_output_syca.df, file=path.h)

####### clean environment (retain functions)
# all_objects <- ls(envir = .GlobalEnv)
all_functions <- Filter(function(x) is.function(get(x, envir = .GlobalEnv)), ls(envir = .GlobalEnv))
rm(list = setdiff(ls(envir = .GlobalEnv), all_functions), envir = .GlobalEnv)

```

#### WLOU

Ouptuts and visualizations for West St Louis Creek, Grand, CO

###### Load model fits and data

```{r load WLOU model fit and data}
#| output: false
#| message: false
#| warning: false

########## Load model fit ______________________________________________________

wlou.fit <- readRDS(here("N_uptake_NEON/data/model_fits/pooledK_logK/wlou.fit.rds"))

# if working from model run
# wlou.fit <- fit.wlou


########## Load model data _____________________________________________________

# data from model

wlou.modeldata <- readRDS(here("N_uptake_NEON/data/model_datalist/wlou.data.rds"))

###### Load dataset (includes datatime info, which model data does not) _______

# # 15-min dataset
# path <- here("N_uptake_NEON/data/neon_data_clean/wlou_clean.csv")
# wlou.df <- read_csv(path) %>%
#   mutate(local_datetime = with_tz(local_datetime, tzone="US/Mountain"), 
#          model_datetime = local_datetime - hours(4))  
# 

# hourly dataset
path_h <- here("N_uptake_NEON/data/neon_data_clean/wlou_hourly_clean.csv")
wlou.df.h <- read_csv(path_h) %>%
  mutate(local_datetime = with_tz(local_datetime, tzone="US/Mountain"), 
         model_datetime = local_datetime - hours(4))  

# daily dataseset for Q, v, z
wlou.qvz <- readRDS(file=here("N_uptake_NEON/data/neon_data_clean/wlou.q.daily.rds"))
  
  
  
```

###### Extract model parameters and real-data values

First, check the fit for sigmas and betas ("sigma", "sigma_U", "b0", "b1"), since these are notoriously tough to fit. Then put the spread and mean/ median/ quartile info for K, U, and N_e into a WLOU tibble (to be saved in one large csv at the end of the document). Finally, assemble tibbles for the visualizations, which include both modeled parameters and real data.

```{r - wlou: extract obs. error model params and real-data values}
#| echo: false
#| message: false
#| warning: false

# get list of variables for the model fit
# get_variables(wlou.fit) - too many to see all, but informs re: structure [hour,day] for hourly variables

########## Check the fit for sigmas and betas (tough to fit): _________________
print(wlou.fit, pars=c("sigma", "sigma_U", "sigma_logK", "logK_mean", "b0", "b1"))


########## Summarize daily variables (K, U, N_e) ______________________________

# First, use spread_draws for the daily parameters - this gives each parameter its own column
# d = days (198), chain = 4, iteration = 1K, draw = 4K (1 per iteration per chain)
wlou.spread <- spread_draws(wlou.fit, K[d], U[d], U_tot[d], Vf_tot[d], N_e[d]) 

# Then get the mean/median + 95% QI (CI) for each parameter/day
# median + CI
wlou.medqi <- wlou.spread %>%
  # dplyr::group_by(d) %>%  # Unnecessary: tidybayes automatically groups by the selected spread_draws group [in this case, 'd']
  median_qi() %>%
  add_column(site = "WLOU")

# ### tried the 'summarise_draws()' fcn instead of median_qi(), but couldn't get it to pivot_wider() and make each variable its own column... So, it was good for checking all 'values_from' columns listed below, but not super useful for plotting etc. 

  # summarise_draws() %>%
  # pivot_wider(names_from = variable, values_from = mean, median, sd, mad, q5, q95, rhat, ess_bulk, ess_tail) %>%

## Also tried: 
#
# wlou.spreadsummary <- wlou.spread %>%
#   summarise_draws() %>%
#   #### pivot_wider(names_from = variable, values_from = mean, median, sd, mad, q5, q95, rhat, ess_bulk, ess_tail) %>%  # DOESN'T WORK HERE
#   add_column(site = "WLOU")
#   
# View(wlou.spreadsummary)


### mean + QI
wlou.meanqi <- wlou.spread %>%
  mean_qi() %>%
  add_column(site = "WLOU")

### Add median param info to the meanqi df
wlou.meanqi$K.median <- wlou.medqi$K
wlou.meanqi$U.median <- wlou.medqi$U
wlou.meanqi$U_tot.median <- wlou.medqi$U_tot
wlou.meanqi$Vf_tot.median <- wlou.medqi$Vf_tot
wlou.meanqi$N_e.median <- wlou.medqi$N_e

##### Get draws for b1 and b0 (for plotting later)
wlou.betas <- spread_draws(wlou.fit, b0, b1) %>%
  sample_n(max(wlou.medqi$d))




########## Summarize hourly variable (conc_pred) ______________________________

# Again, using spread_draws for the hourly parameter, conc_pred (predicted N concentration)
wlou.spread.h <- spread_draws(wlou.fit, conc_pred[h,d])
# View(wlou.spread.h)

# Getting the mean and 95% credible interval values
wlou.meanqi.h <- wlou.spread.h %>%
  dplyr::group_by(d, h) %>%  # This command IS needed here: tidybayes *did* have h + d column outputs, but the values were both different and less precise than when grouping by (d, h). 
  mean_qi() %>%
  add_column(site = "WLOU")

wlou.medqi.h <- wlou.spread.h %>%
  dplyr::group_by(d, h) %>% #again, explicit grouping is needed here
  median_qi() %>%
  add_column(site = "WLOU")

### Add median param info to the hourly meanqi df
wlou.meanqi.h$conc_pred.median <- wlou.medqi.h$conc_pred


########## Compile dataframes __________________________________________________

####### Hourly dataframe: N and N_pred #############

# start w. the hourly dataframe and add mean, QI, modeldata info
N_output_wlou.df <- wlou.df.h %>%  
  add_column(N_pred = wlou.meanqi.h$conc_pred, 
         N_pred.lower = wlou.meanqi.h$.lower, 
         N_pred.upper = wlou.meanqi.h$.upper, 
         z=as.vector(wlou.modeldata$zMA), 
         site = "WLOU") %>%
  rename(N_conc = surfWaterNitrateMean, 
         lightReal = GHI_wm2) %>%
   mutate(hours = hour(local_datetime), 
         mod_hours = hour(model_datetime)) %>%
  select(site, local_datetime, model_datetime, N_conc, N_pred, N_pred.lower, N_pred.upper, lightReal, z, Year, yr_jday, Jday, model_jday, hours, mod_hours) 
  

####### Daily dataframe: U, K, N_e ##################

# Create a domainID vector length(nday)
nday <- wlou.modeldata$D
domainID <- wlou.df.h$domainID[1:nday]

# start w. the meanqi dataframe and add modeldata and datetime info as needed
daily_output_wlou.df <- wlou.meanqi %>%
  add_column(yr_jday = unique(wlou.df.h$yr_jday), 
             sumlightReal = wlou.modeldata$sumlightReal, 
             sumlightIdeal = wlou.modeldata$sumlightIdeal, 
             Q = wlou.qvz$Q_cms,
             v = wlou.qvz$v,
             z = wlou.qvz$z,
             b0 = wlou.betas$b0,
             b1 = wlou.betas$b1,
             domainID = domainID) %>%
  separate(yr_jday, into = c("Year", "model_jday"), sep = "_", convert = TRUE, remove = FALSE) %>%  # makes the new columns numeric if possible, and retains the old column
  mutate(model_date = ymd(paste0(Year, "-01-01")) + days(model_jday - 1)) %>%
  dplyr::select(site, yr_jday, K, K.lower, K.upper, K.median, U, U.lower, U.upper, U.median, U_tot, U_tot.lower, U_tot.upper, U_tot.median, Vf_tot, Vf_tot.lower, Vf_tot.upper, Vf_tot.median, N_e, N_e.lower, N_e.upper, N_e.median, sumlightReal, sumlightIdeal, Q, v, z, b0,b1, model_date, model_jday, Year, domainID, d) #d is the index from the meanqi object - used for plotting 'ts' by index
         
# Check means for all 3 params
# mean(daily_output_wlou.df$K) # 3.225407
# mean(daily_output_wlou.df$U) # 0.1986689
# mean(daily_output_wlou.df$N_e) # 3.395281


```

###### Create visualizations - West St. Louis Creek, CO

```{r - wlou: create visualizations}
#| echo: true

########## N Model fit to data ________________________________________________

######  N model fit over time  #######

# N_and_Npred_wlou <- N_output_wlou.df %>%
#   #group_by(year(model_datetime_wlou)) %>%
#   filter(model_jday >= 35 & model_jday <= 60) %>%  # to see the little boxes better...
#   ggplot(aes(x=mod_hours)) +
#   geom_point(aes(y=N_conc)) + 
#   geom_line(aes(y=N_conc_pred), col='red')+
#   labs(
#     x="Time (h)", y=expression("N"~(mmol~m^-3))
#   ) +
#   ggtitle("NEON: West St. Louis Creek N data + model predictions") +
#   facet_wrap(~yr_jday) +
#   theme_bw()

N_Npred_ts_wlou <- plot_Nmodelfit_ts(data=N_output_wlou.df, start_jday = 311, end_jday = 365, plot_title = "NEON N data + model predictions: West St. Louis Creek, CO ")

quartz(width=6,height=6)
N_Npred_ts_wlou

### w. credible interval ribbon
N_Npred_tsCI_wlou <- plot_Nmodelfit_tsCI(data=N_output_wlou.df, start_jday = 35, end_jday = 60, plot_title = "NEON N data + model predictions: West St. Louis Creek, CO")

# quartz(width=6,height=6)
N_Npred_tsCI_wlou


######  N-pred vs N  #######

# Npred_v_N_wlou <- ggplot(data = N_output_wlou.df, aes(x=N_conc, y=N_conc_pred)) +
#   geom_point() + 
#   labs( x=expression("measured N"~(mu*mol~L^-1)), # fcn changed both to mmol~m^-3
#         y=expression("modeled N"~(mu*mol~L^-1))  # ~ = a space, * = no space
#       ) + 
#   ggtitle("Measured N vs predicted N, West St. Louis Creek, CO") +
#   geom_abline(intercept = 0, slope = 1, color = "red") + 
#   theme_bw()

Npred_v_N_wlou <- plot_Npred_v_Nobs(data=N_output_wlou.df, plot_title="Predicted N vs. measured N, West St. Louis Creek, CO")

#quartz(width=6, height=6)
Npred_v_N_wlou
ggplotly(Npred_v_N_wlou)

########## Equifinality checks ________________________________________________

####### Check equifinality!! K vs U  #######  

# KvU_wlou <- ggplot(data=daily_output_wlou.df, aes(y=K_mod_avg_wlou, x=U_mod_avg_wlou)) +
#   geom_point() + 
#   labs(y=expression("modeled K"~(d^-1)),
#        x=expression("modeled U"~(mmol~m^-2~d^-1))
#         ) + 
#   ggtitle("Equifinality check, West St. Louis Creek, CO: modeled K vs modeled U") + 
#   # geom_abline(intercept = 0, slope = 1, color = "red") + # doesn't show up - off the charts!
#   theme_bw()


KvU_wlou <- plot_KvU(data=daily_output_wlou.df, plot_title = "Equifinality check, West St. Louis Creek, CO: modeled K vs modeled U")

  
quartz(width=7.5, height=6)
KvU_wlou
  
ggplotly(KvU_wlou)
####### Check equifinality!! K vs N_e  #######  

# KvNe_wlou <- ggplot(data=daily_output_wlou.df, aes(y=K, x=N_e)) +
#    geom_point() + 
#    labs(y=expression("modeled K"~(d^-1)),
#         x=expression("modeled N_e"~(mmol~m^-3)) 
#         ) + 
#    ggtitle("Equifinality check, West St. Louis Creek, CO: modeled K vs modeled equilibrium N") + 
#    # geom_abline(intercept = 0, slope = 1, color = "red") + # doesn't show up - off the charts!
#   theme_bw()


KvNe_wlou <- plot_KvNe(data=daily_output_wlou.df, plot_title = "Equifinality check, West St. Louis Creek, CO: modeled K vs modeled equilibrium N")

#quartz(width=7.5, height=6)
KvNe_wlou
ggplotly(KvNe_wlou)

####### Check equifinality!! U vs N_e  #######  

# UvNe_wlou <- ggplot(data=daily_output_wlou.df, aes(y=U, x=N_e)) +
#    geom_point() + 
#    labs(y=expression("modeled U"~(mmol~m^-2~d^-1)),
#         x=expression("modeled N_e"~(mmol~m^-3)) 
#         ) + 
#    ggtitle("Equifinality check, West St. Louis Creek, CO: modeled U vs modeled equilibrium N") + 
#   theme_bw()

UvNe_wlou <- plot_UvNe(data=daily_output_wlou.df, plot_title = "Equifinality check, West St. Louis Creek, CO: modeled U vs modeled equilibrium N")
 
#quartz(width=7.5, height=6)
UvNe_wlou
ggplotly(UvNe_wlou)

########## Other parameter visualizations ______________________________________

######## K over time  #######  

# K_v_time_wlou <- ggplot(data = daily_output_wlou.df, aes(x=d, y=K)) +
#   geom_point() + 
#   # geom_point(y = sumlight.real, color = 'gold') +
#   # ADD IN HIGH AND LOW CIs
#   # xlab("Julian day") + ylab("modeled K (day -1)") + # daily change in N concentration
#   labs( x= "Index", 
#         y=expression("modeled K"~(d^-1)) # ~ = a space, * = no space
#       ) + 
#   #ylim(0,20) +
#   ggtitle("modeled K over time, West St. Louis Creek, CO") +
#   theme_bw()


K_v_time_wlou <- plot_K_ts(data=daily_output_wlou.df, plot_title = "modeled K over time, West St. Louis Creek, CO")

#quartz(width=6.5, height=6)
K_v_time_wlou
ggplotly(K_v_time_wlou)

##### to clip: no function yet
# K_time_clip_wlou <- ggplot(data = daily_output_wlou.df, aes(x=mod_day_wlou, y=K_mod_avg_wlou)) +
#   geom_point() + 
#   # geom_point(y = sumlight.real, color = 'gold') +
#   # ADD IN HIGH AND LOW CIs
#   xlab("Julian day") + ylab("modeled K (day -1)") + # ??? UNITS?
#   ylim(0,10) +
#   ggtitle("modeled K over time, West St. Louis Creek, CO - ylim = 0,10") +
#   theme_bw()
# 
# quartz()
# K_time_clip_wlou


######## U over time  #######  

# U_time_wlou <- ggplot(data = daily_output_wlou.df, aes(x=d, y=U)) +
#   geom_point() + 
#   # geom_point(y = sumlight.real_wlou, color = 'gold') +
#   # ADD IN HIGH AND LOW CIs
#   #xlab("Julian day") + ylab("modeled U (mmol/m2/day)") + 
#   labs( x= "Index", 
#         y=expression("modeled U"~(mmol~m^-2~d^-1)) # ~ = a space, * = no space
#       ) + 
#   #ylim(0,1) +
#   #ggtitle("modeled U over time, Big Creek 2019 pooled model w real light") +
#   ggtitle("Diel nitrate uptake (modeled), West St. Louis Creek, CO") +
#   theme_bw()

U_time_wlou <- plot_U_ts(data=daily_output_wlou.df, plot_title="Diel nitrate uptake (modeled), West St. Louis Creek, CO")

#quartz(width=6, height=6)
U_time_wlou

ggplotly(U_time_wlou)


# U_time_clip_wlou <- ggplot(data = daily_output_wlou.df, aes(x=mod_day_wlou, y=U_mod_avg_wlou)) +
#    geom_point() +
#    # geom_point(y = sumlight.real, color = 'gold') +
#    # ADD IN HIGH AND LOW CIs
#    xlab("Julian day") + ylab("modeled U (mmol/m2/day)") +
#    ylim(0,1) +
#    ggtitle("modeled U over time, West St. Louis Creek, CO - ylim = 0-1") +
#    theme_bw()
# 
#  quartz()
#  U_time_clip_wlou



######## U vs sumlight  #######  

# U_vs_light_wlou <- ggplot(data = daily_output_wlou.df, aes(x=sumlightReal, y=U)) +
#   geom_point() + 
#   #xlab("true light (satellite)") + ylab("modeled U (mmol/m2/day)") +
#   # xlab("light (satellite)") + ylab("modeled U (mmol/m2/day)") +
#   labs( x= "light (satellite)", 
#         y=expression("modeled U"~(mmol~m^-2~d^-1)) # ~ = a space, * = no space
#       ) + 
#   scale_x_log10() + scale_y_log10() + 
#   ggtitle("Scatterplot of NO3 uptake and daily light: West St. Louis Creek, CO") +
#   #ylim = c(-0.2, 1) +
#   theme_bw()

U_vs_light_wlou <- plot_UvLight(data=daily_output_wlou.df, plot_title = "Scatterplot of NO3 uptake and daily light: West St. Louis Creek, CO")

# quartz(width=6.5, height=6)
U_vs_light_wlou
ggplotly(U_vs_light_wlou)

```

###### Compare U_total and U_auto

```{r - wlou: percent autotrophic NO3 uptake}

# NON-AUTOTROPHIC UPTAKE = Equilibrium nitrate * K
# TOTAL UPTAKE = U + (Equilibrium nitrate * K)  
# Since U_auto is very small, there won't be much difference between 'non-autotrophic uptake' and 'total uptake'

daily_output_wlou.df <- daily_output_wlou.df %>%
  mutate(U_other = N_e*K*z,      
         U_tot1 = U_tot + U,
         U_tot2 = U_other + U,
         U_auto_perc = 100*U/U_tot1) 

plot(daily_output_wlou.df$U_tot, daily_output_wlou.df$U_other) # 1:1



U_auto_avg <- mean(daily_output_wlou.df$U_auto_perc) #10.3% on average

# Uauto_plot_wlou <- ggplot(data=daily_output_wlou.df, aes(x=d, y=U_auto_perc)) +
#   geom_point() + 
#   xlab("Index") + ylab("Percent autotrophic uptake") + 
#   ggtitle("Percent of autotrophic uptake each day, West St. Louis Creek, CO") + 
#   theme_bw()


Uauto_perc_wlou <- plot_autoUperc(data=daily_output_wlou.df, plot_title = "Daily  autotrophic uptake (as % of total uptake), West St. Louis Creek, CO")

# quartz(width=7.5, height=6)
Uauto_perc_wlou 
ggplotly(Uauto_perc_wlou )

#### Interactive w. ggplotly, but only gives the x,y info... maybe if I added datetime-associated labels?
# ggplotly(Uauto_plot_wlou)


```

###### Save updated model output datasets

```{r wlou:save updated datasets}
#| output: false
#| message: false
#| warning: false

path = here("N_uptake_NEON/data/model_output/wlou_output_daily.rds")
saveRDS(daily_output_wlou.df, file=path)

path.h = here("N_uptake_NEON/data/model_output/wlou_output_hourly.rds")
saveRDS(N_output_wlou.df, file=path.h)

# clean environment (retain functions)
# all_objects <- ls(envir = .GlobalEnv)
all_functions <- Filter(function(x) is.function(get(x, envir = .GlobalEnv)), ls(envir = .GlobalEnv))
rm(list = setdiff(ls(envir = .GlobalEnv), all_functions), envir = .GlobalEnv)

```

### Remove days w. crazy-high outlier Ks from BIGC and CUPE

One day per site has super-high K relative to the other Ks (later, should likely re-run model w/o these days)

```{r remove outlier}

# For BIGC
#   ID the day, so I can fix it later and re-run model
bigc_daily.df$model_date[which(bigc_daily.df$K > 7)] #"2021-05-20"
bigc_daily.df$model_jday[which(bigc_daily.df$K > 7)] # 2021, 140
# bigc_daily.df$yr_jday[which(bigc_daily.df$K > 7)] # just checking this value!

# remove the day
bigc_daily.df <- bigc_daily.df %>%
  filter(K<=7)
# save
path = here("N_uptake_NEON/data/model_output/bigc_output_daily.rds")
saveRDS(bigc_daily.df, file=path)

# For CUPE
#   ID the day, so I can fix it later and re-run model
cupe_daily.df$model_date[which(cupe_daily.df$K > 20)]  # "2021-07-06"
cupe_daily.df$model_jday[which(cupe_daily.df$K > 20)]  # 187
cupe_daily.df$yr_jday[which(cupe_daily.df$K > 20)]     # "2021_187": just checking this value!

# remove the day
cupe_daily.df <- cupe_daily.df %>%
  filter(K <= 20)

#save
path = here("N_uptake_NEON/data/model_output/cupe_output_daily.rds")
saveRDS(cupe_daily.df, file=path)

```

### Assemble and save mean, median, 95% QI tibble for all sites

Combine the site tibbles into one large tibble, then save it

```{r - assemble and save output tibble}
#| output: false
#| message: false
#| warning: false

########## Reload all output datasets as needed  _______________________________________

##### Load daily datasets
bigc_daily.df <- readRDS(here("N_uptake_NEON/data/model_output/bigc_output_daily.rds"))
cari_daily.df <- readRDS(here("N_uptake_NEON/data/model_output/cari_output_daily.rds"))
cupe_daily.df <- readRDS(here("N_uptake_NEON/data/model_output/cupe_output_daily.rds"))
prin_daily.df <- readRDS(here("N_uptake_NEON/data/model_output/prin_output_daily.rds"))
syca_daily.df <- readRDS(here("N_uptake_NEON/data/model_output/syca_output_daily.rds"))
wlou_daily.df <- readRDS(here("N_uptake_NEON/data/model_output/wlou_output_daily.rds"))

# Checking for any duplicate names (per the initial error: solved by using list())
# map(list(bigc_daily.df, cari_daily.df, cupe_daily.df, prin_daily.df, wlou_daily.df), ~ any(duplicated(names(.))))


##### Load hourly datasets
bigc_hourly.df <- readRDS(here("N_uptake_NEON/data/model_output/bigc_output_hourly.rds")) %>%
  mutate(model_jday = as.numeric(model_jday))
cari_hourly.df <- readRDS(here("N_uptake_NEON/data/model_output/cari_output_hourly.rds")) 
cupe_hourly.df <- readRDS(here("N_uptake_NEON/data/model_output/cupe_output_hourly.rds"))
prin_hourly.df <- readRDS(here("N_uptake_NEON/data/model_output/prin_output_hourly.rds"))
syca_hourly.df <- readRDS(here("N_uptake_NEON/data/model_output/syca_output_hourly.rds")) %>%
  mutate(model_jday = as.numeric(model_jday))
wlou_hourly.df <- readRDS(here("N_uptake_NEON/data/model_output/wlou_output_hourly.rds"))

########## Bind then save daily parameters (K, U, N_e)  _________________________
daily_list <- list(bigc_daily.df, cari_daily.df, cupe_daily.df, prin_daily.df, syca_daily.df, wlou_daily.df)
daily_summary_all.df <- bind_rows(daily_list)
# View(daily_summary_all.df)

### Save daily df
path <- here("N_uptake_NEON/data/model_output/daily_summary_all.rds")
saveRDS(daily_summary_all.df, path)

########## Bind then save hourly parameters (N_pred)  ___________________________

hourly_list <- list(bigc_hourly.df, cari_hourly.df, cupe_hourly.df, prin_hourly.df, syca_hourly.df, wlou_hourly.df)
hourly_summary_all.df <- bind_rows(hourly_list)
path2 <- here("N_uptake_NEON/data/model_output/hourly_summary_all.rds")
saveRDS(hourly_summary_all.df, path2)


```

### Create whole-dataset visualizations

##### Reload the summary dfs if needed

```{r}
 
path <- here("N_uptake_NEON/data/model_output/daily_summary_all.rds")
daily_summary_all.df <- readRDS(file=path)

path2 <- here("N_uptake_NEON/data/model_output/hourly_summary_all.rds")
hourly_summary_all.df <- readRDS(file=path2)

```

##### Set palette and filepath

```{r palette}
# Check for colorblind friendly palettes from MetBrewer
colorblind_palettes

# Archambault is in the colorblind palettes - but I couldn't get it to visualize. Moving on to PNW palettes... ALL PNW palettes are colorblind friendly

pal<- pnw_palette("Sunset2", n=6)
# pal<- pnw_palette("Bay", n=6)

plotpath <- here("N_uptake_NEON/images/pubfigs")


```

##### U v light

```{r whole-dataframe visualizations}

##### U vs measured light
## FIRST CHANGE CARI UNITS FOR LIGHT! - DONE


UvLight_all <- ggplot(daily_summary_all.df, aes(x = sumlightReal, y = U, color = site)) +
   geom_errorbar(aes(ymin = U.lower, ymax = U.upper),
                 width = 0.2, color='grey70') + # CI bars
  geom_point(alpha = 0.7) +  # scatterplot of points
  scale_color_manual(values = pal) +
  scale_x_log10() + scale_y_log10() + 
  # geom_abline(
  #   aes(intercept = b0, slope = b1),
  #   alpha = 0.2,
  #   color = "blue") +  # model fits per site using output b1 and b0
  facet_wrap(~site, scales="free") +  # use scales="free_y" or "free_x" to free only 1 axis
  labs(
    x = expression("Measured Light"~(W~m^-2)),
    y = expression("Autotrophic"~NO[3]~"Uptake"~(mmol~m^-2~d^-1))
  ) +
  # ylim(0,1.7) + 
  theme_bw() +  # clean look
  theme(legend.position = "none",
        # plot.margin = margin(t = 6, r = 12, b = 6, l = 6, unit = "point"), 
        panel.grid.major = element_blank(),  panel.grid.minor = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1,   # keeps labels aligned at the tick
                                   vjust = 1)    # keeps them from floating above
  )

  
quartz(height=4, width=5.5)
UvLight_all

ggsave2(filename="Uauto_vs_light.pdf", plot=UvLight_all, path=plotpath, width=5.5, height=4)

# pubr version only for raster graphics
# ggexport(UvLight_all, filename = file.path(plotpath, "UvLight.pdf"), width = 8.5, height = 5.25, units = "cm")

# cowplot





```

##### U vs. N_e

```{r UvsNe}

UvNe_all <- ggplot(daily_summary_all.df, aes(x = N_e, y = U, color = site)) +
  geom_point(alpha=0.5) +  # scatterplot of points
  # scale_color_met_d(name = "site", palette = "Archambault") + #palette Archimbault from MetBrewer
  scale_color_manual(values = pal) +
  theme_bw() +  # theme_minimal also gives a clean look
  labs(
    # title = "Across sites, autotrophic uptake increases with equilibrium nitrate",
    x = expression("Equilibrium Nitrate"~(mmol~m^-3)),
    y = expression("Autotrophic"~NO_3~"Uptake"~(mmol~m^-2~d^-1))
  ) + 
  ylim(0,2)
  
quartz(height=5, width=8)
UvNe_all

```

##### Stacked barplot w. total uptake and % autotrophic uptake

```{r perc auto uptake}
# Reshape uptake types to long format
site_summary <- daily_summary_all.df %>%
  mutate(perc_otherU = 100-U_auto_perc) %>%
  rename(perc_autoU = U_auto_perc) %>%
  dplyr::group_by(site) %>%
  summarise(perc_autoU = mean(perc_autoU, na.rm = TRUE),
            perc_otherU = mean(perc_otherU, na.rm = TRUE)) %>%
  ungroup()
 
  
long_uptake <- site_summary %>%
  pivot_longer(
    cols = c(perc_otherU, perc_autoU),
    names_to = "uptake_type",
    values_to = "percent"
  )


# Plot the stacked barplot

uptakeBarplot <- ggplot(long_uptake, aes(x = site, y = percent, fill = uptake_type)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(percent), "%")), 
            position = position_stack(vjust = 0.5), 
            color = "white", size = 3.5) +
  # scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +  Use if not already in percent format
  scale_y_continuous(labels = scales::label_number(accuracy = 1, suffix = "%")) +
  scale_fill_manual(
    values = c(
      # "perc_auto" = "#006400",  # dark green
      "perc_autoU" = "palegreen4",  # greyer green
      # "perc_het" = "#8B4513"    # saddle brown
      # "perc_het" = "#A0522D"    # sienna
      # "perc_het" = "#BC8F8F"    # rosy brown
      "perc_otherU" = "grey40"    # darker rosy brown
    ),
    labels = c("Autotrophic", "Other")
  ) +
  labs(x = "Site", y = "Uptake type percentages by site", fill = "Uptake Type") +
  theme_minimal()

quartz(height=5, width=8)
uptakeBarplot

ggsave2(filename="uptakeBarplot.pdf", plot=uptakeBarplot, path=plotpath, width=8, height=5)
ggsave2(filename="uptakeBarplotXL.pdf", plot=uptakeBarplot, path=plotpath, width=16, height=10)


```

##### K over time

```{r K_ts}
# pal<- pnw_palette("Sunset2", n=6) ## load above

Kplot <- ggplot(data=daily_summary_all.df, aes(x=d, y=K, color=site)) +
   geom_errorbar(aes(ymin = K.lower, ymax = K.upper),
                 width = 0.2, color='grey70') + # CI bars
   geom_point() +
  # geom_point(y = sumlight.real, color = 'gold') +
  scale_color_manual(values = pal) +
  facet_wrap(~site)+
   labs(x= "Index of days",
        y=expression(K~(d^-1)) # ~ = a space, * = no space
      ) +
  ylim(0,15) +
  xlim(0,200)+
   # ggtitle(plot_title) +
  theme_bw() + 
  theme(legend.position = "none")


quartz(height=5, width=8)
Kplot

ggplotly(Kplot)

daily_summary_all.df %>%
  group_by(site) %>%
  summarise(mean_K = mean(K, na.rm = TRUE))

daily_summary_all.df %>%
  group_by(site) %>%
  summarise(meanSw = mean(Sw_x, na.rm=TRUE))

```

##### U vs K w. uncertainty

```{r UvsK}

# pal<- pnw_palette("Sunset2", n=5)   ## load above

UvKplot <- ggplot(data=daily_summary_all.df, aes(x=K, y=U, color=site)) +
   geom_errorbar(aes(ymin = U.lower, ymax = U.upper), 
                 width = 0.2, color='grey70') + # vertical CI bars for U
  geom_errorbar(aes(xmin=K.lower, xmax=K.upper), 
                width=0.2, color='grey70') + # horizontal CI bars for K
  geom_point() + 
  scale_color_manual(values = pal) +
  scale_x_log10() + scale_y_log10() + 
  facet_wrap(~site) +
  labs(x=bquote(italic(K[nit])~(d^-1)),
       y=bquote("Autotrophic"~NO[3]~"Uptake"~(mmol~m^-2~d^-1))
        ) + 
  
  # geom_abline(intercept = 0, slope = 1, color = "red") + # doesn't show up - off the charts!
  theme_bw() + 
  theme(legend.position = "none", 
        # panel.grid.major = element_blank(),  panel.grid.minor = element_blank()
        )

quartz(height=4, width=6)
UvKplot

ggplotly(UvKplot)
  
ggsave2(filename="UvsK_loglog.pdf", plot=UvKplot, path=plotpath, width=5, height=3.5)

ggsave2(filename="UvsK_loglog.jpg", plot=UvKplot, path=plotpath, width=6, height=4)

```

##### Uptake length for nitrate at sites

```{r uptake length}

## for each site, horizontal boxplot of avg for 90% distance: x = ln(.90)*v/k
# pal<- pnw_palette("Sunset2", n=5)   ## load above

daily_summary_all.df <- daily_summary_all.df %>%
  mutate(Sw_x = 86400*2.302585*v/K)  #UNITS: remember to convert m s-1 to m d-1...
  # mutate(Sw_x = log(.1)*-v/K)

# 

Swplot <- daily_summary_all.df %>%
  mutate(Sw_km = Sw_x/1000) %>%
  ggplot(aes(x=Sw_km, y=site, fill=site)) + 
  geom_boxplot() + 
  scale_fill_manual(values = pal) +
  scale_y_discrete(limits = rev) +  # Reverse the y axis only, so it reads A->Z, top to bottom
  scale_x_log10() + # log the x axis so all sites visualize better
  # coord_cartesian(xlim = c(0, 20000)) +  # <-- zoom in instead of xlim(): doesn't remove CARI
  labs(x = "Uptake Length (km)", y = "") + 
  theme_bw()
  
quartz(width=8, height=5)
Swplot

ggsave2(filename="Sw_boxplot.pdf", plot=Swplot, path=plotpath, width=6, height=4)

# The jitter didn't look as good...
SwJitter <- daily_summary_all.df %>%
  ggplot(aes(x=Sw_x, y=site, fill=site)) + 
  geom_boxplot(fill = "gray90", color = "black") +
  geom_jitter(aes(color = site), width = 0.2, height = 0.1, alpha = 0.7) +
  scale_color_manual(values = pal) +
  scale_y_discrete(limits = rev) +  # Reverse the y axis only, so it reads A->Z, top to bottom
  coord_cartesian(xlim = c(0, 20000)) +  # <-- zoom in instead of xlim(): doesn't remove CARI
  labs(x = "Uptake Length (m)", y = "") + 
  theme_bw()
  
quartz(width=7.5, height=5)
SwJitter


geom_boxplot(fill = "gray90", color = "black") +
  geom_jitter(aes(color = siteID), width = 0.2, height = 0.1, alpha = 0.7) +
  scale_color_manual(values = pal) +

#   Hmmm, weird. Let's look at the v values... AND check the units. 
vel_plot <- daily_summary_all.df %>%
  ggplot(aes(x=v, y=site, fill=site)) + 
  geom_boxplot() + 
  scale_fill_manual(values = pal) +
  scale_y_discrete(limits = rev) +  # Reverse the y axis only, so it reads A->Z, top to bottom
  labs(y = "", x=expression("velocity"~(m~s^-1))) + 
  theme_bw()

quartz(width=7.5, height=5)
vel_plot

########  K Boxplot  #####################################

K_boxplot <- daily_summary_all.df %>%
  ggplot(aes(y=site, x=K, fill=site)) + 
  geom_boxplot() + 
  scale_fill_manual(values = pal) +
  scale_y_discrete(limits = rev) +  # Reverse the y axis only, so it reads A->Z, top to bottom
  labs(y = "", x=expression(italic(K[nit])~(d^-1))) + 
  theme_bw()

quartz(width=7.5, height=5)
K_boxplot


```

##### Model fit fig - all sites

```{r modelfit-all}

##### Create dataset w. 1 day/site (random selection to start)

hourly_summary_all.df <- hourly_summary_all.df %>%
  mutate(model_datetime = force_tz(model_datetime, "UTC"), 
         model_date = as.Date(model_datetime))

# NB: tz issue: before, the model_datetime tz was set to Pacific, like the 1st site (BIGC), which totally messed up the selection - there was an 8h offset for the model_datetime and model_date values, weird. MB model_date was in UTC?

set.seed(210012)
random_days <- hourly_summary_all.df %>%
  dplyr::group_by(site) %>%
  dplyr::filter(model_date > as.Date("2021-09-30")) %>%
  summarise(model_date = sample(unique(model_date), 1), .groups = "drop")

fitplot_days.df <- hourly_summary_all.df %>%
  inner_join(random_days, by = c("site", "model_date"))



######## exploration needed - many days are fragmented, something is still very off w the df
start_jday <- 160
end_jday <- 190  # range doesnt include CARI; also, can't really check sites. meh
name = "CARI"

explore_plot <- hourly_summary_all.df %>%
  filter(model_jday >= start_jday & model_jday <= end_jday, 
         site == name) %>%
  ggplot(aes(x = mod_hours)) +
    # geom_ribbon(aes(ymin=N_pred.lower, ymax=N_pred.upper), alpha = 0.7, fill = "slategray") +
    geom_point(aes(y = N_conc)) + 
    geom_line(aes(y = N_pred), col = 'red') +
    labs(
      x = "Time (h)",
      y = expression("N" ~ (mmol ~ m^-3))
    ) +
    ggtitle("exploration plots") +
    facet_wrap(~ yr_jday, scales = "free_y") +
    theme_bw()

quartz()
explore_plot

ggplotly(explore_plot)
  
######  N model fit over time - predicted N vs measured N  #######
   
plot_title <- "NEON N data + model predictions: 1 day per site"

fitplot <- fitplot_days.df %>%
    ggplot(aes(x = mod_hours)) +
    geom_ribbon(aes(ymin=N_pred.lower, ymax=N_pred.upper), alpha = 0.7, fill = "slategray") +
    geom_point(aes(y = N_conc)) + 
    geom_line(aes(y = N_pred), col = 'red') +
    labs(
      x = "Time (h)",
      y = expression("N" ~ (mmol ~ m^-3))
    ) +
    ggtitle(plot_title) +
    facet_wrap(~site, scales = "free_y") +
    theme_bw()

quartz()
fitplot


N_Npred_all <- plot_Nmodelfit_ts(data=N_output_cupe.df, start_jday = 35, end_jday = 60, plot_title = "NEON N data + model predictions: 1 day per site")

quartz(width=6,height=6)
N_Npred_ts_cupe

### w. credible interval ribbon
N_Npred_tsCI_cupe <- plot_Nmodelfit_tsCI(data=N_output_cupe.df, start_jday = 35, end_jday = 60, plot_title = "NEON N data + model predictions: Rio Cupeyes, PR")

quartz(width=6,height=6)
N_Npred_tsCI_all


```

##### C:N stoichiometry and GPP vs N uptake

```{r cn stoich}
# gpp.df <- bind_rows(CARI.gpp, WLOU.gpp)

U.df <- daily_summary_all.df %>%
  dplyr::filter(site != "CARI")

U_GPP <- inner_join(U.df, gpp.df, by = c("site", "model_date" = "date")) %>%
  mutate(GPP.mmol = GPP.daily*1000/32, 
         GPP.mmol05 = GPP.mmol * 0.5) %>%  # I *think* Bob's GPP is already molar?
  filter(GPP.mmol >= 0)

# Plot w. same palette as all graphs
# pal<- pnw_palette("Sunset2", n=5)   ## load above
# 
# cn_lines <- data.frame(
#   slope = c(1/10, 1/20, 1/40, 1/60, 1/80, 1/100, 1/120, 1/140, 1/160),
#   label = c("C:N = 10:1", "C:N = 20:1", "C:N = 40:1", "C:N = 60:1",
#             "C:N = 80:1", "C:N = 100:1", "C:N = 120:1", "C:N = 140:1", "C:N = 160:1")
# )


# define ratios
ratios <- c(5, 10, 20, 40, 60, 80, 100, 200)  # for the larger image
ratios <- c(5, 10, 20, 40, 60, 100, 200)      # for the smaller image

cn_lines <- data.frame(
  ratio = ratios,
  slope = 1/ratios,
  label = paste0("C:N = ", ratios, ":1")
)

# axis limits
ymax <- 3
xmax <- max(U_GPP$GPP.mmol05, na.rm = TRUE)
xpos_default <- 0.9 * xmax  # 0.8 for smaller plot, 0.9 for slide size

# compute x/y positions per line
cn_lines <- cn_lines %>%
  rowwise() %>%
  mutate(
    y_at_xpos = slope * xpos_default,
    xpos = ifelse(y_at_xpos <= ymax * 0.95,
                  xpos_default,
                  (ymax * 0.95) / slope),   # solve for x where line ~ymax
    ypos = slope * xpos,
    angle = atan(slope) * 180 / pi
  ) %>%
  ungroup()


UvGPP <- ggplot(data=U_GPP, aes(x=GPP.mmol05, y=U, color=site)) + 
  geom_point(size=3) +
  scale_color_manual(values = pal[-2]) +
  geom_abline(data = cn_lines, aes(intercept = 0, slope = slope),
              linetype = "twodash", color = "grey") +
  geom_text(
    data = cn_lines,
    aes(x = xpos, y = ypos, label = label), # aes(x = xpos, y = ypos, label = label, angle = angle),
    inherit.aes = FALSE,
    hjust = -0.1, vjust = -0.5,
    size = 3, color = "grey40"
  ) +
  labs(
    x = expression(GPP ~ "\u00D7" ~0.5 ~ (mmol~O[2]~or~C~m^-2~d^-1)),
    y = expression(U ~ (mmol~NO[3]-N~m^-2~d^-1)) ) +
  theme_bw() +
  coord_cartesian(ylim = c(0, 3), clip = "on") 

# +
#   theme(plot.margin = margin(10, 40, 10, 10))

##### V2
# UvGPP <- ggplot(data=U_GPP, aes(x=GPP.mmol, y=U, color=site)) + 
#   geom_point(size=3) +
#   scale_color_manual(values = pal[-2]) +
#   geom_abline(data = cn_lines, aes(intercept = 0, slope = slope),
#               linetype = "twodash", color = "grey") +
#   geom_text(data = cn_lines, aes(x = xpos, y = ypos, label = label),
#             inherit.aes = FALSE, hjust = -0.1, vjust = -0.5, size = 3, color = "grey40") +
#   ylim(0,3) + 
#   labs(x = expression("GPP"~(mmol~O[2]~or~C~m^-2~d^-1)),
#        y = expression("U"~(mmol~NO[3]-N~m^-2~d^-1))) +
#   theme_bw()


### ORIG
# UvGPP <- ggplot(data=U_GPP, aes(x=GPP.mmol, y=U, color=site)) + 
#   geom_point(size=3) +
#   scale_color_manual(values = pal[-2]) +
#   geom_abline(intercept= 0, slope = 1/10, linetype = "twodash", color='grey') + 
#   geom_abline(intercept= 0, slope = 1/20, linetype = "twodash", color='grey') +
#   geom_abline(intercept= 0, slope = 1/40, linetype = "twodash", color='grey') +  
#   geom_abline(intercept= 0, slope = 1/60, linetype = "twodash", color='grey') +  
#   geom_abline(intercept= 0, slope = 1/80, linetype = "twodash", color='grey') + 
#   geom_abline(intercept= 0, slope = 1/100, linetype = "twodash", color='grey') +
#   geom_abline(intercept= 0, slope = 1/120, linetype = "twodash", color='grey') + 
#   geom_abline(intercept= 0, slope = 1/140, linetype = "twodash", color='grey') + 
#   geom_abline(intercept= 0, slope = 1/160, linetype = "twodash", color='grey') + 
#   labs(x=expression("GPP"~(mmol~O[2]~or~C~m^-2~d^-1)),
#        y=expression("U"~(mmol~NO[3]-N~m^-2~d^-1))) +
#   # facet_wrap(~site) +
#   theme_bw()

UvGPPmarg <- ggMarginal(UvGPP, type='density', margins = "y", groupFill = TRUE)

quartz(height=10, width=16)
UvGPP

ggsave2(filename="UvsGPPlg.pdf", plot=UvGPP, path=plotpath, width=8, height=5)
ggsave2(filename="UvsGPPxlg.pdf", plot=UvGPP, path=plotpath, width=16, height=10)
ggsave2(filename="UvsGPP.pdf", plot=UvGPP, path=plotpath, width=6, height=3.75)


```

##### My Vf (K\*z) v daily avg N conc w. LINX II (log scale axes)

```{r vf vs no3 conc}


# Calc daily Vf for NEON sites
daily_summary_all.df <- daily_summary_all.df %>%
  # mutate(Vf_tot = K*z,   # units = m/d
  mutate(Vf_tot = U_tot/N_e,  # U_tot = K*z*N_e + U(auto)
         mean_marker = NA)  # adding column so shape difference shows on legend


# get the means for each site
sitemeans <- daily_summary_all.df %>%
  dplyr::group_by(site) %>%
  dplyr::summarise(Ne_mean = mean(N_e), 
                   Vf_tot_mean = mean(Vf_tot)) %>%
  ungroup() 

sitemeans$mean_marker <- factor("site mean", levels = c("site mean")) # adding column so shape difference shows on legend


# Change LINX II df units to match mine: N conc in mmol m^-3; 
linx.df <- linx.df %>%
  mutate(N_conc = NO3conc_ugN_L/14.0067, #convert to mmol m^-3  (just divide by molar wt)
         Vf_tot = Vf_tot_no3_cm_s*864, # convert to m/day (0.001*86400 = 864)
         Vf_assim = Vf_assim_cm_s*864) # convert to m/day (0.001*86400 = 864)

# the usual palette: 
pal<- pnw_palette("Sunset2", n=6)   ## load above
names(pal) <- unique(daily_summary_all.df$site)  # ensure this matches exactly

### Extend the palette to include LINX II
pal_extended <- c(pal, "LINX II" = "grey15")
names(pal_extended) <- c(names(pal), "LINX II")  # ensure the names tranfer to pal_extended

## set up legend to have "LINX II" last 

# Set desired legend order
legend_order <- c(unique(daily_summary_all.df$site), "LINX II")
# Main data: use original site names, but add ordered factor for legend
daily_summary_all.df$site_legend <- factor(daily_summary_all.df$site, levels = legend_order)

sitemeans$site_legend <- factor(sitemeans$site, levels = legend_order)

# LINX data: assign "LINX II" to the new legend column
linx.df$site_legend <- factor("LINX II", levels = legend_order)

# daily_summary_all5 <- daily_summary_all.df %>%
#   filter(site != "SYCA")

# create the plot
Vf_plot <- ggplot() + 
  ###### V1: regular points with color legend (color mapped to site_legend)
  geom_point(data = daily_summary_all.df,
             aes(x = N_e, y = Vf_tot, color = site_legend), alpha = 0.3) +

  # ###### V2: hollow circle:
  # geom_point(data = daily_summary_all.df, aes(x = N_e, y = Vf_tot, color = site_legend),
  #            shape = 1,      # hollow circle
  #            alpha = 0.2) +

  # ###### V3: no-border circle DID NOT WORK
  # geom_point(data = daily_summary_all.df, aes(x = N_e, y = Vf_tot, fill = site_legend),
  #            shape = 21,     # filled circle
  #            border = NA,
  #            inherit.aes = FALSE,
  #            alpha = 0.1,
  #            ) + #keeps the points from becoming all-black

  # # # LINX points, also color mapped to site_legend
  geom_point(data = linx.df,
             aes(x = N_conc, y = Vf_tot, color = site_legend)) + 
  # mean points: fill mapped to site_legend, outline fixed black, no legend for fill/color
  geom_point(data = sitemeans,
             aes(x = Ne_mean, y = Vf_tot_mean, fill = site_legend, shape = mean_marker),
             color = "black", size = 3, stroke = 1, alpha = 1,
             show.legend = c(fill = FALSE, color = FALSE, shape = TRUE)) + 
  # scales: color for sites, fill for means (not shown in legend)
  scale_color_manual(values = pal_extended, 
                    guide = guide_legend(override.aes = list(shape=16, alpha = 1, size = 3))) +  
  scale_fill_manual(values = pal_extended) +
  scale_shape_manual(name = "", values = c("site mean" = 23),
                     guide = guide_legend(override.aes = list(
                       fill = "grey60", color = "grey15", size = 3, stroke = 1,
                       alpha = 1))) +
   # log scale
  scale_x_log10(
  breaks = c(0.01, 0.1, 1, 10, 100, 1000, 10000),   # customize to your data range
  labels = c("0.01", "0.1", "1", "10", "100", "1000", "10000")) + 

  scale_y_log10( breaks = c(0.01, 0.1, 1, 10),   # customize to your data range
  labels = c("0.01", "0.1", "1", "10")) + 
  
  # labels
  labs(
    x = bquote(NO[3]^"-" ~ (mmol ~ m^-3)),
    y = bquote(Uptake~velocity*","~italic(V[f])~~(m~d^-1)),
    color = 'Site',
    fill = 'Site'
  ) +
  
  guides(alpha = "none") + 
  theme_bw()

quartz(height = 3.5, width = 5)
Vf_plot

ggsave2(filename="LINX_Vf_loglog.pdf", plot=Vf_plot, path=plotpath, width=5, height=3.5)


```

```{r Vf plot attempt 2}

# # From chatgpt to fix plotting issues:
# 
# Vf_plot <- ggplot() + 
#   # Regular points, semi-transparent
#   geom_point(data = daily_summary_all.df,
#              aes(x = N_e, y = Vf_tot, color = site_legend, alpha = 0.2)) + 
#   # Mean points as big diamonds with black outline
#   geom_point(data = sitemeans,
#              aes(x = Ne_mean, y = Vf_tot_mean, fill = site_legend, shape = mean_marker),
#              color = "black", size = 5, stroke = 1, alpha = 1) + 
#   # LINX II points
#   geom_point(data = linx.df,
#              aes(x = N_conc, y = Vf_tot, color = site_legend)) + 
#   # Color for points and fill for diamonds
#   scale_color_manual(values = pal_extended) +  
#   scale_fill_manual(values = pal_extended) +
#   # Shape scale for mean points legend (diamond shape 23)
#   scale_shape_manual(name = "", 
#                      values = c("site mean" = 23),
#                      guide = guide_legend(override.aes = list(size = 3, color = "black",
#                                                               stroke = 1, alpha = 1))) +
#   # Log scales
#   # scale_x_log10(breaks = scales::trans_breaks("log10", function(x) 10^x),
#   # labels = scales::trans_format("log10", scales::math_format(10^.x))) +
#   
#   scale_x_log10(
#   breaks = c(0.01, 0.1, 1, 10, 100, 1000, 10000),   # customize to your data range
#   labels = c("0.01", "0.1", "1", "10", "100", "1000", "10000")) + 
# 
#   scale_y_log10( breaks = c(0.01, 0.1, 1, 10),   # customize to your data range
#   labels = c("0.01", "0.1", "1", "10")) + 
#   # Labels
#   labs(
#     x = expression("N" ~ (mmol ~ m^-3)),
#     y = expression("Uptake velocity,"~V[f]~~(m~d^-1)),
#     color = 'Site',
#     fill = 'Site'
#   ) +
#   theme_bw()
# 
# 
# quartz(height = 5, width = 8)
# Vf_plot
# 
# 
# 
# 
# ################
# 
# 
# Vf_plot <- ggplot() + 
#   geom_point(data = daily_summary_all.df,
#              aes(x = N_e, y = Vf_tot, color = site_legend, alpha = 0.2)) + 
#   geom_point(data = sitemeans,
#              aes(x = Ne_mean, y = Vf_tot_mean, color = site_legend, shape = mean_marker),
#              size = 5, stroke = 1,
#              show.legend = c(color = FALSE, shape = TRUE)) + 
#   geom_point(data = linx.df,
#              aes(x = N_conc, y = Vf_tot, color = site_legend)) + 
#   scale_color_manual(values = pal_extended) +  
#   scale_shape_manual(name = "", values = c("site mean" = 23)) +
#   # log scale
#   scale_x_log10(
#   breaks = c(0.01, 0.1, 1, 10, 100, 1000, 10000),   # customize to your data range
#   labels = c("0.01", "0.1", "1", "10", "100", "1000", "10000")) + 
# 
#   scale_y_log10( breaks = c(0.01, 0.1, 1, 10),   # customize to your data range
#   labels = c("0.01", "0.1", "1", "10")) + 
#   # Labels
#   labs(
#     x = expression("N" ~ (mmol ~ m^-3)),
#     y = expression("Uptake velocity,"~V[f]~~(m~d^-1)),
#     color = 'Site'
#   ) +
#   theme_bw()
# 
# 
# 
# ################ attempt 4(ish)
# 
# Vf_plot <- ggplot() + 
#   geom_point(data = daily_summary_all.df,
#              aes(x = N_e, y = Vf_tot, color = site_legend, alpha = 0.5),
#              show.legend = TRUE) + 
#   geom_point(data = sitemeans,
#              aes(x = Ne_mean, y = Vf_tot_mean, fill = site_legend, shape = mean_marker),
#              color = "black", size = 3, stroke = 1, alpha = 1,
#              show.legend = c(fill = FALSE, color = FALSE, shape = TRUE)) + 
#   geom_point(data = linx.df,
#              aes(x = N_conc, y = Vf_tot, color = site_legend)) + 
#   scale_color_manual(values = pal_extended) +  
#   scale_fill_manual(values = pal_extended) +
#   scale_shape_manual(name = "", 
#                      values = c("site mean" = 23),
#                      guide = guide_legend(override.aes = list(size = 3, color = "black",
#                                                               stroke = 1, alpha = 1))) +
#   # log scale
#   scale_x_log10(
#   breaks = c(0.01, 0.1, 1, 10, 100, 1000, 10000),   # customize to your data range
#   labels = c("0.01", "0.1", "1", "10", "100", "1000", "10000")) + 
# 
#   scale_y_log10( breaks = c(0.01, 0.1, 1, 10),   # customize to your data range
#   labels = c("0.01", "0.1", "1", "10")) + 
#   # labels
#   labs(
#     x = expression("N" ~ (mmol ~ m^-3)),
#     y = expression("Uptake velocity,"~V[f]~~(m~d^-1)),
#     color = 'Site',
#     fill = 'Site'
#   ) +
#   guides(alpha = "none") +   # removes alpha from legend
#   theme_bw()
# 
# 
# 
# ############# attempt 5(ish) WORKED - copying above
# Vf_plot <- ggplot() + 
#   # regular points with color legend (color mapped to site_legend)
#   geom_point(data = daily_summary_all.df,
#              aes(x = N_e, y = Vf_tot, color = site_legend, alpha = 0.2)) + 
#   # LINX points, also color mapped to site_legend
#   geom_point(data = linx.df,
#              aes(x = N_conc, y = Vf_tot, color = site_legend)) + 
#   # mean points: fill mapped to site_legend, outline fixed black, no legend for fill/color
#   geom_point(data = sitemeans,
#              aes(x = Ne_mean, y = Vf_tot_mean, fill = site_legend, shape = mean_marker),
#              color = "black", size = 3, stroke = 1, alpha = 1,
#              show.legend = c(fill = FALSE, color = FALSE, shape = TRUE)) + 
#   # scales: color for sites, fill for means (not shown in legend)
#   scale_color_manual(values = pal_extended) +  
#   scale_fill_manual(values = pal_extended) +
#   scale_shape_manual(name = "", values = c("site mean" = 23),
#                      guide = guide_legend(override.aes = list(
#                        fill = "grey60", color = "black", size = 3, stroke = 1,
#                        alpha = 1))) +
#    # log scale
#   scale_x_log10(
#   breaks = c(0.01, 0.1, 1, 10, 100, 1000, 10000),   # customize to your data range
#   labels = c("0.01", "0.1", "1", "10", "100", "1000", "10000")) + 
# 
#   scale_y_log10( breaks = c(0.01, 0.1, 1, 10),   # customize to your data range
#   labels = c("0.01", "0.1", "1", "10")) + 
#   
#   # labels
#   labs(
#     x = expression("N" ~ (mmol ~ m^-3)),
#     y = expression("Uptake velocity,"~V[f]~~(m~d^-1)),
#     color = 'Site',
#     fill = 'Site'
#   ) +
#   guides(alpha = "none") + 
#   theme_bw()
# 
# quartz(height = 5, width = 8)
# Vf_plot
# 
# 
# 


```

```{r troubleshooting Vf plot}

# dput(levels(daily_summary_all.df$site_legend))
# dput(names(pal_extended))
# 
# setdiff(levels(daily_summary_all.df$site_legend), names(pal_extended))
# 

```
